{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook to download images from unsplash dataset and save metadata to a database\n",
    "# Imports\n",
    "The code imports several libraries to work correctly. The libraries are as follows:\n",
    "\n",
    "pickle: This module implements binary protocols for serializing and de-serializing a Python object structure. It is used here to save the metadata information in binary format.\n",
    "\n",
    "os: This module provides a portable way of using operating system dependent functionality like reading or writing to the file system. It is used here to join paths and create directories.\n",
    "\n",
    "zipfile: This module allows us to read and write ZIP archive files. It is used here to extract images from a compressed archive.\n",
    "\n",
    "requests: This module allows us to send HTTP/1.1 requests using Python. It is used here to download images from a URL.\n",
    "\n",
    "functools: This module provides various functions that can be used to create higher-order functions. It is used here to define the cached_property decorator.\n",
    "\n",
    "pathlib: This module provides a higher-level interface to working with file system paths than the os module. It is used here to create directories.\n",
    "\n",
    "tqdm: This module provides a progress bar that can be used to track the progress of a long-running operation.\n",
    "\n",
    "json: This module provides methods for working with JSON data. It is used here to save the metadata information in JSON format.\n",
    "\n",
    "sqlite3: This module provides a Python interface to SQLite databases. It is used here to create and interact with a SQLite database.\n",
    "\n",
    "pandas: This module is used for data manipulation and analysis. It is used here to read CSV files and create a pandas DataFrame.\n",
    "\n",
    "PIL: This module provides an interface for opening, manipulating, and saving many different image file formats. It is used here to extract metadata from images.\n",
    "\n",
    "nest_asyncio: This module provides a way to run nested event loops in asyncio.\n",
    "\n",
    "asyncio: This module provides infrastructure for writing single-threaded concurrent code using coroutines, multiplexing I/O access over sockets and other resources, running network clients and servers, and other related primitives.\n",
    "\n",
    "aiohttp: This module provides an asynchronous HTTP client/server implementation using asyncio.\n",
    "\n",
    "time: This module provides various time-related functions. It is used here to measure the time taken to download images.\n",
    "\n",
    "subprocess: This module provides a way to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. It is used here to check if the exiftool command line tool is installed.\n",
    "\n",
    "tqdm_asyncio: This module provides a progress bar for asyncio applications. It is used here to track the progress of processing all images."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install requests pathlib tqdm pandas pillow nest_asyncio aiohttp mysql-connector-python python-dotenv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import asyncio\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings base variables and paths\n",
    "For this project, we used the unsplash dataset, which is a large-scale image dataset. The dataset contains over 25,000 images.\n",
    "The code sets the base variables and paths for the project. The variables are as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "include_path = os.path.join(output_path, \"include\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, include_path]\n",
    "\n",
    "# Set the base URL for the dataset\n",
    "dataset_url = \"https://unsplash.com/data/lite/latest\"\n",
    "# metadata mode (used to save metadata)\n",
    "metadata_mode = \"sqlite\"\n",
    "\n",
    "# Set the number of images to download\n",
    "num_images = 1000\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create folder structure\n",
    "The code creates the folder structure for the project. The folder structure is as follows:\n",
    "- output\n",
    "    - images\n",
    "    - metadata\n",
    "    - config\n",
    "\n",
    "This method creates a folder with the given path if it doesn't already exist, It also outputs a message to inform the user if the folder was created or if it already exists.\n",
    "This is useful for organizing and managing files in a project. By creating a folder to store data and resources, it keeps the working directory tidy and makes it easier to locate files. Additionally, by checking if the folder exists before creating it, it prevents the program from overwriting existing data or throwing an error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    This function creates a folder at the specified path.\n",
    "    If the folder already exists, it will print a message saying so.\n",
    "    If there is an error creating the folder, it will print the error message.\n",
    "\n",
    "    Parameters:\n",
    "        :param path (str): The path of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use os.mkdir to create the folder at the specified path\n",
    "        os.mkdir(path)\n",
    "        print(f\"Folder {path} created\")\n",
    "    except FileExistsError:\n",
    "        # If the folder already exists, print a message saying so\n",
    "        print(f\"Folder {path} already exists\")\n",
    "    except Exception as e:\n",
    "        # If there is an error creating the folder, print the error message\n",
    "        print(f\"Error creating folder {path}: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the folder structure\n",
    "This method initializes a list of folders by calling the create_folder method for each folder in the list.\n",
    "The purpose of this method is to make sure that all necessary folders exist before the program continues its execution.\n",
    "If a folder does not exist, the create_folder method will create it. If a folder already exists, the method will simply print a message indicating that the folder already exists. In case of any other error, the method will print the error message."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_folder(folder_names: list):\n",
    "    for folder_name in folder_names:\n",
    "        create_folder(folder_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init_folder(list_of_paths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define methods for downloading the dataset\n",
    "The following code block is a method to download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "The method starts by creating a session (s = requests.Session()) and then mounting it to the URL (s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))). This sets the maximum number of retries to 3 if the connection to the URL fails.\n",
    "\n",
    "Then, the method makes a GET request to the URL (r = s.get(url, stream=True, allow_redirects=True)) and checks if it returns a successful response (r.raise_for_status()). If there was an HTTP error during the request, the error message is printed (print(f\"HTTP error occurred while downloading dataset: {e}\")).\n",
    "\n",
    "The method also checks the file size specified in the response headers and assigns it to the variable file_size (file_size = int(r.headers.get('Content-Length', 0))). If the file size is 0, a default value of \"(Unknown total file size)\" is assigned to the variable desc; otherwise, the variable desc is left empty.\n",
    "\n",
    "Next, the method resolves the file path and creates a directory if it doesn't already exist (path.parent.mkdir(parents=True, exist_ok=True)). The method then creates a tqdm progress bar to show the download progress (with tqdm.tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:).\n",
    "\n",
    "Finally, the method writes the contents of the file to disk in chunks (for chunk in r.iter_content(chunk_size=1024):), updating the progress bar for each chunk that is written to disk (pbar.update(len(chunk))). If an error occurred during the download, a message with the error is printed (print(f\"Error occurred while downloading dataset: {e}\")). The file path is returned when the method is finished."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    \"\"\"\n",
    "    This download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "    Parameters:\n",
    "        :param url (str): The URL of the file to be downloaded.\n",
    "        :param filename (str): The filename to save the file as.\n",
    "\n",
    "    Returns:\n",
    "    path (str): The path of the downloaded file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a session object to persist the state of connection\n",
    "        s = requests.Session()\n",
    "        # Retry connecting to the URL up to 3 times\n",
    "        s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        # Send a GET request to the URL to start the download\n",
    "        r = s.get(url, stream=True, allow_redirects=True)\n",
    "        # Raise an error if the response is not 200 OK\n",
    "        r.raise_for_status()\n",
    "        # Get the file size from the Content-Length header, default to 0 if not present\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        # Get the absolute path to the target file\n",
    "        path = pathlib.Path(filename).expanduser().resolve()\n",
    "        # Create parent directories if they don't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Set the description to display while downloading, \"(Unknown total file size)\" if file size is 0\n",
    "        desc = \"(Unknown total file size)\" if file_size == 0 else \"\"\n",
    "        # Enable decoding the response content\n",
    "        r.raw.read = functools.partial(r.raw.read, decode_content=True)\n",
    "        # Use tqdm to display the download progress\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "            # Open the target file in binary write mode\n",
    "            with path.open(\"wb\") as f:\n",
    "                # Write each chunk of data from the response to the file\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        # Return the path to the downloaded file\n",
    "        return path\n",
    "    # Handle HTTP error if the response is not 200 OK\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading dataset: {e}\")\n",
    "    # Handle any other exceptions that might occur while downloading the file\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading dataset: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download the dataset\n",
    "The following code block downloads the dataset from the URL and saves it to the specified filename. The method also prints a message to inform the user that the download is complete."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def download_dataset(dataset_url, image_path):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from the given URL, unzips it, and stores the images in the specified image path.\n",
    "\n",
    "    Args:\n",
    "        :param dataset_url (str): URL of the dataset to be downloaded\n",
    "        :param image_path (str): Path to store the images after unzipping the dataset\n",
    "    \"\"\"\n",
    "    # Check if the dataset has already been downloaded\n",
    "    # Check if the archive.zip file exists or if the images folder is empty\n",
    "    if not os.path.exists('archive.zip'):\n",
    "        # Download the dataset from the given url\n",
    "        download(dataset_url, 'archive.zip')\n",
    "        print(\"Dataset downloaded!\")\n",
    "        try:\n",
    "            # Extract the contents of the archive.zip to the specified image path\n",
    "            with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(image_path)\n",
    "            print(\"Dataset unzipped\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while unzipping dataset: {e}\")\n",
    "        try:\n",
    "            # Remove the archive.zip file\n",
    "            os.remove('archive.zip')\n",
    "            print(\"archive.zip removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while removing archive.zip: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "download_dataset(dataset_url, images_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Allow the notebook to run asynchronously"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read photo.tsv file in images folder\n",
    "photo_df = pd.read_csv(os.path.join(images_path, 'photos.tsv000'), sep='\\t')\n",
    "# read photo_image_url column and photo_id in index\n",
    "photo_df = photo_df[['photo_id', 'photo_image_url']]\n",
    "\n",
    "print(photo_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method downloads an image from a given URL using an asynchronous HTTP client library called aiohttp. The downloaded image is saved to a file on the local file system with a filename in the format \"image_#index.jpg\", where #index is a given integer value.\n",
    "\n",
    "The method takes four arguments:\n",
    "\n",
    "session: an instance of an aiohttp client session that manages HTTP requests and responses.\n",
    "url: a string representing the URL of the image to download.\n",
    "i: an integer representing the index of the image to download.\n",
    "err_cnt: an optional integer representing the number of times that the download has failed due to a client error. If this is not provided, it defaults to 0.\n",
    "The method first checks whether an error count was provided, and if not, sets it to 0. It then attempts to download the image using the aiohttp session.get() method, which returns a response object. The async with statement ensures that the response is properly handled and that the connection to the server is closed when the request is completed.\n",
    "\n",
    "Once the response is obtained, the method constructs a filename using the given index value, and writes the image content to a file with that filename in binary mode. It then prints a message indicating that the image was downloaded and saved to the specified filename.\n",
    "\n",
    "If the download fails due to a client error (e.g., a network timeout), the method catches the error using an except block, prints an error message indicating the URL that failed and the error that occurred, and then waits for 10 seconds before retrying the download. If the error count reaches 10, the method returns without attempting to download the image again.\n",
    "\n",
    "If the download fails due to a server error (e.g., a 404 Not Found response), the exception is not caught and will propagate up the call stack.\n",
    "\n",
    "If the download is successful, the method returns nothing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def download_image(session: aiohttp.ClientSession, url: str, i: int, err_cnt=None):\n",
    "    \"\"\"\n",
    "    Downloads an image from the given URL using an aiohttp client session and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        session: An aiohttp client session that manages HTTP requests and responses.\n",
    "        url: The URL of the image to download.\n",
    "        i: An integer representing the index of the image to download.\n",
    "        err_cnt: An optional integer representing the number of times that the download has failed due to a client error.\n",
    "                 If not provided, it defaults to 0.\n",
    "\n",
    "    Raises:\n",
    "        This method does not raise any exceptions.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    if err_cnt is None:\n",
    "        err_cnt = 0\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            filename = os.path.join(images_path, \"image_\" + str(i) + \".jpg\")\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(await response.content.read())\n",
    "            print(f\"Downloaded {url} to {filename} idx: {i}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Error occurred while downloading {url}: {e}\")\n",
    "        if err_cnt == 10:\n",
    "            return\n",
    "        await asyncio.sleep(10)\n",
    "        err_cnt += 1\n",
    "        await download_image(session, url, i, err_cnt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method, download_images, is a function that downloads a list of images from the web using the aiohttp library and saves them to the local file system. The method takes two arguments: image_urls, a list of URLs where the images are hosted, and images_ids, a list of identifiers for the images that will be used to name the files when they are saved locally.\n",
    "\n",
    "The method starts by creating an aiohttp.ClientSession object, which is used to manage HTTP requests and responses. It then initializes an empty list tasks and creates a semaphore object with a maximum value of 5000. The semaphore is used to limit the number of concurrent downloads and prevent overloading the server.\n",
    "\n",
    "Next, the method loops through the image_urls list using the built-in enumerate function to keep track of the index of each URL. For each URL, the method tries to acquire a permit from the semaphore to start a new download. If the maximum number of concurrent downloads has been reached, the method blocks until a permit becomes available.\n",
    "\n",
    "Once a permit is acquired, the method creates a new task using the asyncio.ensure_future function and adds it to the tasks list. The task represents the asynchronous download of the image from the current URL using the download_image method. A callback function is also added to the task that releases the semaphore permit when the task completes, so that another download can start.\n",
    "\n",
    "If an error occurs while trying to download an image, the method prints an error message and releases the semaphore permit. The method then continues with the next URL in the list.\n",
    "\n",
    "After all tasks have been created and added to the tasks list, the method waits for them to complete using the asyncio.wait function. Finally, it gathers all the completed tasks using the asyncio.gather function, which ensures that all tasks have finished before the method returns.\n",
    "\n",
    "Overall, the download_images method is an efficient and asynchronous way to download a large number of images from the web and save them to the local file system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def download_images(image_urls, images_ids):\n",
    "    \"\"\"\n",
    "    Downloads a list of images from the given URLs using an aiohttp client session and saves them to the local file system.\n",
    "\n",
    "    Args:\n",
    "        image_urls: A list of strings representing the URLs of the images to download.\n",
    "        images_ids: A list of integers representing the indices of the images to download.\n",
    "\n",
    "    Raises:\n",
    "        This method does not raise any exceptions.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Create a new aiohttp client session to manage HTTP requests and responses\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []  # Create an empty list to hold the tasks that will download the images\n",
    "        semaphore = asyncio.Semaphore(5000)  # Create a semaphore to limit the number of concurrent downloads\n",
    "        # Loop through the image URLs and create a new task for each one\n",
    "        for i, url in enumerate(image_urls):\n",
    "            try:\n",
    "                await semaphore.acquire()  # Acquire a permit from the semaphore to limit concurrency\n",
    "                #url = url + \"?w=1000&fm=jpg&fit=max\"  # Append query parameters to resize and optimize the image\n",
    "                task = asyncio.ensure_future(download_image(session, url, images_ids[i]))  # Create a new download task\n",
    "                task.add_done_callback(\n",
    "                    lambda x: semaphore.release())  # Release the semaphore permit when the task completes\n",
    "                tasks.append(task)  # Add the task to the list of download tasks\n",
    "            except Exception:\n",
    "                print(f\"Error occurred while downloading {url}\")\n",
    "                semaphore.release()  # Release the semaphore permit if an exception occurs\n",
    "        # Wait for all download tasks to complete\n",
    "        await asyncio.wait(tasks)\n",
    "        # Gather the results of all download tasks (not necessary because the tasks have already completed)\n",
    "        await asyncio.gather(*tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the list of image urls and image ids\n",
    "image_urls = photo_df['photo_image_url'].values.tolist()[:num_images]\n",
    "# img id are from 0 to size of the list\n",
    "images_ids = [i for i in range(len(image_urls))][:num_images]\n",
    "# filter by looking if the image already exist in fact of the image_id is already in the folder\n",
    "# Loop on the image_id and check if the image exist in the folder\n",
    "image_urls = [url for url, image_id in zip(image_urls, images_ids) if\n",
    "              not os.path.exists(os.path.join(images_path, \"image_\" + str(image_id) + \".jpg\"))]\n",
    "print(f\"Number of images to download: {len(image_urls)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the list of image urls into chunks of max and add a timeout of 30 seconds\n",
    "chunks = [image_urls[i:i + 5000] for i in range(0, len(image_urls), 5000)]\n",
    "start_t = time.time()\n",
    "loop = None\n",
    "for i, chunk in enumerate(chunks):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.run_until_complete(download_images(chunk, images_ids[i * 5000:(i + 1) * 5000]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading chunk {i}: {e}\")\n",
    "    finally:\n",
    "        loop.close()\n",
    "        print(f\"[Chunk {i}] Downloaded {len(chunk)} images in {time.time() - start} seconds\")\n",
    "\n",
    "print(f'Downloaded {len(image_urls)} images in {time.time() - start_t} seconds')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clear the images folder from all files except images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove all files except images\n",
    "for file in os.listdir(images_path):\n",
    "    if file.endswith('.jpg'):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            # Don't delete TERMS.md\n",
    "            if file == 'TERMS.md':\n",
    "                continue\n",
    "            os.remove(os.path.join(images_path, file))\n",
    "        except Exception as e:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define methods to get all the image paths\n",
    "The get_all_images method is used to retrieve all images present in the specified image path. It uses the os.walk function to traverse through all subdirectories within the image path and collects the file names that end with either '.png' or '.jpg' extensions. The full path of each image is then generated by joining the root directory and the file name. The method returns a list of all images' full paths. In case of any error, an error message is printed and an empty list is returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_all_images(path):\n",
    "    \"\"\"Get all images from the given path.\n",
    "\n",
    "    Args:\n",
    "    param: image_path (str): path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "    - list: a list of full path to all the images with png or jpg extensions.\n",
    "    - empty list: an empty list if an error occurred while fetching images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use os.walk to traverse all the subdirectories and get all images\n",
    "        return [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(path)\n",
    "                for name in files\n",
    "                if name.endswith((\".png\", \".jpg\"))]\n",
    "    except Exception as e:\n",
    "        # return an empty list and log the error message if an error occurred\n",
    "        print(f\"An error occurred while fetching images: {e}\")\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define methods to get metadata\n",
    "The goal of the get_metadata method is to extract metadata information from a list of image files and return it in a dictionary format. This method uses the ExifTool software to extract the metadata information from the images. The input parameter is a string containing all image file paths separated by a space. The output of the method is a dictionary containing the metadata information of the images. If an error occurs for any image, the metadata for that image will be None. This method is implemented as an asynchronous coroutine using the asyncio module in Python.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function takes a list of dictionaries containing metadata information of images and generates a list of SQL requests to insert the metadata into a database.\n",
    "\n",
    "The function first creates an empty list to store the SQL requests. It then loops over each metadata dictionary in the input list using the tqdm function to provide a progress bar. For each metadata dictionary, the function extracts the filename of the image and then loops over all the items in the dictionary.\n",
    "\n",
    "For each key-value pair in the metadata dictionary, the function creates an SQL request to insert the metadata into the database. The SQL request is in the form of a string that contains the filename, key, and value of the metadata item. The function adds each SQL request to the list of SQL requests.\n",
    "\n",
    "After processing all the metadata dictionaries, the function returns the list of SQL requests."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_sql_requests(metadatas):\n",
    "    \"\"\"\n",
    "    This function generates a list of SQL requests to insert metadata into a database.\n",
    "\n",
    "    Parameters:\n",
    "    metadatas (list): A list of dictionaries containing the metadata information of the images.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of SQL requests to insert metadata into a database.\n",
    "    \"\"\"\n",
    "    # Create a list to store SQL requests\n",
    "    sql_requests = []\n",
    "    # Loop over all metadata\n",
    "    for i, metadata in enumerate(metadatas):\n",
    "        try:\n",
    "            # Get the filename of the image\n",
    "            filename = metadatas[i]['filename']\n",
    "\n",
    "            # Loop over all metadata items\n",
    "            for key, value in metadatas[i].items():\n",
    "                # Create SQL request to insert metadata into database\n",
    "                # replace \" by space\n",
    "                value = value.replace('\"', ' ')\n",
    "                # replace ' by space\n",
    "                value = value.replace(\"'\", ' ')\n",
    "\n",
    "                sql_request = f\"INSERT INTO metadata VALUES ('{filename}', '{key}', '{value}')\"\n",
    "                # Add SQL request to list\n",
    "                sql_requests.append(sql_request)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print an error message if an error occurs\n",
    "            print(\"An error occurred while generating SQL requests: \", e)\n",
    "            continue\n",
    "    # Return the list of SQL requests\n",
    "    return sql_requests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method executes a SQL query on a SQLite database. The method takes a single parameter, queries, which should be a valid SQL queries that is compatible with the SQLite database.\n",
    "\n",
    "The method first establishes a connection to the SQLite database using the connect() method of the sqlite3 module in Python. It then executes the SQL query using the execute() method of the connection object. After executing the query, the method commits the changes to the database using the commit() method of the connection object, and then closes the connection using the close() method of the connection object.\n",
    "\n",
    "This method is typically used to insert metadata information into a SQLite database. It assumes that the SQLite database already exists and is located in the metadata_path directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_server_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            passwd=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "        print(\"MySQL Database connection successful\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "\n",
    "    return connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an asynchronous coroutine get_all_metadata that extracts metadata from all images in a directory and saves the metadata information in either pickle or JSON format. The function takes two parameters: image_path which is the path to the directory where the images are stored, and metadata_path which is the path to the directory where the metadata will be saved.\n",
    "\n",
    "Firstly, the function retrieves a list of all images in the directory using the get_all_images method. It then creates a Semaphore object with a limit of 5000 to limit the number of simultaneous coroutines to 5000.\n",
    "\n",
    "A progress bar is created using the tqdm_asyncio library to track the progress of processing all images. A list of coroutines is created using a list comprehension with each coroutine being an instance of the get_metadata method applied to each image file in the directory.\n",
    "\n",
    "The coroutines are then executed concurrently using the asyncio.as_completed method, with a maximum of 5000 coroutines being executed at a time, and their results are appended to a metadatas list. For each successfully extracted metadata, a SQL query is generated and executed to insert the metadata into the database using the gen_sql_requests and execute_query functions.\n",
    "\n",
    "Once all coroutines are completed, the metadata information is saved into the database. If an error occurs during the metadata extraction process, the None value is returned for that image and the error message is printed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def get_all_metadata(images_path):\n",
    "    \"\"\"\n",
    "    This coroutine extracts metadata from all images in a directory and saves the metadata information in either pickle or json format.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the directory where the images are stored.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Use the binary exifextract from include path\n",
    "    binary = include_path + '/exifextract'\n",
    "    command = [binary, images_path, metadata_path + '/metadata.csv']\n",
    "    import subprocess\n",
    "    # execute command\n",
    "    popen = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "    popen.wait()\n",
    "\n",
    "    # wait for the process to terminate\n",
    "    output, error = popen.communicate()\n",
    "\n",
    "    while popen.poll() is None:\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # check if the process terminated successfully\n",
    "    if popen.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(popen.returncode, command)\n",
    "\n",
    "    # load metadata from csv\n",
    "    with open(metadata_path + '/metadata.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        metadata = list(reader)\n",
    "        header = metadata[0]\n",
    "\n",
    "    metadata = metadata[1:]\n",
    "    metadata_dict = {}\n",
    "    for i, row in enumerate(metadata):\n",
    "        metadata_dict[i] = {}\n",
    "        for j in range(1, len(header)):\n",
    "            metadata_dict[i][header[j]] = row[j]\n",
    "        # add filename to metadata\n",
    "        # remove ' from row[0]\n",
    "        row[0] = row[0].replace(\"'\", '')\n",
    "        metadata_dict[i]['filename'] = row[0]\n",
    "\n",
    "\n",
    "    # save metadata to database\n",
    "    print(\"Generating SQL requests...\")\n",
    "    queries = gen_sql_requests(metadata_dict)\n",
    "    # save to file queries.sql\n",
    "    with open(metadata_path + '/queries.sql', 'w') as f:\n",
    "        f.write(\";\\n\".join(queries))\n",
    "\n",
    "\n",
    "    print(\"Saving metadata to database...\")\n",
    "    # TODO: Save to database\n",
    "    conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "    # Execute file to database\n",
    "    cursor = conn.cursor()\n",
    "    #cursor.execute(\"DROP TABLE IF EXISTS metadata\")\n",
    "    #cursor.execute(\"CREATE TABLE metadata (filename VARCHAR(255), key VARCHAR(255), value VARCHAR(255))\")\n",
    "    cursor.execute(\"LOAD DATA LOCAL INFILE '\" + metadata_path + \"/queries.sql' INTO TABLE metadata FIELDS TERMINATED BY ';' LINES TERMINATED BY '\\n'\")\n",
    "    conn.commit()\n",
    "    conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "asyncio.run(get_all_metadata(images_path))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to look at the metadata (sqlite format)\n",
    "This is the way to look at the metadata information of an image in sqlite database format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM metadata WHERE filename = 'image_0.jpg'\")\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for x in result:\n",
    "    print(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "This code imports several libraries in order to perform some data processing tasks. The libraries used are:\n",
    "import pickle: imports the pickle module that provides a way to serialize and deserialize Python objects.\n",
    "\n",
    "import os: imports the os module that provides a way to interact with the operating system.\n",
    "\n",
    "import sqlite3: imports the sqlite3 module that provides a way to work with SQLite databases.\n",
    "\n",
    "import json: imports the json module that provides a way to encode and decode JSON data.\n",
    "\n",
    "import cv2: imports the cv2 module which is an OpenCV library for image processing and computer vision tasks.\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans: imports the MiniBatchKMeans class from the sklearn.cluster module which provides a way to perform KMeans clustering on a large dataset.\n",
    "\n",
    "from tqdm import tqdm: imports the tqdm module which provides a progress bar for long-running operations.\n",
    "\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection: imports the DetrImageProcessor and DetrForObjectDetection classes from the transformers module which provides a way to perform object detection using the DETR (DEtection TRansformer) model.\n",
    "\n",
    "import torch: imports the torch module which is a PyTorch library for machine learning and deep learning tasks.\n",
    "\n",
    "from PIL import Image: imports the Image class from the PIL module which provides a way to manipulate and analyze image data.\n",
    "\n",
    "With these libraries, you should be able to perform a wide range of data processing and analysis tasks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install opencv-python scikit-learn tqdm transformers torch Pillow python-dotenv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from tqdm import tqdm\n",
    "from transformers import DetrImageProcessor, DetrForObjectDetection\n",
    "import torch\n",
    "from PIL import Image\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings base variables and paths\n",
    "This code sets up the file structure and URL's for a project that uses data from an image dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "config_path = os.path.join(output_path, \"config\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, config_path]\n",
    "\n",
    "# Set the base URL for the dataset\n",
    "metadata_extension = \"sqlite\"\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create folder structure\n",
    "The code creates the folder structure for the project. The folder structure is as follows:\n",
    "- output\n",
    "    - images\n",
    "    - metadata\n",
    "    - config\n",
    "\n",
    "This method creates a folder with the given path if it doesn't already exist, It also outputs a message to inform the user if the folder was created or if it already exists.\n",
    "This is useful for organizing and managing files in a project. By creating a folder to store data and resources, it keeps the working directory tidy and makes it easier to locate files. Additionally, by checking if the folder exists before creating it, it prevents the program from overwriting existing data or throwing an error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    This function creates a folder at the specified path.\n",
    "    If the folder already exists, it will print a message saying so.\n",
    "    If there is an error creating the folder, it will print the error message.\n",
    "\n",
    "    Parameters:\n",
    "        :param path (str): The path of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use os.mkdir to create the folder at the specified path\n",
    "        os.mkdir(path)\n",
    "        print(f\"Folder {path} created\")\n",
    "    except FileExistsError:\n",
    "        # If the folder already exists, print a message saying so\n",
    "        print(f\"Folder {path} already exists\")\n",
    "    except Exception as e:\n",
    "        # If there is an error creating the folder, print the error message\n",
    "        print(f\"Error creating folder {path}: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the folder structure\n",
    "This method initializes a list of folders by calling the create_folder method for each folder in the list.\n",
    "The purpose of this method is to make sure that all necessary folders exist before the program continues its execution.\n",
    "If a folder does not exist, the create_folder method will create it. If a folder already exists, the method will simply print a message indicating that the folder already exists. In case of any other error, the method will print the error message."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_folder(folder_names: list):\n",
    "    for folder_name in folder_names:\n",
    "        create_folder(folder_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "init_folder(list_of_paths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define methods to get all the image paths\n",
    "The get_all_images method is used to retrieve all images present in the specified image path. It uses the os.walk function to traverse through all subdirectories within the image path and collects the file names that end with either '.png' or '.jpg' extensions. The full path of each image is then generated by joining the root directory and the file name. The method returns a list of all images' full paths. In case of any error, an error message is printed and an empty list is returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_all_images(path):\n",
    "    \"\"\"Get all images from the given path.\n",
    "\n",
    "    Args:\n",
    "    param: image_path (str): path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "    - list: a list of full path to all the images with png or jpg extensions.\n",
    "    - empty list: an empty list if an error occurred while fetching images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use os.walk to traverse all the subdirectories and get all images\n",
    "        return [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(path)\n",
    "                for name in files\n",
    "                if name.endswith((\".png\", \".jpg\"))]\n",
    "    except Exception as e:\n",
    "        # return an empty list and log the error message if an error occurred\n",
    "        print(f\"An error occurred while fetching images: {e}\")\n",
    "        return []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Facebook DETR model (detr-resnet-101)\n",
    "\n",
    "The detect_with_transformers function takes an image file path as an input, then uses a pre-trained model called DEtection TRansformer (DETR) to detect objects within the image.\n",
    "\n",
    "The function first opens the input image using the Python Imaging Library (PIL) Image.open method. Then it instantiates two components of the DETR model: a DetrImageProcessor and a DetrForObjectDetection model. The DetrImageProcessor is responsible for processing the input image into a format that can be fed into the DetrForObjectDetection model. The DetrForObjectDetection model then takes the processed image and performs object detection by predicting bounding boxes and class labels for each detected object.\n",
    "\n",
    "Once the model has made its predictions, the function uses the processor.post_process_object_detection method to convert the bounding box and class label predictions into a format that is compatible with the Common Objects in Context (COCO) dataset. This conversion is necessary in order to use the COCO API, which provides a common framework for evaluating object detection models.\n",
    "\n",
    "The function then filters the detected objects by only keeping those with a confidence score above a certain threshold (0.9 in this case), and extracts the corresponding class labels. Finally, the function prints out a message for each detected object, indicating its class label, confidence score, and location within the image. The function returns a list of the detected object class labels."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def detect_with_transformers(image):\n",
    "    \"\"\"\n",
    "    This function detects objects in an image using the DETR (DEtection TRansformer) model by Facebook.\n",
    "\n",
    "    Args:\n",
    "    image: A string representing the path of the image to be processed.\n",
    "\n",
    "    Returns:\n",
    "    A list containing the labels of the detected objects in the image.\n",
    "\n",
    "    Raises:\n",
    "    None.\n",
    "    \"\"\"\n",
    "    #image = Image.open(image)\n",
    "    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\")\n",
    "    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # convert outputs (bounding boxes and class logits) to COCO API\n",
    "    # let's only keep detections with score > 0.9\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n",
    "    labels = []\n",
    "    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "        box = [round(i, 2) for i in box.tolist()]\n",
    "        labels.append(model.config.id2label[label.item()])\n",
    "        #print(\n",
    "        #    f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        #    f\"{round(score.item(), 3)} at location {box}\"\n",
    "        #)\n",
    "    return labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Save  metadata\n",
    "\n",
    "The function save_metadata allows you to save metadata information of an image in either pickle, json, or sqlite format. The function takes four parameters: metadata, img_name, metadata_path, and save_format.\n",
    "\n",
    "metadata is a dictionary that contains the metadata information of an image. img_name is a string that represents the file name of the image. metadata_path is a string that specifies the path to the directory where the metadata will be saved. save_format is an optional parameter that specifies the format in which the metadata will be saved. The default value is pickle.\n",
    "\n",
    "The function saves the metadata in the specified format. If save_format is set to pickle, the metadata is saved in the pickle format. If save_format is set to json, the metadata is saved in the json format. If save_format is set to sqlite, the metadata is saved in the sqlite database.\n",
    "\n",
    "If an error occurs while saving the metadata, the function will print an error message indicating the image name and the error that occurred.\n",
    "\n",
    "The function does not return any value."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_server_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            passwd=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "\n",
    "    return connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_metadata(metadata, img_name):\n",
    "    \"\"\"\n",
    "    This function saves the metadata information of an image in either pickle or json format.\n",
    "    Parameters:\n",
    "    metadata (dict): The metadata information of an image.\n",
    "    img_name (str): The file name of the image.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "    save_format (str): The format in which the metadata will be saved. The default is 'pickle'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get only the file name of the image\n",
    "        img_name = os.path.basename(img_name)\n",
    "        # img_name = img_name + '.jpg'\n",
    "        img_name = img_name.split('.')[0] + '.jpg'\n",
    "        # Open a connection to the database\n",
    "        conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "        # Create a cursor\n",
    "        c = conn.cursor()\n",
    "        # Insert the metadata into the table\n",
    "        for key, value in metadata.items():\n",
    "            # Convert key, value to string\n",
    "            key = str(key)\n",
    "            value = str(value)\n",
    "            # Check if the key is already in the table\n",
    "            c.execute(\"SELECT * FROM metadata WHERE filename=? AND key=?\", (img_name, key))\n",
    "            # If the key is already in the table, update the value\n",
    "            if c.fetchone():\n",
    "                c.execute(\"UPDATE metadata SET value=? WHERE filename=? AND key=?\", (value, img_name, key))\n",
    "                # Commit the changes\n",
    "                conn.commit()\n",
    "            # If the key is not in the table, insert the key, value pair\n",
    "            else:\n",
    "                c.execute(\"INSERT INTO metadata VALUES (?, ?, ?)\", (img_name, key, value))\n",
    "                # Commit the changes\n",
    "                conn.commit()\n",
    "        # Close the connection\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        # print an error message if an error occurs\n",
    "        print(f\"An error occurred while saving metadata for {img_name}: {str(e)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read SQLite metadata\n",
    "\n",
    "The read_sqlite method is a function used to read metadata information from a SQLite database. The method takes two parameters: metadata_path, the path to the directory where the metadata is saved, and filename, the name of the file for which the metadata is to be retrieved.\n",
    "\n",
    "The method starts by connecting to the SQLite database located at metadata_path/metadata.db using the sqlite3.connect method. A cursor is then created to allow interaction with the database. If the metadata table doesn't exist in the database, it is created.\n",
    "\n",
    "The metadata for the specified filename is retrieved from the database by executing a SQL query that selects all rows where the filename column is equal to the filename parameter. The retrieved metadata is stored in a dictionary, where the keys are taken from the key column and the values from the value column.\n",
    "\n",
    "Finally, the database connection is closed and the metadata dictionary is returned as the result of the function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_mariadb(filename):\n",
    "    # Open a connection to the database\n",
    "    conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "    # Create a table if it doesn't exist : filename, mkey, mvalue\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS metadata (filename text, mkey text, mvalue text)''')\n",
    "    # Insert the metadata into the table\n",
    "    c.execute(\"SELECT * FROM metadata WHERE filename=?\", (filename,))\n",
    "    # If the key is already in the table, update the value\n",
    "    metadata = {}\n",
    "    for row in c.fetchall():\n",
    "        metadata[row[1]] = row[2]\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    return metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Set tags in metadata\n",
    "This function \"update_tags\" is used to run the YOLOv3 algorithm on a set of images, update the metadata of each image with the detected labels (tags) and save the updated metadata.\n",
    "\n",
    "The function takes 3 parameters:\n",
    "\n",
    "images: a list of file paths for the images that need to be processed.\n",
    "metadata_path: a file path to the directory where the metadata files are stored.\n",
    "save_format: the format of the metadata files. Can be either 'pickle' or 'sqlite'.\n",
    "The function uses the tqdm library to display a progress bar for the image processing. For each image, the function tries to retrieve its metadata based on the save_format. If the metadata file format is 'sqlite', the function calls the read_sqlite function to retrieve the metadata. If the metadata file format is 'pickle', the function reads the metadata file directly.\n",
    "\n",
    "If the metadata already contains a \"tags\" key, it means that the image has already been processed and its metadata has been updated with the labels, so the function skips that image.\n",
    "\n",
    "The function then calls the detect function to run the YOLOv3 algorithm on the image and retrieve the labels (tags). The labels are added to the metadata under the \"tags\" key.\n",
    "\n",
    "Finally, the function calls the save_metadata function to save the updated metadata. If an error occurs while processing an image (e.g. the metadata file is not found), the function prints an error message and continues processing the next image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_sql(metadata):\n",
    "    \"\"\"Generate SQL query to insert metadata into the database\"\"\"\n",
    "    sql = \"INSERT INTO metadata (filename, mkey, mvalue) VALUES \"\n",
    "    for filename, data in metadata.items():\n",
    "        for key, value in data.items():\n",
    "            sql += f\"('{filename}', '{key}', '{value}'),\"\n",
    "\n",
    "    return sql[:-1]\n",
    "\n",
    "\n",
    "def update_tags(images):\n",
    "    # Run the YOLOv3 algorithm on each image\n",
    "    # display progress bar in the first thread only\n",
    "    metadata = {}\n",
    "    for image in tqdm(images, desc=\"Updating tags\"):\n",
    "        # read pickle file from ../output/metadata/file_name.pkl\n",
    "        file_name = os.path.basename(image)\n",
    "        file_name, ext = file_name.split(\".\")\n",
    "        try:\n",
    "            image = Image.open(image)\n",
    "            # resize image to 416x416\n",
    "            image = image.resize((416, 416))\n",
    "            labels = detect_with_transformers(image)\n",
    "            image.close()\n",
    "\n",
    "            # Remove duplicates from labels\n",
    "            labels = list(set(labels))\n",
    "            # add labels to metadata\n",
    "            metadata[file_name + '.jpg'] = {\"tags\": labels}\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found: \", file_name)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    sql = gen_sql(metadata)\n",
    "    # Save metadata to tags_queries.sql\n",
    "    with open(\"tags_queries.sql\", \"w\") as f:\n",
    "        f.write(sql)\n",
    "\n",
    "    # TODO: Save to database\n",
    "    conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "    # Execute file to database\n",
    "    cursor = conn.cursor()\n",
    "    #cursor.execute(\"DROP TABLE IF EXISTS metadata\")\n",
    "    #cursor.execute(\"CREATE TABLE metadata (filename VARCHAR(255), key VARCHAR(255), value VARCHAR(255))\")\n",
    "    cursor.execute(\n",
    "        \"LOAD DATA LOCAL INFILE '\" + os.getcwd() + \"/tags_queries.sql' INTO TABLE metadata FIELDS TERMINATED BY ',' ENCLOSED BY '(' LINES TERMINATED BY ')';\")\n",
    "    conn.commit()\n",
    "    conn.close()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the list of images\n",
    "images = os.listdir(images_path)\n",
    "images = [os.path.join(images_path, image) for image in images]\n",
    "\n",
    "update_tags(images, metadata_path, save_format='sqlite')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "    ### Now, find dominant colors in the images\n",
    "The functions rgb_to_hex and find_dominant_colors are used to find the dominant colors in an image.\n",
    "\n",
    "The function rgb_to_hex takes in an RGB array with 3 values, and returns the hexadecimal representation of the color. This can be useful for formatting colors in a standardized way, as hexadecimal codes are widely used in web development and other applications.\n",
    "\n",
    "The function find_dominant_colors takes in an image and optional parameters k and image_processing_size. The k parameter specifies the number of dominant colors to return, with a default value of 4. The image_processing_size parameter allows you to resize the image to a smaller size, to speed up the processing, if desired.\n",
    "\n",
    "The image is first converted from BGR to RGB, and then reshaped into a list of pixels. The KMeans algorithm is used to cluster the pixels into k clusters, and the most popular clusters are identified. The color values for each of the k clusters are converted to hexadecimal representation and returned as a list, along with the percentage of the image covered by each color."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % (int(rgb[0]), int(rgb[1]), int(rgb[2]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_dominant_colors(image_path, k=4, downsample=2, resize=(200, 200)):\n",
    "    # Load image and convert to RGB\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Downsample the image\n",
    "    image = cv2.resize(image, (image.shape[1] // downsample, image.shape[0] // downsample))\n",
    "\n",
    "    # Resize the image if requested\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image, resize)\n",
    "\n",
    "    # Flatten the image\n",
    "    image_flat = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "\n",
    "    # Cluster the pixels using KMeans and find percentage of image covered by each color\n",
    "    clt = MiniBatchKMeans(n_clusters=k, n_init=10, batch_size=100, random_state=42)\n",
    "    labels = clt.fit_predict(image_flat)\n",
    "\n",
    "    # Count the number of pixels assigned to each cluster\n",
    "    counts = np.bincount(labels)\n",
    "\n",
    "    # Calculate the percentage of pixels assigned to each cluster\n",
    "    percentages = counts / len(labels)\n",
    "\n",
    "    # Get the dominant colors\n",
    "    dominant_colors = clt.cluster_centers_\n",
    "\n",
    "    # Convert to hexadecimal format\n",
    "    dominant_colors_hex = [rgb_to_hex(color) for color in dominant_colors]\n",
    "\n",
    "    # Combine the dominant colors and their percentages into a array of tuples\n",
    "    result = list(zip(dominant_colors_hex, percentages))\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function takes a list of dictionaries containing metadata information of images and generates a list of SQL requests to insert the metadata into a database.\n",
    "\n",
    "The function first creates an empty list to store the SQL requests. It then loops over each metadata dictionary in the input list using the tqdm function to provide a progress bar. For each metadata dictionary, the function extracts the filename of the image and then loops over all the items in the dictionary.\n",
    "\n",
    "For each key-value pair in the metadata dictionary, the function creates an SQL request to insert the metadata into the database. The SQL request is in the form of a string that contains the filename, key, and value of the metadata item. The function adds each SQL request to the list of SQL requests.\n",
    "\n",
    "After processing all the metadata dictionaries, the function returns the list of SQL requests."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_sql_requests(filenames, colors):\n",
    "    \"\"\"\n",
    "    This function generates a list of SQL requests to insert metadata into a database.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of SQL requests to insert metadata into a database.\n",
    "    \"\"\"\n",
    "    # Create a list to store SQL requests\n",
    "    sql_requests = []\n",
    "\n",
    "    # Loop over all metadata\n",
    "    for filename, color in tqdm(zip(filenames, colors), desc=\"Generating SQL requests\"):\n",
    "        # Create SQL request to insert metadata into database (filename, key, value)\n",
    "        # format color to avoid errors with quote marks\n",
    "\n",
    "        sql_request = f\"INSERT INTO metadata VALUES ('{filename}', 'dominant_color', '{color}')\"\n",
    "        # Add SQL request to list\n",
    "        sql_requests.append(sql_request)\n",
    "\n",
    "    # Return the list of SQL requests\n",
    "    return sql_requests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method executes a SQL query on a SQLite database. The method takes a single parameter, query, which should be a valid SQL query that is compatible with the SQLite database.\n",
    "\n",
    "The method first establishes a connection to the SQLite database using the connect() method of the sqlite3 module in Python. It then executes the SQL query using the execute() method of the connection object. After executing the query, the method commits the changes to the database using the commit() method of the connection object, and then closes the connection using the close() method of the connection object.\n",
    "\n",
    "This method is typically used to insert metadata information into a SQLite database. It assumes that the SQLite database already exists and is located in the metadata_path directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code block is used to process images and find their dominant colors. The code first retrieves all the images present in the folder specified by the images_path variable. Then, it iterates over each image, reads the metadata associated with the image and finds its dominant color if it hasn't been calculated already.\n",
    "\n",
    "For each image, the code first reads the image using OpenCV's cv2.imread() function and stores the result in the img variable. The code then reads the metadata of the image. The type of metadata file (e.g. .json, .pkl, .sqlite) is specified by the metadata_extension variable. Based on the file extension, the code reads the metadata using either read_sqlite(), json.load(), or pickle.load() functions. If the metadata file is not found, the code continues to the next iteration of the loop, but if there is an error, it prints the error message and continues to the next iteration.\n",
    "\n",
    "If the metadata does not contain information about the dominant color of the image, the code calculates the dominant color by calling the find_dominant_colors() function. The result of the find_dominant_colors() function is then added to the metadata under the key \"dominant_color\". Finally, the updated metadata is saved using the save_metadata() function, which saves the metadata to the specified location using the specified file format (metadata_extension)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_all_colors(image_path):\n",
    "    \"\"\"\n",
    "    This coroutine extracts dominant colors from all images in a directory and saves the color information in the database.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the directory where the images are stored.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get a list of all images in the directory\n",
    "    img_files = get_all_images(image_path)\n",
    "    colors = []\n",
    "\n",
    "    # Create a progress bar to track the progress of processing all images\n",
    "    for img in tqdm(img_files, desc=\"Processing images (Aprox: 25 minutes\"):\n",
    "        try:\n",
    "            # Create a list of coroutines to extract metadata for all images\n",
    "            color = find_dominant_colors(img, downsample=2, resize=(100, 100))\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "            continue\n",
    "\n",
    "        if color:\n",
    "            # color to string to avoid errors with quote marks\n",
    "            color = str(color)\n",
    "            # replace quotes by double quotes\n",
    "            color = color.replace(\"'\", '\"')\n",
    "            colors.append(color)\n",
    "\n",
    "    img_files = [os.path.basename(img) for img in img_files]\n",
    "\n",
    "    queries = gen_sql_requests(img_files, colors)\n",
    "\n",
    "    # save queries in a file colors_queries.sql\n",
    "    with open('colors_queries.sql', 'w') as f:\n",
    "        f.write(\";\\n\".join(queries))\n",
    "\n",
    "    # TODO: Save to database\n",
    "    conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "    # Execute file to database\n",
    "    cursor = conn.cursor()\n",
    "    #cursor.execute(\"DROP TABLE IF EXISTS metadata\")\n",
    "    #cursor.execute(\"CREATE TABLE metadata (filename VARCHAR(255), key VARCHAR(255), value VARCHAR(255))\")\n",
    "    cursor.execute(\"LOAD DATA LOCAL INFILE '\" + os.path.join(os.getcwd(),\n",
    "                                                             'colors_queries.sql') + \"' INTO TABLE metadata FIELDS TERMINATED BY ';' LINES TERMINATED BY ';'\")\n",
    "    conn.commit()\n",
    "    conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_all_colors(images_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n",
    "\n",
    "The code above imports various Python modules and libraries for data processing, visualization, and analysis. Below is a description of each module and library imported:\n",
    "\n",
    "- `os`: Provides a way to interact with the operating system, such as navigating directories and working with files.\n",
    "- `ast`: Provides a way to parse Python code into an abstract syntax tree, which can be used to analyze and manipulate the code.\n",
    "- `spacy`: A library for natural language processing, including tasks such as tokenization, part-of-speech tagging, and named entity recognition.\n",
    "- `folium`: A library for creating interactive maps using the Leaflet JavaScript library.\n",
    "- `sqlite3`: A module for working with SQLite databases.\n",
    "- `squarify`: A library for generating treemaps, which visualize hierarchical data using nested rectangles.\n",
    "- `itertools`: Provides a collection of functions for working with iterators, such as combining multiple iterators or creating permutations.\n",
    "- `webcolors`: A library for working with CSS-style color strings.\n",
    "- `tqdm`: A library for creating progress bars for loops.\n",
    "- `pandas`: A library for data manipulation and analysis, including reading and writing data to/from various file formats.\n",
    "- `ipywidgets`: Provides interactive widgets for Jupyter notebooks and other IPython environments.\n",
    "- `matplotlib.pyplot`: A library for creating visualizations, including line plots, scatter plots, bar charts, and histograms.\n",
    "- `collections.Counter`: A container that keeps track of the frequency of elements in a collection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import spacy\n",
    "import folium\n",
    "import sqlite3\n",
    "import datetime\n",
    "import squarify\n",
    "import itertools\n",
    "import webcolors\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from geopy.geocoders import Nominatim\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings base variables and paths\n",
    "\n",
    "The code above sets the base folder path and creates four subdirectories within that base path: \"images\", \"metadata\", and \"config\". These subdirectories are created by joining the base path with their respective names using the os.path.join() function."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "config_path = os.path.join(output_path, \"config\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, config_path]\n",
    "whitelist = ['Make', 'DateTimeOriginal', 'ImageWidth', 'ImageHeight', 'filename', 'Artist', 'Latitude', 'Longitude', 'Orientation', 'tags']\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Get the metadata from db and sort it with list into a dictionary\n",
    "\n",
    "### Get the metadata from the database\n",
    "\n",
    "The function \"get_metadata_from_sqlite_DB\" is used to retrieve metadata from a SQLite database. It takes an optional argument \"db_name\" to specify the name of the database. It opens a connection to the database, creates a cursor, and retrieves metadata for the first file in the images' directory. The metadata is stored in a dictionary format, where the filename is the key and the metadata is a list of key-value pairs. The function then closes the connection and returns the dictionary of metadata."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_metadata_from_sqlite_db(db_name='metadata.db') :\n",
    "    \"\"\"\n",
    "    Get the metadata from the sqlite database\n",
    "\n",
    "    :param db_name: The name of the database\n",
    "    :return: A dictionary with the metadata\n",
    "    \"\"\"\n",
    "    # Open a connection to the database\n",
    "    conn = sqlite3.connect(os.path.join(metadata_path, db_name))\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Retrieve the metadata\n",
    "    c.execute(\"\"\"\n",
    "        SELECT filename, GROUP_CONCAT(key || '\\t' || value, '\\n') AS metadata\n",
    "        FROM metadata\n",
    "        GROUP BY filename;\n",
    "    \"\"\")\n",
    "    metadata = c.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Convert the metadata to a DataFrame\n",
    "    result = {}\n",
    "    for image in tqdm(metadata, desc=\"Get metadata from database\"):\n",
    "        try :\n",
    "            result[image[0]] = {}\n",
    "            props = image[1].split('\\n')\n",
    "            for prop in props:\n",
    "                if prop:\n",
    "                    k, value = prop.split('\\t')\n",
    "                    if k in whitelist:\n",
    "                        result[image[0]][k] = value\n",
    "        except Exception as e:\n",
    "            print(e, image)\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import mysql.connector\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_metadata_from_mariadb_db(db_name='bigdata', user='root', password='', host='localhost', port='3306'):\n",
    "    \"\"\"\n",
    "    Get the metadata from the MariaDB database\n",
    "\n",
    "    :param db_name: The name of the database\n",
    "    :param user: The username to connect to the database\n",
    "    :param password: The password to connect to the database\n",
    "    :param host: The hostname or IP address of the database server\n",
    "    :param port: The port number to connect to the database server\n",
    "    :return: A dictionary with the metadata\n",
    "    \"\"\"\n",
    "    # Open a connection to the database\n",
    "    conn = mysql.connector.connect(\n",
    "        user=user,\n",
    "        password=password,\n",
    "        host=host,\n",
    "        port=port,\n",
    "        database=db_name\n",
    "    )\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Retrieve the metadata\n",
    "    c.execute(\"\"\"\n",
    "        SELECT filename, GROUP_CONCAT(CONCAT(mkey, '\\t', mvalue) SEPARATOR '\\n') AS metadata\n",
    "        FROM metadata\n",
    "        GROUP BY filename;\n",
    "    \"\"\")\n",
    "    metadata = c.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Convert the metadata to a dictionary\n",
    "    result = {}\n",
    "    for image in tqdm(metadata, desc=\"Get metadata from database\"):\n",
    "        try:\n",
    "            result[image[0]] = {}\n",
    "            props = image[1].split('\\n')\n",
    "            for prop in props:\n",
    "                if prop:\n",
    "                    k, value = prop.split('\\t')\n",
    "                    if k in whitelist:\n",
    "                        result[image[0]][k] = value\n",
    "        except Exception as e:\n",
    "            print(e, image)\n",
    "\n",
    "    return result\n",
    "\n",
    "get_metadata_from_mariadb_db(sql_database, sql_user, sql_password, sql_host)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clean the metadata\n",
    "\n",
    "The function \"clean_metadata\" is used to clean the metadata. It takes a dictionary of metadata as an argument and returns a dictionary with the cleaned metadata. The function removes special characters from the 'Make' property values and removes the 'T' and '-' characters from the 'DateTime' property values."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean_metadata(metadata_to_clean):\n",
    "    \"\"\"\n",
    "    Clean the metadata\n",
    "    Remove special characters from the 'Make' property values\n",
    "    Remove the 'T' and '-' characters from the 'DateTime' property values\n",
    "\n",
    "    :param metadata_to_clean: The metadata to clean\n",
    "    :return: A dictionary with the cleaned metadata\n",
    "    \"\"\"\n",
    "    cln_meta = metadata_to_clean.copy()\n",
    "\n",
    "    # Clean 'Make' property values\n",
    "    try:\n",
    "\n",
    "        for file in tqdm(cln_meta, desc=\"Clean 'Make' property values\"):\n",
    "            if 'Make' in cln_meta[file]:\n",
    "                cln_meta[file]['Make'] = ''.join(filter(str.isalpha, cln_meta[file]['Make'])).replace('CORPORATION', '').replace('CORP', '').replace('COMPANY', '').replace('LTD', '').replace('IMAGING', '')\n",
    "    except Exception as e:\n",
    "        print(e, file)\n",
    "\n",
    "    try:\n",
    "        # Clean 'DateTime' property values\n",
    "        cpt, cpt_error = 0, 0\n",
    "        date_error = []\n",
    "\n",
    "        for file in tqdm(cln_meta, desc=\"Clean 'DateTime' property values\"):\n",
    "            if 'DateTimeOriginal' in cln_meta[file]:\n",
    "                date = cln_meta[file]['DateTimeOriginal']\n",
    "                try :\n",
    "                    if date is not None:\n",
    "                        tmp = date.replace('T', ' ').replace('-', ':').split('+')[0]\n",
    "                        cln_meta[file]['DateTimeOriginal'] = datetime.datetime.strptime(tmp[:19], '%Y:%m:%d %H:%M:%S')\n",
    "                        # if the year is after actual year, we assume that the date is wrong\n",
    "                        if cln_meta[file]['DateTimeOriginal'].year > datetime.datetime.now().year:\n",
    "                            date_error.append(cln_meta[file]['DateTimeOriginal'])\n",
    "                            cln_meta[file]['DateTimeOriginal'] = None\n",
    "                            cpt_error += 1\n",
    "                        else:\n",
    "                            cpt += 1\n",
    "                except ValueError:\n",
    "                    date_error.append(date)\n",
    "                    cln_meta[file]['DateTimeOriginal'] = None\n",
    "                    cpt_error += 1\n",
    "    except Exception as e:\n",
    "        print(e, file)\n",
    "\n",
    "    print(f\"Metadata cleaned ! {cpt}/{len(cln_meta)} dates OK, {cpt_error} dates KO\")\n",
    "    print(f\"Dates KO : {date_error}\")\n",
    "\n",
    "    # Clean 'tags' property values\n",
    "    for file in tqdm(cln_meta, desc=\"Clean 'tags' property values\"):\n",
    "        if 'tags' in cln_meta[file]:\n",
    "            if cln_meta[file]['tags'] is not None:\n",
    "                val = eval(cln_meta[file]['tags'])\n",
    "            cln_meta[file]['tags'] = val\n",
    "\n",
    "    return cln_meta"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the metadata from the SQLite database\n",
    "brut_metadata = get_metadata_from_mariadb_db(sql_database, sql_user, sql_password, sql_host)\n",
    "# Clean the metadata\n",
    "cln_metadata = clean_metadata(brut_metadata)\n",
    "# Convert the metadata to a DataFrame\n",
    "df_metadata = pd.DataFrame.from_dict(cln_metadata).transpose()\n",
    "df_metadata['Make'].fillna('Undefined', inplace=True)\n",
    "df_metadata['GPSInfo'].fillna('Undefined', inplace=True)\n",
    "df_metadata.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overview of the metadata\n",
    "\n",
    "The function \"count_data_per_property\" is used to count the number of non-null values for each property in the metadata dictionary. It takes a dictionary of metadata as an argument and prints the properties that have more than 70 non-null values.\n",
    "\n",
    "The function \"metadata_extract_example\" is used to print the first 3 elements of each list in the dict_metadata dictionary. It takes a dictionary of metadata as an argument and prints the first 3 elements of each list in the dict_metadata dictionary."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def count_data_per_property(metadata_to_count, significant_limit=70):\n",
    "    \"\"\"\n",
    "    Count the number of non-null values for each property in the metadata dictionary\n",
    "    Display the properties that have significant non-null values\n",
    "\n",
    "    :param metadata_to_count: The metadata to count\n",
    "    :param significant_limit: The limit after which a property is considered significant\n",
    "    \"\"\"\n",
    "    # Count the number of non-null values for each property in the metadata dictionary\n",
    "    prop_len = {}\n",
    "    for prop in metadata_to_count:\n",
    "        prop_len[prop] = metadata_to_count[prop].count()\n",
    "\n",
    "        # Print the properties that have more than 70 non-null values\n",
    "        if prop_len[prop] > significant_limit:\n",
    "            print(f'{prop} : {prop_len[prop]}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Number of images : {len(df_metadata[\"File Name\"])}')\n",
    "print(\"-------------- Properties with more significant non-null values --------------\")\n",
    "count_data_per_property(df_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the functions to display the metadata\n",
    "\n",
    "- \"display_bar\" is used to display a bar chart.\n",
    "- \"display_pie\" is used to display a pie chart.\n",
    "- \"display_curve\" is used to display a curve chart."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_bar(title, x_label, y_label, x_values, y_values):\n",
    "    \"\"\"\n",
    "    Display a bar chart\n",
    "\n",
    "    :param title: The title of the chart\n",
    "    :param x_label: The x-axis label\n",
    "    :param y_label: The y-axis label\n",
    "    :param x_values: The values of the x-axis\n",
    "    :param y_values: The values of the y-axis\n",
    "    \"\"\"\n",
    "    plt.bar(x_values, y_values)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_pie(title, values, labels):\n",
    "    \"\"\"\n",
    "    Display a pie chart\n",
    "\n",
    "    :param title: The title of the chart\n",
    "    :param values: The values of the chart\n",
    "    :param labels: The labels of the chart\n",
    "    \"\"\"\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%')\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_curve(title, x_label, y_label, x_values, y_values):\n",
    "    \"\"\"\n",
    "    Display a curve\n",
    "\n",
    "    :param title: The title of the curve\n",
    "    :param x_label: The label of the x_axis\n",
    "    :param y_label: The label of the y_axis\n",
    "    :param x_values: The values of the x_axis\n",
    "    :param y_values: The values of the y_axis\n",
    "    \"\"\"\n",
    "\n",
    "    plt.plot(x_values, y_values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : size (static)\n",
    "\n",
    "The function \"graph_images_size_static\" is used to graph the number of images per size category. It takes a dictionary of metadata as an argument and returns a graph with the number of images per size category. The interval size is 200 by default.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_images_size_static(df_meta, interval_size=200, nb_intervals=4):\n",
    "    \"\"\"\n",
    "    Graph the number of images per size category\n",
    "    The interval size is 200 by default\n",
    "\n",
    "    :param df_meta: The metadata to graph\n",
    "    :param interval_size: The size of the intervals\n",
    "    :param nb_intervals: The number of intervals\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the minimum size of each image and store it in a new column\n",
    "    df_meta['min_size'] = df_meta[['Width', 'Height']].min(axis=1)\n",
    "\n",
    "    # Determine the maximum minimum size\n",
    "    max_min_size = df_meta['min_size'].max()\n",
    "\n",
    "    # Create a list of intervals based on the interval size and number of intervals\n",
    "    inter = [i * interval_size for i in range(nb_intervals + 1)]\n",
    "\n",
    "    # Create a list of labels for each interval\n",
    "    labels = [f'{inter[i]}-{inter[i + 1]}' for i in range(nb_intervals)]\n",
    "\n",
    "    # Categorize each image based on its size and interval\n",
    "    df_meta['size_category'] = pd.cut(df_meta['min_size'], bins=inter, labels=labels)\n",
    "\n",
    "    # Count the number of images in each category\n",
    "    size_counts = df_meta['size_category'].value_counts()\n",
    "\n",
    "    display_bar('Number of images per size category', 'Size category', 'Number of images', size_counts.index, size_counts.values)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_images_size_static(df_metadata, 2000, 3)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : size (dynamic)\n",
    "\n",
    "The function \"graph_images_size_dynamic\" is used to graph the number of images per size category. It takes a dictionary of metadata as an argument and returns a graph with the number of images per size category. The interval size is calculated dynamically. The number of columns in the graph is 7 by default.\n",
    "\n",
    "You can choose the type of graph to display (bar, pie or all for both)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_images_size_dynamic(df_meta, nb_intervals=7, graph_type='all'):\n",
    "    \"\"\"\n",
    "    Graph the number of images per size category\n",
    "    The interval size is calculated dynamically\n",
    "\n",
    "    :param df_meta: The metadata to graph\n",
    "    :param nb_intervals: The number of intervals in the graph\n",
    "    :param graph_type: The type of graph to display (bar, pie or all for both)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the minimum size of each image and store it in a new column\n",
    "    df_meta['min_size'] = df_meta[['Height', 'Width']].min(axis=1)\n",
    "\n",
    "    # Determine the maximum minimum size and calculate the number of bins dynamically based on the number of columns\n",
    "    max_min_size = df_meta['min_size'].max()\n",
    "    num_images = len(df_meta)\n",
    "    num_bins = int(num_images / (num_images / nb_intervals))\n",
    "\n",
    "    # Create a list of bins based on the maximum minimum size and number of bins\n",
    "    bins = [i * (max_min_size / num_bins) for i in range(num_bins + 1)]\n",
    "\n",
    "    # Create a list of labels for each bin\n",
    "    labels = [f'{int(bins[i])}-{int(bins[i + 1])}' for i in range(num_bins)]\n",
    "\n",
    "    # Categorize each image based on its size and bin\n",
    "    df_meta['size_category'] = pd.cut(df_meta['min_size'], bins=bins, labels=labels)\n",
    "\n",
    "    # Count the number of images in each category\n",
    "    size_counts = df_meta['size_category'].value_counts()\n",
    "\n",
    "    title = 'Number of images per size category'\n",
    "\n",
    "    # Create the appropriate chart based on the graph type parameter\n",
    "    if graph_type == 'bar':\n",
    "        display_bar(title, 'Image size', 'Number of images', size_counts.index, size_counts.values)\n",
    "    elif graph_type == 'pie':\n",
    "        display_pie(title, size_counts.values, size_counts.index)\n",
    "    elif graph_type == 'all':\n",
    "        display_bar(title, 'Image size', 'Number of images', size_counts.index, size_counts.values)\n",
    "        display_pie(title, size_counts.values, size_counts.index)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Invalid graph type')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_images_size_dynamic(df_metadata, 5, 'all')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : DateTime\n",
    "\n",
    "The function \"graph_images_datetime\" is used to graph the number of images per year. It takes a dictionary of metadata as an argument and returns a graph with the number of images per year.\n",
    "\n",
    "You can choose the type of graph to display (bar, pie, curve or all for all).\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_images_datetime(df_meta, nb_intervals=10, graph_type='all'):\n",
    "    \"\"\"\n",
    "    Graph the number of images per year\n",
    "\n",
    "    :param df_meta: The metadata to graph (expects a list of dictionaries)\n",
    "    :param graph_type: The type of graph to display (bar, pie, curve or all for all)\n",
    "    :param nb_intervals: The number of intervals to display\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract year from the 'DateTime' column and create a new 'Year' column\n",
    "    df_meta['Year'] = pd.DatetimeIndex(df_meta['DateTimeOriginal']).year\n",
    "\n",
    "\n",
    "    # Group the data by year and count the number of images for each year\n",
    "    image_count = df_meta.groupby('Year').size().reset_index(name='count').sort_values('count', ascending=False)[\n",
    "                  :nb_intervals]\n",
    "    image_count['Year'] = image_count['Year'].astype(int)\n",
    "\n",
    "    # Set the title of the graph\n",
    "    title = 'Number of images per year'\n",
    "\n",
    "    # Display different types of graphs based on the 'graph_type' parameter\n",
    "    if graph_type == 'bar':\n",
    "        # Display a bar chart\n",
    "        image_count.plot(kind='bar', x='Year', y='count')\n",
    "        display_bar(title, 'Year', 'Number of images', image_count['Year'], image_count['count'])\n",
    "\n",
    "    elif graph_type == 'pie':\n",
    "        # Display a pie chart using a custom function 'display_pie'\n",
    "        display_pie(title, image_count['count'], image_count['Year'])\n",
    "\n",
    "    elif graph_type == 'curve':\n",
    "        # Display a line chart using a custom function 'display_curve'\n",
    "        image_count = df_meta.groupby('Year').size().reset_index(name='count').sort_values('Year', ascending=True)\n",
    "        display_curve(title, 'Year', 'Number of images', image_count['Year'], image_count['count'])\n",
    "\n",
    "    elif graph_type == 'all':\n",
    "        # Display all three types of graphs: bar, pie, and line charts\n",
    "\n",
    "        # Bar chart\n",
    "        image_count.plot(kind='bar', x='Year', y='count')\n",
    "        display_bar(title, 'Year', 'Number of images', image_count['Year'], image_count['count'])\n",
    "\n",
    "        # Pie chart\n",
    "        display_pie(title, image_count['count'], image_count['Year'])\n",
    "\n",
    "        # Line chart\n",
    "        image_count = image_count.sort_values('Year', ascending=True)\n",
    "        display_curve(title, 'Year', 'Number of images', image_count['Year'], image_count['count'])\n",
    "    else:\n",
    "        # Raise an error if an invalid 'graph_type' parameter is passed\n",
    "        raise ValueError('Invalid graph type')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_images_datetime(df_metadata, 10, 'all')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : Brand\n",
    "\n",
    "The function \"graph_images_brand\" is used to graph the number of images per brand. It takes a dictionary of metadata as an argument and returns a graph with the number of images per brand.\n",
    "\n",
    "You can choose the type of graph to display (bar, pie or all for both), and the number of columns to display."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_images_brand(df_meta, graph_type='all', nb_columns=5):\n",
    "    \"\"\"\n",
    "    Graph the number of images per brand\n",
    "\n",
    "    :param df_meta: The metadata to graph\n",
    "    :param graph_type: The type of graph to display (bar, pie or all for both)\n",
    "    :param nb_columns: The number of columns to display\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty dictionary to store the counts of each brand\n",
    "    counts = {}\n",
    "\n",
    "    # Loop through each brand in the metadata and count the number of occurrences\n",
    "    for make in df_meta['Make']:\n",
    "        if make is not None :\n",
    "            counts[make] = counts.get(make, 0) + 1\n",
    "\n",
    "    sorted_counts = dict(sorted(counts.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    # Convert the dictionary into two lists of labels and values for graphing\n",
    "    labels = list(sorted_counts.keys())[:nb_columns]\n",
    "    values = list(sorted_counts.values())[:nb_columns]\n",
    "\n",
    "    # Set the title for the graph\n",
    "    title = 'Number of images per brand'\n",
    "\n",
    "    # Determine which type of graph to display based on the 'graph_type' parameter\n",
    "    if graph_type == 'bar':\n",
    "        # Display a bar graph\n",
    "        display_bar(title, 'Brand', 'Number of images', labels, values)\n",
    "    elif graph_type == 'pie':\n",
    "        # Display a pie chart\n",
    "        display_pie(title, values, labels)\n",
    "    elif graph_type == 'all':\n",
    "        # Display both a bar graph and a pie chart\n",
    "        display_bar(title, 'Brand', 'Number of images', labels, values)\n",
    "        display_pie(title, values, labels)\n",
    "    else:\n",
    "        # Raise an error if the 'graph_type' parameter is invalid\n",
    "        raise ValueError('Invalid graph type')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_images_brand(df_metadata, 'all', 10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : Images with GPS\n",
    "\n",
    "#### Overview\n",
    "\n",
    "The function \"gps_info_overview\" is used to display the number of images with GPS data. It takes a dictionary of metadata as an argument and returns the number of images with GPS data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gps_info_overview(df_meta):\n",
    "    \"\"\"\n",
    "    Display the number of images with GPS data\n",
    "    \"\"\"\n",
    "    cpt = 0\n",
    "    # get images with GPS data and print it name and the GPS data\n",
    "    for idx, meta in enumerate(df_meta['GPSInfo']):\n",
    "        if meta is not None and len(meta) > 24:\n",
    "            # print(dict_metadata['file'][idx])\n",
    "            # print(meta)\n",
    "            cpt += 1\n",
    "    print(f\"Number of images with GPS data : {cpt}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gps_info_overview(df_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract GPS coordinates\n",
    "\n",
    "The function \"get_coordinates\" is used to extract the coordinates of the images with GPS data. It takes a dictionary of metadata as an argument and returns a dictionary with the coordinates of the images with GPS data.\n",
    "\n",
    "It uses the function \"dms_to_dd\" to convert the coordinates from DMS (degrees, minutes, seconds) to DD (decimal degrees)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def dms_to_dd(degrees, minutes, seconds, direction):\n",
    "    \"\"\"\n",
    "    Convert DMS (degrees, minutes, seconds) coordinates to DD (decimal degrees)\n",
    "\n",
    "    :param degrees: degrees\n",
    "    :param minutes: minutes\n",
    "    :param seconds: seconds\n",
    "    :param direction: direction (N, S, E, W)\n",
    "    :return: decimal degrees\n",
    "    \"\"\"\n",
    "\n",
    "    dd = float(degrees) + float(minutes) / 60 + float(seconds) / (60 * 60)\n",
    "    if direction == 'S' or direction == 'W':\n",
    "        dd *= -1\n",
    "    return dd"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_coordinates(metadata):\n",
    "    \"\"\"\n",
    "    Extract the coordinates of the images with GPS data\n",
    "\n",
    "    :param metadata: The metadata to extract the coordinates from\n",
    "    \"\"\"\n",
    "    coordinates = {}\n",
    "    for i, gps_info in enumerate(metadata['GPSInfo']):\n",
    "        if gps_info is not None:\n",
    "            try:\n",
    "                gps_info = eval(gps_info)\n",
    "                latitude, longitude = None, None\n",
    "                for key, val in gps_info.items():\n",
    "                    if val == 'N' or val == 'S':\n",
    "                        nxt = gps_info[key + 1]\n",
    "                        latitude = dms_to_dd(nxt[0], nxt[1], nxt[2], val)\n",
    "                    elif val == 'E' or val == 'W':\n",
    "                        nxt = gps_info[key + 1]\n",
    "                        longitude = dms_to_dd(nxt[0], nxt[1], nxt[2], val)\n",
    "\n",
    "                if latitude is not None and longitude is not None:\n",
    "                    coordinates.update({dict_metadata['file'][i]: [latitude, longitude]})\n",
    "            except:\n",
    "                print(f\"Error with {dict_metadata['file'][i]}\")\n",
    "                # print(gps_info)\n",
    "    print(f\"Number of images with valid GPS data : {len(coordinates)}\")\n",
    "\n",
    "    return coordinates"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "coordinates = get_coordinates(dict_metadata)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using a Map with markers\n",
    "\n",
    "The function \"display_coordinates_on_map\" is used to display the coordinates of the images with GPS data on a map. It takes a dictionary of coordinates as an argument and returns a map with the coordinates displayed as markers.\n",
    "\n",
    "Inside the method comments, you can find a way to display the images as markers instead of the default markers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def display_coordinates_on_map(coordinates_list):\n",
    "    \"\"\"\n",
    "    Display the coordinates on a map\n",
    "\n",
    "    :param coordinates_list: The coordinates to display\n",
    "    :return: The map with the coordinates displayed as markers\n",
    "    \"\"\"\n",
    "\n",
    "    # create a map centered at a specific location\n",
    "    m = folium.Map(location=[0, 0], zoom_start=1)\n",
    "\n",
    "    # add markers for each set of coordinates\n",
    "    for image, coords in coordinates_list.items():\n",
    "        lat, lon = coords\n",
    "\n",
    "        # Create a marker with the image as the icon\n",
    "        # !warning! : the image must be download and you need to add :\n",
    "        # from folium.features import CustomIcon\n",
    "\n",
    "        # image_path = '../output/images/' + key\n",
    "        # icon = CustomIcon(icon_image=image_path, icon_size=(100, 100))\n",
    "        # folium.Marker(location=coord, icon=icon).add_to(m)\n",
    "        folium.Marker(location=[lat, lon], tooltip=image, popup=f'file:{image}\\ncoord:{coords}').add_to(m)\n",
    "    return m"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "display_coordinates_on_map(coordinates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using graphs by country\n",
    "\n",
    "#### get country\n",
    "\n",
    "The function \"get_country\" is used to get the country of each coordinate. It takes a dictionary of coordinates as an argument and returns a dictionary with the coordinates and the country."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_country(coordinates_list):\n",
    "    \"\"\"\n",
    "    Get the country of each coordinate\n",
    "\n",
    "    :param coordinates_list: The coordinates to get the country from\n",
    "    :return: The coordinates with the country added\n",
    "    \"\"\"\n",
    "    # Create a geolocator\n",
    "    geolocator = Nominatim(user_agent=\"geoapiExercises\")\n",
    "\n",
    "    # Get the continent information for each coordinate\n",
    "    for key, coord in tqdm(coordinates_list.items(), desc='Getting country information'):\n",
    "        if len(coord) < 3:  # If the country hasn't been found yet\n",
    "            try:\n",
    "                location = geolocator.reverse(coord, exactly_one=True, language='en')\n",
    "                address = location.raw['address']\n",
    "                country = address.get('country')\n",
    "                coordinates[key].append(country)\n",
    "            except:\n",
    "                print(f\"Error with {key} : {coord}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_country(coordinates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display graphs\n",
    "\n",
    "The function \"graph_images_countries\" is used to display graphs about the number of images by country. It takes a dictionary of coordinates as an argument and returns a graph.\n",
    "\n",
    "The parameter \"nb_inter\" is used to set the number of interval to display. The parameter \"graph\" is used to set the type of graph to display (bar, pie, all)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def graph_images_countries(coord_list, nb_inter=5, graph='all'):\n",
    "    \"\"\"\n",
    "    Display graphs about the number of images by country\n",
    "\n",
    "    :param coord_list: list of coordinates\n",
    "    :param nb_inter: number of interval\n",
    "    :param graph: type of graph to display (bar, pie, all)\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a pandas DataFrame from the coordinates dictionary\n",
    "    df = pd.DataFrame.from_dict(coord_list, orient='index', columns=['Latitude', 'Longitude', 'Country'])\n",
    "\n",
    "    # Group the DataFrame by continent and count the number of images\n",
    "    country_count = df.groupby('Country')['Country'].count()\n",
    "    country_count = country_count.sort_values(ascending=False)[:nb_inter]\n",
    "\n",
    "    title = 'Number of images by country'\n",
    "\n",
    "    if graph == 'bar':\n",
    "        display_bar(title, \"Country\", \"Image Count\", country_count.index, country_count.values)\n",
    "    elif graph == 'pie':\n",
    "        display_pie(title, country_count.values, country_count.index)\n",
    "    else:\n",
    "        display_bar(title, \"Country\", \"Image Count\", country_count.index, country_count.values)\n",
    "        display_pie(title, country_count.values, country_count.index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "graph_images_countries(coordinates, 10, 'all')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : by Dominant Color"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MAX_COLUMNS = 20\n",
    "\n",
    "\n",
    "def closest_colour(requested_colour):\n",
    "    min_colours = {}\n",
    "    for key, name in webcolors.CSS3_HEX_TO_NAMES.items():\n",
    "        r_c, g_c, b_c = webcolors.hex_to_rgb(key)\n",
    "        rd = (r_c - requested_colour[0]) ** 2\n",
    "        gd = (g_c - requested_colour[1]) ** 2\n",
    "        bd = (b_c - requested_colour[2]) ** 2\n",
    "        min_colours[(rd + gd + bd)] = name\n",
    "    return min_colours[min(min_colours.keys())]\n",
    "\n",
    "\n",
    "def get_colour_name(requested_colour):\n",
    "    try:\n",
    "        closest_name = actual_name = webcolors.rgb_to_name(requested_colour)\n",
    "    except ValueError:\n",
    "        closest_name = closest_colour(requested_colour)\n",
    "        actual_name = None\n",
    "    return actual_name, closest_name\n",
    "\n",
    "\n",
    "dict_dom_color = {}\n",
    "for idx, dom_color in enumerate(dict_metadata['dominant_color']):\n",
    "    if dom_color is not None:\n",
    "        list_dom_color = eval(dom_color)\n",
    "        dict_dom_color.update({dict_metadata['file'][idx]: list_dom_color})\n",
    "\n",
    "color_counts = Counter()\n",
    "for image_colors in dict_dom_color.values():\n",
    "    for color, percentage in image_colors:\n",
    "        color_counts[color] += percentage\n",
    "\n",
    "# Map hexadecimal codes to color names\n",
    "color_names = {}\n",
    "for code in color_counts.keys():\n",
    "    try:\n",
    "        rgb = webcolors.hex_to_rgb(code)\n",
    "        actual, closest = get_colour_name(rgb)\n",
    "        color_names[code] = closest\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "dict_res = {}\n",
    "for key, val in color_names.items():\n",
    "    if val in dict_res:\n",
    "        dict_res[val] += round(color_counts[key] / 100, 5)\n",
    "    else:\n",
    "        dict_res[val] = round(color_counts[key] / 100, 5)\n",
    "\n",
    "if sum(dict_res.values()) > 100:\n",
    "    raise Exception('sum of dict_res.values() > 100')\n",
    "\n",
    "columns = dict_res.__len__()\n",
    "if columns > MAX_COLUMNS:\n",
    "    columns = MAX_COLUMNS\n",
    "\n",
    "sorted_colors = sorted(dict_res.items(), key=lambda x: x[1], reverse=True)\n",
    "top_colors = dict(sorted_colors[:columns])\n",
    "\n",
    "# Create a bar graph showing the dominant colors in the images\n",
    "plt.bar(top_colors.keys(), top_colors.values(), color=top_colors.keys())\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "# Create a pie chart showing the dominant colors in the images\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "colors = list(top_colors.keys())\n",
    "ax.pie(top_colors.values(), labels=top_colors.keys(), autopct='%1.1f%%', colors=colors, textprops={'color': 'white'})\n",
    "ax.set_title('Top Colors')\n",
    "ax.legend(title='Colors', loc='center right', bbox_to_anchor=(1.2, 0.5))\n",
    "plt.show()\n",
    "\n",
    "# Create a treemap showing the dominant colors in the images\n",
    "color = [webcolors.name_to_hex(c) for c in top_colors]\n",
    "labels = list(top_colors.keys())\n",
    "sizes = list(top_colors.values())\n",
    "squarify.plot(sizes=sizes, label=labels, color=color, alpha=.7)\n",
    "plt.title(\"Top Colors\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Graph images : by Tags"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert tag strings to a list of tags\n",
    "# convert tag strings to a list of tags\n",
    "#tags = list(itertools.chain.from_iterable([ast.literal_eval(t) for t in dict_metadata['tags']]))\n",
    "\n",
    "tags = dict_metadata['tags']\n",
    "# flatten the list of tags\n",
    "tags = [item for sublist in tags for item in sublist]\n",
    "\n",
    "# count the occurrences of each tag\n",
    "tag_counts = Counter(tags)\n",
    "\n",
    "# plot the most common tags\n",
    "n = 10\n",
    "top_tags = dict(tag_counts.most_common(n))\n",
    "plt.bar(top_tags.keys(), top_tags.values())\n",
    "plt.title(f\"Top {n} most common tags\")\n",
    "plt.xlabel(\"Tags\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tags = dict_metadata['tags']\n",
    "# flatten the list of tags\n",
    "tags = [item for sublist in tags for item in sublist]\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_lg\")  # load pre-trained word embedding model\n",
    "except OSError:\n",
    "    !python -m spacy download en_core_web_lg\n",
    "\n",
    "categories = {\n",
    "    \"landscape\": {}, \"animal\": {}, \"people\": {}, \"food\": {}, \"building\": {}, \"vehicle\": {}, \"object\": {}, \"other\": {}\n",
    "}\n",
    "\n",
    "# categorize words based on similarity to category prototypes\n",
    "for word in tags:\n",
    "    # find the most similar category prototype for the word\n",
    "    max_similarity = -1\n",
    "    chosen_category = \"other\"\n",
    "    for category in categories:\n",
    "        similarity = nlp(word).similarity(nlp(category))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            chosen_category = category\n",
    "\n",
    "    # add the word into the appropriate category dictionary\n",
    "    categories[chosen_category].update({word: max_similarity})\n",
    "\n",
    "print(categories)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Convert the dictionary into a numpy array\n",
    "# Convert the dictionary into a numpy array\n",
    "def dict_to_array(categories):\n",
    "    n_categories = len(categories)\n",
    "    arr = np.zeros((n_categories, n_categories))\n",
    "    for i, (cat, subcat) in enumerate(categories.items()):\n",
    "        for j, (subcat, val) in enumerate(subcat.items()):\n",
    "            if j < n_categories:\n",
    "                arr[i, j] = val\n",
    "    return arr\n",
    "\n",
    "\n",
    "# Generate the linkage matrix\n",
    "Z = linkage(dict_to_array(categories), 'ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "dn = dendrogram(Z, labels=list(categories.keys()))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tags = dict_metadata['tags']\n",
    "# flatten the list of tags\n",
    "tags = [item for sublist in tags for item in sublist]\n",
    "user_data = {}\n",
    "\n",
    "# Create a label for the title\n",
    "title_label = widgets.Label(value='User Information Form')\n",
    "\n",
    "# Create text boxes for first and last name\n",
    "pseudo = widgets.Text(description='Pseudo :')\n",
    "\n",
    "# Create a color picker for favorite colors\n",
    "color_picker = widgets.ColorPicker(\n",
    "    concise=True,\n",
    "    description='Favorite Colors:',\n",
    "    value='#FF0000',\n",
    "    continuous_update=False,\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a dropdown list of tags\n",
    "tag_dropdown = widgets.SelectMultiple(\n",
    "    options=tags,\n",
    "    value=[],\n",
    "    description='Tags:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a dropdown for image orientation\n",
    "orientation_dropdown = widgets.Dropdown(\n",
    "    options=['Portrait', 'Landscape'],\n",
    "    value='Portrait',\n",
    "    description='Orientation:'\n",
    ")\n",
    "\n",
    "# Create sliders for image height and width\n",
    "height_slider = widgets.IntSlider(min=100, max=4000, step=100, description='Height:')\n",
    "width_slider = widgets.IntSlider(min=100, max=4000, step=100, description='Width:')\n",
    "\n",
    "#  Create a button to submit the form\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "\n",
    "# Create a VBox container for the widgets\n",
    "form_container = widgets.VBox([\n",
    "    title_label,\n",
    "    pseudo,\n",
    "    color_picker,\n",
    "    orientation_dropdown,\n",
    "    height_slider,\n",
    "    width_slider,\n",
    "    tag_dropdown,\n",
    "    submit_button\n",
    "])\n",
    "\n",
    "form_container.layout = widgets.Layout(\n",
    "    width='600px',\n",
    "    height='500px',\n",
    "    justify_content='center',  # Centrer les widgets horizontalement\n",
    "    align_items='center'  # Centrer les widgets verticalement\n",
    ")\n",
    "\n",
    "\n",
    "# Define a function to handle form submission\n",
    "def on_submit_button_clicked(b):\n",
    "    user_data.update({\n",
    "        pseudo.value: {\n",
    "            'fav_color': color_picker.value,\n",
    "            'fav_orientation': orientation_dropdown.value,\n",
    "            'fav_height': height_slider.value,\n",
    "            'fav_width': width_slider.value,\n",
    "            'tags': tag_dropdown.value\n",
    "        }\n",
    "    }\n",
    "    )\n",
    "\n",
    "\n",
    "# Attach the on_submit_button_clicked function to the button click event\n",
    "clicked = submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "# Display the form container\n",
    "display(form_container)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(user_data['Yannis']['tags'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_columns = ['fav_color', 'fav_orientation', 'fav_height', 'fav_width', 'tags']\n",
    "\n",
    "\n",
    "def save_metadata(user_data):\n",
    "    try:\n",
    "        # Open a connection to the database\n",
    "        conn = sqlite3.connect(os.path.join(metadata_path, 'metadata.db'))\n",
    "        # Create a cursor\n",
    "        c = conn.cursor()\n",
    "        # Create a table if it doesn't exist : filename, key, value\n",
    "        c.execute(\n",
    "            '''CREATE TABLE IF NOT EXISTS users (\n",
    "            pseudo text PRIMARY KEY,\n",
    "            fav_color text,\n",
    "            fav_orientation text,\n",
    "            fav_height integer,\n",
    "            fav_width integer,\n",
    "            fav_tags text\n",
    "        )''')\n",
    "\n",
    "        nb_users = len(user_data.keys())\n",
    "\n",
    "        check = True\n",
    "        # check if all data are usable\n",
    "        if nb_users >= 1:\n",
    "            for pseudo in user_data.keys():\n",
    "                for column in list_columns:\n",
    "                    if user_data[pseudo][column] is None:\n",
    "                        check = False\n",
    "                        return\n",
    "        else:\n",
    "            check = False\n",
    "\n",
    "        if check:\n",
    "            for pseudo in user_data.keys():\n",
    "                c.execute(\"SELECT * FROM users WHERE pseudo=?\", (pseudo,))\n",
    "                if c.fetchone():\n",
    "                    c.execute(\n",
    "                        \"UPDATE users SET fav_color=?, fav_orientation=?, fav_height=?, fav_width=?, tags = ? WHERE pseudo=?\",\n",
    "                        (user_data[pseudo]['fav_color'], user_data[pseudo]['fav_orientation'],\n",
    "                         user_data[pseudo]['fav_height'], user_data[pseudo]['fav_width'], str(user_data[pseudo]['tags']),\n",
    "                         pseudo))\n",
    "                    conn.commit()\n",
    "                else:\n",
    "                    c.execute(\"INSERT INTO users VALUES (?, ?, ?, ?, ?, ?)\", (\n",
    "                    pseudo, user_data[pseudo]['fav_color'], user_data[pseudo]['fav_orientation'],\n",
    "                    user_data[pseudo]['fav_height'], user_data[pseudo]['fav_width'], str(user_data[pseudo]['tags'])))\n",
    "                    conn.commit()\n",
    "                print(f\"User {pseudo} saved to database successfully\")\n",
    "            conn.close()\n",
    "        else:\n",
    "            print(\"Invalid User data\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error while saving user data to database\" + str(e))\n",
    "\n",
    "\n",
    "save_metadata(user_data)\n",
    "print(user_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sqlite3\n",
    "import itertools\n",
    "import ipywidgets as widgets\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "config_path = os.path.join(output_path, \"config\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, config_path]\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dict_metadata = {}\n",
    "\n",
    "\n",
    "tags = set(list(itertools.chain.from_iterable([ast.literal_eval(t) for t in dict_metadata['tags']])))\n",
    "user_data = {}\n",
    "\n",
    "# Create a label for the title\n",
    "title_label = widgets.Label(value='User Information Form')\n",
    "\n",
    "# Create text boxes for first and last name\n",
    "pseudo = widgets.Text(description='Pseudo :')\n",
    "\n",
    "# Create a color picker for favorite colors\n",
    "color_picker = widgets.ColorPicker(\n",
    "    concise=True,\n",
    "    description='Favorite Colors:',\n",
    "    value='#FF0000',\n",
    "    continuous_update=False,\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a dropdown list of tags\n",
    "tag_dropdown = widgets.SelectMultiple(\n",
    "    options=tags,\n",
    "    value=[],\n",
    "    description='Tags:',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Create a dropdown for image orientation\n",
    "orientation_dropdown = widgets.Dropdown(\n",
    "    options=['Portrait', 'Landscape'],\n",
    "    value='Portrait',\n",
    "    description='Orientation:'\n",
    ")\n",
    "\n",
    "# Create sliders for image height and width\n",
    "height_slider = widgets.IntSlider(min=100, max=4000, step=100, description='Height:')\n",
    "width_slider = widgets.IntSlider(min=100, max=4000, step=100, description='Width:')\n",
    "\n",
    "#  Create a button to submit the form\n",
    "submit_button = widgets.Button(description='Submit')\n",
    "\n",
    "# Create a VBox container for the widgets\n",
    "form_container = widgets.VBox([\n",
    "    title_label,\n",
    "    pseudo,\n",
    "    color_picker,\n",
    "    orientation_dropdown,\n",
    "    height_slider,\n",
    "    width_slider,\n",
    "    tag_dropdown,\n",
    "    submit_button\n",
    "])\n",
    "\n",
    "form_container.layout = widgets.Layout(\n",
    "    width='600px',\n",
    "    height='500px',\n",
    "    justify_content='center',  # Centrer les widgets horizontalement\n",
    "    align_items='center'  # Centrer les widgets verticalement\n",
    ")\n",
    "\n",
    "# Define a function to handle form submission\n",
    "def on_submit_button_clicked(b):\n",
    "    user_data.update({\n",
    "        pseudo.value : {\n",
    "            'fav_color' : color_picker.value,\n",
    "            'fav_orientation' : orientation_dropdown.value,\n",
    "            'fav_height' : height_slider.value,\n",
    "            'fav_width' : width_slider.value,\n",
    "            'tags': tag_dropdown.value\n",
    "        }\n",
    "    }\n",
    "    )\n",
    "\n",
    "# Attach the on_submit_button_clicked function to the button click event\n",
    "clicked = submit_button.on_click(on_submit_button_clicked)\n",
    "\n",
    "# Display the form container\n",
    "display(form_container)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(user_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "list_columns = ['fav_color', 'fav_orientation', 'fav_height', 'fav_width']\n",
    "\n",
    "def save_metadata(user_data):\n",
    "    try:\n",
    "        # Open a connection to the database\n",
    "        conn = sqlite3.connect(os.path.join(metadata_path, 'metadata.db'))\n",
    "        # Create a cursor\n",
    "        c = conn.cursor()\n",
    "        # Create a table if it doesn't exist : filename, key, value\n",
    "        c.execute(\n",
    "            '''CREATE TABLE IF NOT EXISTS users (\n",
    "            pseudo text PRIMARY KEY,\n",
    "            fav_color text,\n",
    "            fav_orientation text,\n",
    "            fav_height integer,\n",
    "            fav_width integer\n",
    "        )''')\n",
    "\n",
    "        nb_users = len(user_data.keys())\n",
    "\n",
    "        check = True\n",
    "        # check if all data are usable\n",
    "        if nb_users > 1:\n",
    "            for pseudo in user_data.keys() :\n",
    "                for column in list_columns :\n",
    "                    if user_data[pseudo][column] is None :\n",
    "                        check = False\n",
    "                        return\n",
    "        else :\n",
    "            check = False\n",
    "\n",
    "        if check :\n",
    "            for pseudo in user_data.keys() :\n",
    "                c.execute(\"SELECT * FROM users WHERE pseudo=?\", (pseudo,))\n",
    "                if c.fetchone():\n",
    "                    c.execute(\"UPDATE users SET fav_color=?, fav_orientation=?, fav_height=?, fav_width=? WHERE pseudo=?\",\n",
    "                              (user_data[pseudo]['fav_color'], user_data[pseudo]['fav_orientation'], user_data[pseudo]['fav_height'], user_data[pseudo]['fav_width'], pseudo))\n",
    "                    conn.commit()\n",
    "                else :\n",
    "                    c.execute(\"INSERT INTO users VALUES (?, ?, ?, ?, ?)\", (pseudo, user_data[pseudo]['fav_color'], user_data[pseudo]['fav_orientation'], user_data[pseudo]['fav_height'], user_data[pseudo]['fav_width']))\n",
    "                    conn.commit()\n",
    "                print(f\"User {pseudo} saved to database successfully\")\n",
    "            conn.close()\n",
    "        else :\n",
    "            print(\"Invalid User data\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error saving user to database : \", e)\n",
    "\n",
    "save_metadata(user_data)\n",
    "print(user_data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mysql.connector import pooling\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preferences = {\n",
    "    'Make': '',\n",
    "    'ImageWidth': '',\n",
    "    'ImageHeight': '',\n",
    "    'Orientation': 1,\n",
    "    'dominant_color': '#73AD3D',\n",
    "    'tags': ['vase', 'toilet']\n",
    "}\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")\n",
    "\n",
    "# set the database config\n",
    "config = {\n",
    "    'user': sql_user,\n",
    "    'password': sql_password,\n",
    "    'host': sql_host,\n",
    "    'port': '3306',\n",
    "    'database': sql_database,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a connection pool\n",
    "connection_pool = pooling.MySQLConnectionPool(pool_name=\"mypool\",\n",
    "                                              pool_size=2,\n",
    "                                              **config)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_metadata_from_mariadb_db():\n",
    "    \"\"\"\n",
    "    Get the metadata from the MariaDB database\n",
    "\n",
    "    :return: A pandas DataFrame with the metadata\n",
    "    \"\"\"\n",
    "    # Open a connection to the database\n",
    "    conn = connection_pool.get_connection()\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Retrieve the metadata\n",
    "    c.execute(\"\"\"\n",
    "        SELECT filename, GROUP_CONCAT(CONCAT(mkey, '\\t', mvalue) SEPARATOR '\\n') AS metadata\n",
    "        FROM metadata\n",
    "        GROUP BY filename;\n",
    "    \"\"\")\n",
    "    metadata = c.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Create an empty DataFrame with the desired columns\n",
    "    columns = ['filename', 'Make', 'Software', 'ImageWidth', 'ImageHeight', 'Orientation', 'DateTimeOriginal',\n",
    "               'dominant_color', 'tags']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Fill the DataFrame with the metadata\n",
    "    for image in tqdm(metadata, desc=\"Get metadata from database\"):\n",
    "        try:\n",
    "            props = {'filename': image[0]}\n",
    "            metadata_str = image[1].split('\\n')\n",
    "            for prop in metadata_str:\n",
    "                if prop:\n",
    "                    k, value = prop.split('\\t')\n",
    "                    if k in columns[1:]:\n",
    "                        if k == 'dominant_color':\n",
    "                            color_list = eval(value)\n",
    "                            color_list = [c[0] for c in color_list]\n",
    "                            props[k] = color_list\n",
    "                        elif k == 'tags':\n",
    "                            props[k] = eval(value)\n",
    "                        else:\n",
    "                            props[k] = value\n",
    "            df = df.append(props, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e, image)\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hex_to_rgb(color):\n",
    "    try:\n",
    "        # remove the # from the color\n",
    "        color = color[1:]\n",
    "        # convert the color to rgb values\n",
    "        rgb = tuple(int(color[i:i + 2], 16) for i in (0, 2, 4))\n",
    "        return rgb\n",
    "    except:\n",
    "        return 0, 0, 0\n",
    "\n",
    "\n",
    "def get_clean_preferences(df_preferences):\n",
    "    # remove the rows with nan in dominant_color\n",
    "    df_preferences = df_preferences.dropna(subset=['dominant_color'])\n",
    "    # split dominant color into 4 columns and remove the dominant_color column\n",
    "    # convert the tags column to a list of strings\n",
    "    # Replace all NaN values with empty strings with the fillna() method\n",
    "    df_preferences = df_preferences.fillna(0)\n",
    "    # convert colors to rgb values\n",
    "    df_preferences['dominant_color'] = df_preferences['dominant_color'].apply(lambda x: hex_to_rgb(x))\n",
    "    # replace all 0 values with empty strings\n",
    "    df_preferences['dominant_color'] = df_preferences['dominant_color'].replace(0, '')\n",
    "\n",
    "    return df_preferences\n",
    "\n",
    "\n",
    "def get_clean_dataset():\n",
    "    metadata = get_metadata_from_mariadb_db()\n",
    "    df_metadata = pd.DataFrame(metadata)\n",
    "    # remove the rows with nan in dominant_color\n",
    "    df_metadata = df_metadata.dropna(subset=['dominant_color'])\n",
    "    # split dominant color into 4 columns and remove the dominant_color column\n",
    "    if 'dominant_color' in df_metadata.columns:\n",
    "        df_metadata['color1'] = df_metadata['dominant_color'].apply(lambda x: x[0] if len(x) >= 1 else 0)\n",
    "        df_metadata['color2'] = df_metadata['dominant_color'].apply(lambda x: x[1] if len(x) >= 2 else 0)\n",
    "        df_metadata['color3'] = df_metadata['dominant_color'].apply(lambda x: x[2] if len(x) == 3 else 0)\n",
    "        df_metadata['color4'] = df_metadata['dominant_color'].apply(lambda x: x[3] if len(x) == 4 else 0)\n",
    "        # convert colors to rgb values\n",
    "        df_metadata['color1'] = df_metadata['color1'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata['color2'] = df_metadata['color2'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata['color3'] = df_metadata['color3'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata['color4'] = df_metadata['color4'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata = df_metadata.drop('dominant_color', axis=1)\n",
    "    else:\n",
    "        df_metadata['color1'] = 0\n",
    "        df_metadata['color2'] = 0\n",
    "        df_metadata['color3'] = 0\n",
    "        df_metadata['color4'] = 0\n",
    "\n",
    "    # convert the tags column to a list of strings\n",
    "    df_metadata = df_metadata.fillna(0)\n",
    "    # remove all columns except filename, tags, color1, color2, color3, color4, Make, Width, Height\n",
    "    df_metadata = df_metadata[\n",
    "        ['filename', 'Make', 'ImageWidth', 'ImageHeight', 'Orientation', 'DateTimeOriginal', 'tags', 'color1', 'color2',\n",
    "         'color3', 'color4']]\n",
    "    # replace all 0 values with empty strings\n",
    "    df_metadata['Make'] = df_metadata['Make'].replace(0, '')\n",
    "\n",
    "    return df_metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pref = pd.DataFrame([preferences])\n",
    "df_preferences = get_clean_preferences(df_pref)\n",
    "df_preferences.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_metadata = get_clean_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_metadata.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Color Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_colors(df_metadata, df_preferences, n=0):\n",
    "    # Load the dataset into a Pandas DataFrame\n",
    "    data = df_metadata.copy()\n",
    "\n",
    "    # Extract the individual r, g, and b values from tupbles in the color columns\n",
    "    data[['r1', 'g1', 'b1']] = pd.DataFrame(data['color1'].tolist(), index=data.index)\n",
    "    data[['r2', 'g2', 'b2']] = pd.DataFrame(data['color2'].tolist(), index=data.index)\n",
    "    data[['r3', 'g3', 'b3']] = pd.DataFrame(data['color3'].tolist(), index=data.index)\n",
    "    data[['r4', 'g4', 'b4']] = pd.DataFrame(data['color4'].tolist(), index=data.index)\n",
    "\n",
    "    # Normalize the r, g, and b columns to be between 0 and 1\n",
    "    data[['r1', 'g1', 'b1', 'r2', 'g2', 'b2', 'r3', 'g3', 'b3', 'r4', 'g4', 'b4']] = data[['r1', 'g1', 'b1', 'r2', 'g2',\n",
    "                                                                                           'b2', 'r3', 'g3', 'b3', 'r4',\n",
    "                                                                                           'g4', 'b4']] / 255\n",
    "\n",
    "    # Normalize the input RGB color to be between 0 and 1\n",
    "    r, g, b = df_preferences['dominant_color'][0]\n",
    "    r_norm, g_norm, b_norm = r / 255, g / 255, b / 255\n",
    "\n",
    "    # Compute the Euclidean distance between the input color and all the colors in the dataset\n",
    "    data['similarity_dominant_color'] = euclidean_distances(\n",
    "        [[r_norm, g_norm, b_norm, r_norm, g_norm, b_norm, r_norm, g_norm, b_norm, r_norm, g_norm, b_norm]],\n",
    "        data[['r1', 'g1', 'b1', 'r2', 'g2', 'b2', 'r3', 'g3', 'b3', 'r4', 'g4', 'b4']])[0]\n",
    "\n",
    "    # Sort the dataset by Euclidean distance in ascending order and return the top 10 closest matches\n",
    "    if n == 0:\n",
    "        closest_matches = data.sort_values('similarity_dominant_color', ascending=True)[\n",
    "            ['filename', 'color1', 'color2', 'color3', 'color4', 'similarity_dominant_color']]\n",
    "    else:\n",
    "        closest_matches = data.sort_values('similarity_dominant_color', ascending=True).head(n)[\n",
    "            ['filename', 'color1', 'color2', 'color3', 'color4', 'similarity_dominant_color']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_colors(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tag Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_tags(df_metadata, df_preferences, n=0, nlp=None):\n",
    "    # Load the spaCy model if it hasn't been loaded\n",
    "    if not nlp:\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Define the preferences list and the dataframe\n",
    "    preferences = df_preferences['tags'][0]\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.copy()\n",
    "    df = df.dropna(subset=[\"tags\"]).reset_index(drop=True)\n",
    "    # replace int with empty list\n",
    "    df['tags'] = df['tags'].apply(lambda x: x if x else [])\n",
    "\n",
    "    # Precompute the similarity between each tag word and each preference word\n",
    "    similarity_dict = {}\n",
    "    for tag_word in set([word for tags in df['tags'] for word in tags]):\n",
    "        for pref_word in set(preferences):\n",
    "            similarity_dict[(tag_word, pref_word)] = nlp(tag_word).similarity(nlp(pref_word))\n",
    "\n",
    "    # Compute the average similarity for each row in the dataframe\n",
    "    similarities = []\n",
    "    for tags in df['tags']:\n",
    "        sum_similarity = 0\n",
    "        for tag_word in tags:\n",
    "            for pref_word in preferences:\n",
    "                sum_similarity += similarity_dict[(tag_word, pref_word)]\n",
    "        avg_similarity = sum_similarity / (len(tags) * len(preferences)) if len(tags) > 0 else 0\n",
    "        similarities.append(avg_similarity)\n",
    "\n",
    "    # Add the similarity scores to a new column in the dataframe\n",
    "    df['similarity_tags'] = similarities\n",
    "    if n == 0:\n",
    "        closest_matches = df.sort_values('similarity_tags', ascending=False)[\n",
    "            ['filename', 'similarity_tags']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_tags', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_tags']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_tags(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_make(df_metadata, df_preferences, n=0):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Define the preferences list and the dataframe\n",
    "    make = df_preferences['Make'][0]\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.copy()\n",
    "    df = df.dropna(subset=[\"Make\"]).reset_index(drop=True)\n",
    "\n",
    "    # Convert make and Make to document objects\n",
    "    make_doc = nlp(make)\n",
    "    df['Make'] = df['Make'].apply(nlp)\n",
    "\n",
    "    # Compute the cosine similarity between the make preferences and all the makes in the dataset\n",
    "    similarities = [make_doc.similarity(doc) for doc in df['Make']]\n",
    "\n",
    "    # Add the similarity scores to a new column in the dataframe\n",
    "    df['similarity_make'] = similarities\n",
    "    if n == 0:\n",
    "        closest_matches = df.sort_values('similarity_make', ascending=False)[\n",
    "            ['filename', 'similarity_make']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_make', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_make']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_make(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Orientation Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_orientation(df_metadata, df_preferences, n=0):\n",
    "    # Define the preferences list and the dataframe\n",
    "    orientation = df_preferences['Orientation'][0]\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.dropna(subset=[\"Orientation\"]).reset_index(drop=True)\n",
    "    # if Orientation contain '' or '0' or '1' then replace with 0 or 1\n",
    "    df['Orientation'] = df['Orientation'].apply(lambda x: 0 if x == '' or x == '0' else 1)\n",
    "\n",
    "    # Convert the Orientation column to integer type\n",
    "    df['Orientation'] = df['Orientation'].astype(int)\n",
    "\n",
    "    # Orientation is 0 or 1, so we can just subtract the preference from the orientation\n",
    "    df['similarity_orientation'] = df['Orientation'].apply(lambda x: abs(x - orientation))\n",
    "\n",
    "    # sort by similarity\n",
    "    if n > 0:\n",
    "        closest_matches = df.sort_values('similarity_orientation', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_orientation']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_orientation', ascending=False)[\n",
    "            ['filename', 'similarity_orientation']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_orientation(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Size Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_size(df_metadata, df_preferences, n=0):\n",
    "    # Define the preferences list and the dataframe\n",
    "    width = int(df_preferences['ImageWidth'][0])\n",
    "    height = int(df_preferences['ImageHeight'][0])\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.dropna(subset=[\"ImageWidth\", \"ImageHeight\"]).reset_index(drop=True)\n",
    "\n",
    "    # Convert the ImageWidth and ImageHeight column to integer type\n",
    "    df[['ImageWidth', 'ImageHeight']] = df[['ImageWidth', 'ImageHeight']].astype(int)\n",
    "\n",
    "    # Compute the product of width and height outside the loop\n",
    "    product = width * height\n",
    "\n",
    "    # Use apply method to compute similarity score for each row\n",
    "    df['similarity_size'] = df.apply(lambda x: 1 - abs(product - (x['ImageWidth'] * x['ImageHeight'])) / product, axis=1)\n",
    "\n",
    "    if n == 0:\n",
    "        closest_matches = df.sort_values('similarity_size', ascending=False)[\n",
    "            ['filename', 'similarity_size']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_size', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_size']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_size(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend(df_metadata, df_preferences, n=0):\n",
    "    # Assign weights to properties based on user preferences\n",
    "    weights = {\n",
    "        'Make': float(5.0),\n",
    "        'ImageWidth': float(1.0),\n",
    "        'ImageHeight': float(1.0),\n",
    "        'Orientation': float(2.0),\n",
    "        'dominant_color': float(3.0),\n",
    "        'tags': float(5.0)\n",
    "    }\n",
    "\n",
    "    # Create a dictionary with the preferences and the corresponding recommendation methods\n",
    "    preference_methods = {\n",
    "        'Make': recommend_make,\n",
    "        'ImageWidth': recommend_size,\n",
    "        'ImageHeight': recommend_size,\n",
    "        'Orientation': recommend_orientation,\n",
    "        'dominant_color': recommend_colors,\n",
    "        'tags': recommend_tags\n",
    "    }\n",
    "\n",
    "    # Remove preferences with no values\n",
    "    preferences = {k: v for k, v in df_preferences.squeeze().to_dict().items() if v != ''}\n",
    "\n",
    "    # Calculate the sum of the weights\n",
    "    weights_sum = 0\n",
    "    for key in weights:\n",
    "        weights_sum += weights[key]\n",
    "    for key in weights:\n",
    "        weights[key] = weights[key] / weights_sum\n",
    "\n",
    "    # Calculate similarity score for each property\n",
    "    df_metadata['similarity_score'] = 0.0\n",
    "    for preference, value in preferences.items():\n",
    "        method = preference_methods[preference]\n",
    "        similarity = method(df_metadata, df_preferences, n)[f'similarity_{preference.lower()}'].astype(float)\n",
    "        df_metadata['similarity_score'] += similarity * (weights[preference] / weights_sum)\n",
    "\n",
    "    # Replace NaN values in the 'similarity_score' column with 0\n",
    "    df_metadata['similarity_score'].fillna(0, inplace=True)\n",
    "\n",
    "    # Sort by similarity score\n",
    "    if n == 0:\n",
    "        closest_matches = df_metadata.sort_values('similarity_score', ascending=False)[\n",
    "            ['filename', 'similarity_score']]\n",
    "    else:\n",
    "        closest_matches = df_metadata.sort_values('similarity_score', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_score']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
