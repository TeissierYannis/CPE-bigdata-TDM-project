{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "This code block imports several libraries that are used in the code.\n",
    "\n",
    "- `os`: This library provides a portable way of using operating system dependent functionality.\n",
    "\n",
    "- `zipfile`: This library provides tools to create, read, write, append, and list a ZIP file.\n",
    "\n",
    "- `requests`: This library is used for making HTTP requests to retrieve data from a specified URL.\n",
    "\n",
    "- `functools`: This library provides tools for working with functions.\n",
    "\n",
    "- `pathlib`: This library provides an object-oriented way of working with file paths.\n",
    "\n",
    "- `tqdm`: This library provides a progress bar to indicate the progress of a task.\n",
    "\n",
    "- `pandas`: This library provides data structures and data analysis tools for handling and manipulating numerical tables and time series data.\n",
    "\n",
    "- `nest_asyncio`: This library is used to run multiple asyncio loops in the same thread.\n",
    "\n",
    "- `aiohttp`: This library provides an asynchronous HTTP client/server for asyncio.\n",
    "\n",
    "- `time`: This library provides functions to work with time.\n",
    "\n",
    "- `asyncio`: This library provides asynchronous I/O, event loop, and concurrency tools.\n",
    "\n",
    "- `csv`: This library provides functionality to read from and write to CSV (Comma-Separated Values) files.\n",
    "\n",
    "- `load_dotenv`: This function is imported from the `dotenv` library and is used to load environment variables from a .env file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests pathlib tqdm pandas pillow nest_asyncio aiohttp python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import asyncio\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:39.640205Z",
     "end_time": "2023-04-23T08:31:41.879390Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Paths and Variables\n",
    "This code block sets several paths and variables that are used throughout the code.\n",
    "\n",
    "- `output_path`: This variable stores the base folder path for the project.\n",
    "\n",
    "- `images_path`, `metadata_path`, `include_path`: These variables store the paths for the images, metadata, and include folders, respectively. The paths are constructed by joining the base folder path with the respective sub-folder names.\n",
    "\n",
    "- `list_of_paths`: This list stores the paths of the output, images, metadata, and include folders.\n",
    "\n",
    "- `dataset_url`: This variable stores the base URL for the dataset.\n",
    "\n",
    "- `num_images`: This variable stores the number of images to be downloaded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:41.807168Z",
     "end_time": "2023-04-23T08:31:41.882637Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "include_path = os.path.join(output_path, \"include\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, include_path]\n",
    "\n",
    "# Set the base URL for the dataset\n",
    "dataset_url = \"https://unsplash.com/data/lite/latest\"\n",
    "\n",
    "# Set the number of images to download\n",
    "num_images = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a Folder\n",
    "This function `create_folder` creates a folder at the specified path.\n",
    "\n",
    "**Function Parameters:**\n",
    "\n",
    "- `path (str)`: The path of the folder to be created.\n",
    "\n",
    "**Function Returns:**\n",
    "\n",
    "- `None`\n",
    "\n",
    "**Function Behavior:**\n",
    "\n",
    "- The function uses the `os.mkdir` method to create the folder at the specified path.\n",
    "- If the folder already exists, the function prints a message saying so.\n",
    "- If there is an error creating the folder, the function prints the error message.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:43.017658Z",
     "end_time": "2023-04-23T08:31:43.022775Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    This function creates a folder at the specified path.\n",
    "    If the folder already exists, it will print a message saying so.\n",
    "    If there is an error creating the folder, it will print the error message.\n",
    "\n",
    "    Parameters:\n",
    "        :param path (str): The path of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use os.mkdir to create the folder at the specified path\n",
    "        os.mkdir(path)\n",
    "        print(f\"Folder {path} created\")\n",
    "    except FileExistsError:\n",
    "        # If the folder already exists, print a message saying so\n",
    "        print(f\"Folder {path} already exists\")\n",
    "    except Exception as e:\n",
    "        # If there is an error creating the folder, print the error message\n",
    "        print(f\"Error creating folder {path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initializing Folders\n",
    "This function `init_folder` initializes the specified folders.\n",
    "\n",
    "**Function Parameters:**\n",
    "\n",
    "- `folder_names (list)`: A list of folder names to be created.\n",
    "\n",
    "**Function Behavior:**\n",
    "\n",
    "- The function iterates over the list of folder names and calls the `create_folder` function for each name.\n",
    "- This function is used to create the required output, images, metadata, and include folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:44.507949Z",
     "end_time": "2023-04-23T08:31:44.512405Z"
    }
   },
   "outputs": [],
   "source": [
    "def init_folder(folder_names: list):\n",
    "    for folder_name in folder_names:\n",
    "        create_folder(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:44.782805Z",
     "end_time": "2023-04-23T08:31:44.790654Z"
    }
   },
   "outputs": [],
   "source": [
    "init_folder(list_of_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading a File\n",
    "This function `download` downloads a file from a given URL and saves it to a specified filename.\n",
    "\n",
    "**Function Parameters:**\n",
    "\n",
    "- `url (str)`: The URL of the file to be downloaded.\n",
    "- `filename (str)`: The filename to save the file as.\n",
    "\n",
    "**Function Returns:**\n",
    "\n",
    "- `path (str)`: The path of the downloaded file.\n",
    "\n",
    "**Function Behavior:**\n",
    "\n",
    "- The function creates a `requests.Session` object to persist the state of the connection.\n",
    "- The function then sends a GET request to the URL to start the download.\n",
    "- The function raises an error if the response is not 200 OK.\n",
    "- The function retrieves the file size from the `Content-Length` header and uses it to display the download progress using the `tqdm` library.\n",
    "- The function opens the target file in binary write mode and writes each chunk of data from the response to the file.\n",
    "- The function returns the path to the downloaded file.\n",
    "- If an HTTP error occurs while downloading the file, the function prints an error message.\n",
    "- If any other error occurs while downloading the file, the function prints a general error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:47.420794Z",
     "end_time": "2023-04-23T08:31:47.429602Z"
    }
   },
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    \"\"\"\n",
    "    This download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "    Parameters:\n",
    "        :param url (str): The URL of the file to be downloaded.\n",
    "        :param filename (str): The filename to save the file as.\n",
    "\n",
    "    Returns:\n",
    "    path (str): The path of the downloaded file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a session object to persist the state of connection\n",
    "        s = requests.Session()\n",
    "        # Retry connecting to the URL up to 3 times\n",
    "        s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        # Send a GET request to the URL to start the download\n",
    "        r = s.get(url, stream=True, allow_redirects=True)\n",
    "        # Raise an error if the response is not 200 OK\n",
    "        r.raise_for_status()\n",
    "        # Get the file size from the Content-Length header, default to 0 if not present\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        # Get the absolute path to the target file\n",
    "        path = pathlib.Path(filename).expanduser().resolve()\n",
    "        # Create parent directories if they don't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Set the description to display while downloading, \"(Unknown total file size)\" if file size is 0\n",
    "        desc = \"(Unknown total file size)\" if file_size == 0 else \"\"\n",
    "        # Enable decoding the response content\n",
    "        r.raw.read = functools.partial(r.raw.read, decode_content=True)\n",
    "        # Use tqdm to display the download progress\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "            # Open the target file in binary write mode\n",
    "            with path.open(\"wb\") as f:\n",
    "                # Write each chunk of data from the response to the file\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        # Return the path to the downloaded file\n",
    "        return path\n",
    "    # Handle HTTP error if the response is not 200 OK\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading dataset: {e}\")\n",
    "    # Handle any other exceptions that might occur while downloading the file\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and Extracting the Dataset\n",
    "This function `download_dataset` downloads the dataset from a given URL, unzips it, and stores the images in a specified image path.\n",
    "\n",
    "**Function Parameters:**\n",
    "\n",
    "- `dataset_url (str)`: The URL of the dataset to be downloaded.\n",
    "- `image_path (str)`: The path to store the images after unzipping the dataset.\n",
    "\n",
    "**Function Behavior:**\n",
    "\n",
    "- The function first checks if the dataset has already been downloaded by checking if the `archive.zip` file exists or if the images folder is empty.\n",
    "- If the dataset has not been downloaded, the function downloads it from the given URL using the `download` function.\n",
    "- The function then uses the `zipfile` library to extract the contents of the `archive.zip` file to the specified image path.\n",
    "- The function then removes the `archive.zip` file.\n",
    "- If an error occurs while unzipping the dataset, the function prints an error message.\n",
    "- If an error occurs while removing the `archive.zip` file, the function prints an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:48.785016Z",
     "end_time": "2023-04-23T08:31:48.793073Z"
    }
   },
   "outputs": [],
   "source": [
    "def download_dataset(dataset_url, image_path):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from the given URL, unzips it, and stores the images in the specified image path.\n",
    "\n",
    "    Args:\n",
    "        :param dataset_url (str): URL of the dataset to be downloaded\n",
    "        :param image_path (str): Path to store the images after unzipping the dataset\n",
    "    \"\"\"\n",
    "    # Check if the dataset has already been downloaded\n",
    "    # Check if the archive.zip file exists or if the images folder is empty\n",
    "    if not os.path.exists('archive.zip'):\n",
    "        # Download the dataset from the given url\n",
    "        download(dataset_url, 'archive.zip')\n",
    "        print(\"Dataset downloaded!\")\n",
    "        try:\n",
    "            # Extract the contents of the archive.zip to the specified image path\n",
    "            with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(image_path)\n",
    "            print(\"Dataset unzipped\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while unzipping dataset: {e}\")\n",
    "        try:\n",
    "            # Remove the archive.zip file\n",
    "            os.remove('archive.zip')\n",
    "            print(\"archive.zip removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while removing archive.zip: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:31:50.102005Z",
     "end_time": "2023-04-23T08:32:18.596523Z"
    }
   },
   "outputs": [],
   "source": [
    "download_dataset(dataset_url, images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading and Processing the Photos Data\n",
    "This code block reads the `photos.tsv000` file in the images folder and processes the data.\n",
    "\n",
    "- The `pd.read_csv` method is used to read the `photos.tsv000` file, and the `sep` parameter is set to `'\\t'` to indicate that the data is separated by tabs.\n",
    "- The resulting data is stored in a Pandas DataFrame called `photo_df`.\n",
    "- The `photo_df` DataFrame is then modified to only include the `photo_id` and `photo_image_url` columns.\n",
    "- The `print(photo_df.head())` statement is used to display the first 5 rows of the `photo_df` DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:32:43.578233Z",
     "end_time": "2023-04-23T08:32:43.588440Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read photo.tsv file in images folder\n",
    "photo_df = pd.read_csv(os.path.join(images_path, 'photos.tsv000'), sep='\\t')\n",
    "# read photo_image_url column and photo_id in index\n",
    "photo_df = photo_df[['photo_id', 'photo_image_url']]\n",
    "\n",
    "print(photo_df.head())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:32:43.855561Z",
     "end_time": "2023-04-23T08:32:44.004668Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Downloading an Image Asynchronously\n",
    "This is an asynchronous function `download_image` that downloads an image from a given URL and saves it to the local file system.\n",
    "\n",
    "**Function Parameters:**\n",
    "\n",
    "- `session (aiohttp.ClientSession)`: An `aiohttp` client session that manages HTTP requests and responses.\n",
    "- `url (str)`: The URL of the image to download.\n",
    "- `i (int)`: An integer representing the index of the image to download.\n",
    "- `err_cnt (int, optional)`: An optional integer representing the number of times that the download has failed due to a client error. Defaults to 0.\n",
    "\n",
    "**Function Returns:**\n",
    "\n",
    "- This function does not return anything.\n",
    "\n",
    "**Function Behavior:**\n",
    "\n",
    "- The function uses the `session.get` method from the `aiohttp` library to send a GET request to the URL of the image.\n",
    "- The function opens a file in binary write mode with the filename `image_i.jpg`, where `i` is the index of the image, and writes the content of the response to the file.\n",
    "- If a `ClientError` occurs while downloading the image, the function retries the download up to 10 times, with a 10-second delay between each retry.\n",
    "- If the download still fails after 10 retries, the function stops trying and prints an error message."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def download_image(session: aiohttp.ClientSession, url: str, i: int, err_cnt=None):\n",
    "    \"\"\"\n",
    "    Downloads an image from the given URL using an aiohttp client session and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        session: An aiohttp client session that manages HTTP requests and responses.\n",
    "        url: The URL of the image to download.\n",
    "        i: An integer representing the index of the image to download.\n",
    "        err_cnt: An optional integer representing the number of times that the download has failed due to a client error.\n",
    "                 If not provided, it defaults to 0.\n",
    "\n",
    "    Raises:\n",
    "        This method does not raise any exceptions.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    if err_cnt is None:\n",
    "        err_cnt = 0\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            filename = os.path.join(images_path, \"image_\" + str(i) + \".jpg\")\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(await response.content.read())\n",
    "            print(f\"Downloaded {url} to {filename} idx: {i}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Error occurred while downloading {url}: {e}\")\n",
    "        if err_cnt == 10:\n",
    "            return\n",
    "        await asyncio.sleep(10)\n",
    "        err_cnt += 1\n",
    "        await download_image(session, url, i, err_cnt)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:32:45.908448Z",
     "end_time": "2023-04-23T08:32:45.912640Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Download Images\n",
    "\n",
    "This code defines an asynchronous function `download_images` that downloads a list of images from a set of given URLs. The function uses the `aiohttp` library to manage the HTTP requests and responses during the download process. The function takes two arguments: `image_urls`, a list of strings representing the URLs of the images to be downloaded, and `images_ids`, a list of integers representing the indices of the images to be downloaded.\n",
    "\n",
    "The function starts by creating a new `aiohttp` client session, which will be used to manage the HTTP requests and responses during the download process. A semaphore with a limit of 5000 concurrent downloads is created to prevent overloading the server. The function then loops through the `image_urls` list and creates a new task for each URL using the `asyncio.ensure_future` method. Before creating the task, the function acquires a permit from the semaphore to limit the number of concurrent downloads. Each task calls the `download_image` function to download the image from the URL and save it to the local file system. After the task is created, the function releases the semaphore permit when the task completes.\n",
    "\n",
    "Once all the tasks are created, the function waits for all download tasks to complete using the `asyncio.wait` method. The results of all the tasks are then gathered using the `asyncio.gather` method, although this step is not necessary as the tasks have already completed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def download_images(image_urls, images_ids):\n",
    "    \"\"\"\n",
    "    Downloads a list of images from the given URLs using an aiohttp client session and saves them to the local file system.\n",
    "\n",
    "    Args:\n",
    "        image_urls: A list of strings representing the URLs of the images to download.\n",
    "        images_ids: A list of integers representing the indices of the images to download.\n",
    "\n",
    "    Raises:\n",
    "        This method does not raise any exceptions.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Create a new aiohttp client session to manage HTTP requests and responses\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []  # Create an empty list to hold the tasks that will download the images\n",
    "        semaphore = asyncio.Semaphore(5000)  # Create a semaphore to limit the number of concurrent downloads\n",
    "        # Loop through the image URLs and create a new task for each one\n",
    "        for i, url in enumerate(image_urls):\n",
    "            try:\n",
    "                await semaphore.acquire()  # Acquire a permit from the semaphore to limit concurrency\n",
    "                #url = url + \"?w=1000&fm=jpg&fit=max\"  # Append query parameters to resize and optimize the image\n",
    "                task = asyncio.ensure_future(download_image(session, url, images_ids[i]))  # Create a new download task\n",
    "                task.add_done_callback(\n",
    "                    lambda x: semaphore.release())  # Release the semaphore permit when the task completes\n",
    "                tasks.append(task)  # Add the task to the list of download tasks\n",
    "            except Exception:\n",
    "                print(f\"Error occurred while downloading {url}\")\n",
    "                semaphore.release()  # Release the semaphore permit if an exception occurs\n",
    "        # Wait for all download tasks to complete\n",
    "        await asyncio.wait(tasks)\n",
    "        # Gather the results of all download tasks (not necessary because the tasks have already completed)\n",
    "        await asyncio.gather(*tasks)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:33:16.452710Z",
     "end_time": "2023-04-23T08:33:16.458174Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the list of image urls and image ids\n",
    "image_urls = photo_df['photo_image_url'].values.tolist()[:num_images]\n",
    "# img id are from 0 to size of the list\n",
    "images_ids = [i for i in range(len(image_urls))][:num_images]\n",
    "# filter by looking if the image already exist in fact of the image_id is already in the folder\n",
    "# Loop on the image_id and check if the image exist in the folder\n",
    "image_urls = [url for url, image_id in zip(image_urls, images_ids) if\n",
    "              not os.path.exists(os.path.join(images_path, \"image_\" + str(image_id) + \".jpg\"))]\n",
    "print(f\"Number of images to download: {len(image_urls)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:34:24.655784Z",
     "end_time": "2023-04-23T08:34:24.675754Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The code filters a list of image URLs and their respective image IDs by checking if the images already exist in a specified folder.\n",
    "\n",
    "1. The `image_urls` list is populated with the values of the `photo_image_url` column of the `photo_df` dataframe. The list is then sliced to a specified number of images using the `[:num_images]` syntax.\n",
    "\n",
    "2. The `images_ids` list is created by generating a range of integers from 0 to the length of the `image_urls` list and slicing it to the specified number of images.\n",
    "\n",
    "3. The code then creates a new list `image_urls` that only contains URLs of images that do not already exist in the specified folder. This is done by looping through the `image_urls` and `images_ids` lists and checking if the image file with the corresponding ID exists in the folder. If it does, the URL is not added to the new list.\n",
    "\n",
    "4. Finally, the code prints the number of images that will be downloaded."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the list of image urls into chunks of max and add a timeout of 30 seconds\n",
    "chunks = [image_urls[i:i + 5000] for i in range(0, len(image_urls), 5000)]\n",
    "start_t = time.time()\n",
    "loop = None\n",
    "for i, chunk in enumerate(chunks):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.run_until_complete(download_images(chunk, images_ids[i * 5000:(i + 1) * 5000]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading chunk {i}: {e}\")\n",
    "    finally:\n",
    "        loop.close()\n",
    "        print(f\"[Chunk {i}] Downloaded {len(chunk)} images in {time.time() - start} seconds\")\n",
    "\n",
    "print(f'Downloaded {len(image_urls)} images in {time.time() - start_t} seconds')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:34:27.232960Z",
     "end_time": "2023-04-23T08:34:27.235910Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This code removes all files in the `images_path` directory except for `.jpg` files. It uses the `os.listdir()` method to get a list of all files in the directory and loops through each file. If the file ends with `.jpg`, it continues to the next file. If the file does not end with `.jpg`, it uses the `os.remove()` method to delete the file. However, if the file is named `TERMS.md`, it skips it and does not delete it. If an exception occurs while removing a file, the code continues to the next file.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove all files except images\n",
    "for file in os.listdir(images_path):\n",
    "    if file.endswith('.jpg'):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            # Don't delete TERMS.md\n",
    "            if file == 'TERMS.md':\n",
    "                continue\n",
    "            os.remove(os.path.join(images_path, file))\n",
    "        except Exception as e:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:34:30.267364Z",
     "end_time": "2023-04-23T08:34:30.305385Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get_all_images\n",
    "This function returns a list of full paths to all the images with .png or .jpg extensions in the given path. If an error occurs while fetching images, the function returns an empty list and logs the error message.\n",
    "\n",
    "### Args\n",
    "- path (str): The path to the directory containing the images.\n",
    "\n",
    "### Returns\n",
    "- list: A list of full path to all the images with .png or .jpg extensions.\n",
    "- empty list: An empty list if an error occurred while fetching images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-04-23T08:34:37.300462Z",
     "end_time": "2023-04-23T08:34:37.307786Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_images(path):\n",
    "    \"\"\"Get all images from the given path.\n",
    "\n",
    "    Args:\n",
    "    param: image_path (str): path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "    - list: a list of full path to all the images with png or jpg extensions.\n",
    "    - empty list: an empty list if an error occurred while fetching images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use os.walk to traverse all the subdirectories and get all images\n",
    "        return [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(path)\n",
    "                for name in files\n",
    "                if name.endswith((\".png\", \".jpg\"))]\n",
    "    except Exception as e:\n",
    "        # return an empty list and log the error message if an error occurred\n",
    "        print(f\"An error occurred while fetching images: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `get_all_metadata` coroutine function extracts metadata from all images in a directory and saves the metadata information in either pickle or json format. The function takes two parameters, `images_path` and `metadata_path`, which are the paths to the directory where the images are stored and the directory where the metadata will be saved, respectively.\n",
    "\n",
    "The function starts by executing the binary `exifextract` from the `include_path` and passing `images_path` and `metadata_path/metadata.csv` as arguments. The function then waits for the process to terminate and checks if the process terminated successfully. If the process is not successful, a `subprocess.CalledProcessError` is raised.\n",
    "\n",
    "Once the metadata has been extracted, the function opens the `metadata.csv` file and loads the metadata using a `csv.reader` object. The metadata is stored in a list and the first row of the list is treated as the header. The metadata is then processed and stored in a dictionary where each key represents the index of a metadata item and the value is another dictionary containing the metadata information. The `filename` is also added to the metadata for each item.\n",
    "\n",
    "Finally, the metadata dictionary is converted to a pandas dataframe, and the dataframe is saved to a `metadata.csv` file in the `metadata_path` directory.\n",
    "\n",
    "\n",
    "# Note : https://github.com/TeissierYannis/cpe-bigdata-project-cpp-dependencies (exifextract) build the binary from this repository"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def get_all_metadata(images_path):\n",
    "    \"\"\"\n",
    "    This coroutine extracts metadata from all images in a directory and saves the metadata information in either pickle or json format.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the directory where the images are stored.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Use the binary exifextract from include path\n",
    "    binary = include_path + '/exifextract' # https://github.com/TeissierYannis/cpe-bigdata-project-cpp-dependencies (exifextract) build the binary from this repository\n",
    "    command = [binary, images_path, metadata_path + '/metadata.csv']\n",
    "    import subprocess\n",
    "    # execute command\n",
    "    popen = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "    popen.wait()\n",
    "\n",
    "    # wait for the process to terminate\n",
    "    output, error = popen.communicate()\n",
    "\n",
    "    while popen.poll() is None:\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # check if the process terminated successfully\n",
    "    if popen.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(popen.returncode, command)\n",
    "\n",
    "    # load metadata from csv\n",
    "    with open(metadata_path + '/metadata.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        metadata = list(reader)\n",
    "        header = metadata[0]\n",
    "\n",
    "    metadata = metadata[1:]\n",
    "    metadata_dict = {}\n",
    "    for i, row in enumerate(metadata):\n",
    "        metadata_dict[i] = {}\n",
    "        for j in range(1, len(header)):\n",
    "            metadata_dict[i][header[j]] = row[j]\n",
    "        # add filename to metadata\n",
    "        # remove ' from row[0]\n",
    "        row[0] = row[0].replace(\"'\", '')\n",
    "        metadata_dict[i]['filename'] = row[0]\n",
    "\n",
    "    # convert dict to dataframe\n",
    "    metadata_df = pd.DataFrame.from_dict(metadata_dict, orient='index')\n",
    "    # save metadata to csv\n",
    "    metadata_df.to_csv(metadata_path + '/metadata.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:36:29.337981Z",
     "end_time": "2023-04-23T08:36:29.345895Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "asyncio.run(get_all_metadata(images_path))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:36:30.214124Z",
     "end_time": "2023-04-23T08:36:33.666388Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read metadata from CSV file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_metadata = pd.read_csv(metadata_path + '/metadata.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:36:49.664094Z",
     "end_time": "2023-04-23T08:36:49.681594Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "read_metadata.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-23T08:36:52.975718Z",
     "end_time": "2023-04-23T08:36:53.006570Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
