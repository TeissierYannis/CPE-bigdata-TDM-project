{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "The code imports several libraries to work correctly. The libraries are as follows:\n",
    "\n",
    "pickle: This module implements binary protocols for serializing and de-serializing a Python object structure. It is used here to save the metadata information in binary format.\n",
    "\n",
    "os: This module provides a portable way of using operating system dependent functionality like reading or writing to the file system. It is used here to join paths and create directories.\n",
    "\n",
    "zipfile: This module allows us to read and write ZIP archive files. It is used here to extract images from a compressed archive.\n",
    "\n",
    "requests: This module allows us to send HTTP/1.1 requests using Python. It is used here to download images from a URL.\n",
    "\n",
    "functools: This module provides various functions that can be used to create higher-order functions. It is used here to define the cached_property decorator.\n",
    "\n",
    "pathlib: This module provides a higher-level interface to working with file system paths than the os module. It is used here to create directories.\n",
    "\n",
    "tqdm: This module provides a progress bar that can be used to track the progress of a long-running operation.\n",
    "\n",
    "json: This module provides methods for working with JSON data. It is used here to save the metadata information in JSON format.\n",
    "\n",
    "sqlite3: This module provides a Python interface to SQLite databases. It is used here to create and interact with a SQLite database.\n",
    "\n",
    "pandas: This module is used for data manipulation and analysis. It is used here to read CSV files and create a pandas DataFrame.\n",
    "\n",
    "PIL: This module provides an interface for opening, manipulating, and saving many different image file formats. It is used here to extract metadata from images.\n",
    "\n",
    "nest_asyncio: This module provides a way to run nested event loops in asyncio.\n",
    "\n",
    "asyncio: This module provides infrastructure for writing single-threaded concurrent code using coroutines, multiplexing I/O access over sockets and other resources, running network clients and servers, and other related primitives.\n",
    "\n",
    "aiohttp: This module provides an asynchronous HTTP client/server implementation using asyncio.\n",
    "\n",
    "time: This module provides various time-related functions. It is used here to measure the time taken to download images.\n",
    "\n",
    "subprocess: This module provides a way to spawn new processes, connect to their input/output/error pipes, and obtain their return codes. It is used here to check if the exiftool command line tool is installed.\n",
    "\n",
    "tqdm_asyncio: This module provides a progress bar for asyncio applications. It is used here to track the progress of processing all images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (2.28.2)\r\n",
      "Requirement already satisfied: pathlib in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (1.0.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (4.64.1)\r\n",
      "Requirement already satisfied: pandas in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (1.5.3)\r\n",
      "Requirement already satisfied: pillow in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (9.4.0)\r\n",
      "Requirement already satisfied: nest_asyncio in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (1.5.6)\r\n",
      "Requirement already satisfied: aiohttp in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (3.8.4)\r\n",
      "Requirement already satisfied: mysql-connector-python in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (8.0.32)\r\n",
      "Collecting python-dotenv\r\n",
      "  Using cached python_dotenv-1.0.0-py3-none-any.whl (19 kB)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from requests) (2022.12.7)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from requests) (1.26.14)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from requests) (3.0.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from requests) (3.4)\r\n",
      "Requirement already satisfied: numpy>=1.20.3 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from pandas) (1.24.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from pandas) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from pandas) (2022.7.1)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from aiohttp) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from aiohttp) (4.0.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from aiohttp) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from aiohttp) (1.3.1)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from aiohttp) (1.8.2)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from aiohttp) (22.2.0)\r\n",
      "Requirement already satisfied: protobuf<=3.20.3,>=3.11.0 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from mysql-connector-python) (3.19.6)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/yannisteissier/Documents/bigdata/projet/venv/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\r\n",
      "Installing collected packages: python-dotenv\r\n",
      "Successfully installed python-dotenv-1.0.0\r\n"
     ]
    }
   ],
   "source": [
    "!pip install requests pathlib tqdm pandas pillow nest_asyncio aiohttp mysql-connector-python python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "import aiohttp\n",
    "import time\n",
    "import asyncio\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import csv\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings base variables and paths\n",
    "For this project, we used the unsplash dataset, which is a large-scale image dataset. The dataset contains over 25,000 images.\n",
    "The code sets the base variables and paths for the project. The variables are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "include_path = os.path.join(output_path, \"include\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, include_path]\n",
    "\n",
    "# Set the base URL for the dataset\n",
    "dataset_url = \"https://unsplash.com/data/lite/latest\"\n",
    "# metadata mode (used to save metadata)\n",
    "metadata_mode = \"sqlite\"\n",
    "\n",
    "# Set the number of images to download\n",
    "num_images = 1000\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create folder structure\n",
    "The code creates the folder structure for the project. The folder structure is as follows:\n",
    "- output\n",
    "    - images\n",
    "    - metadata\n",
    "    - config\n",
    "\n",
    "This method creates a folder with the given path if it doesn't already exist, It also outputs a message to inform the user if the folder was created or if it already exists.\n",
    "This is useful for organizing and managing files in a project. By creating a folder to store data and resources, it keeps the working directory tidy and makes it easier to locate files. Additionally, by checking if the folder exists before creating it, it prevents the program from overwriting existing data or throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    This function creates a folder at the specified path.\n",
    "    If the folder already exists, it will print a message saying so.\n",
    "    If there is an error creating the folder, it will print the error message.\n",
    "\n",
    "    Parameters:\n",
    "        :param path (str): The path of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use os.mkdir to create the folder at the specified path\n",
    "        os.mkdir(path)\n",
    "        print(f\"Folder {path} created\")\n",
    "    except FileExistsError:\n",
    "        # If the folder already exists, print a message saying so\n",
    "        print(f\"Folder {path} already exists\")\n",
    "    except Exception as e:\n",
    "        # If there is an error creating the folder, print the error message\n",
    "        print(f\"Error creating folder {path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the folder structure\n",
    "This method initializes a list of folders by calling the create_folder method for each folder in the list.\n",
    "The purpose of this method is to make sure that all necessary folders exist before the program continues its execution.\n",
    "If a folder does not exist, the create_folder method will create it. If a folder already exists, the method will simply print a message indicating that the folder already exists. In case of any other error, the method will print the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_folder(folder_names: list):\n",
    "    for folder_name in folder_names:\n",
    "        create_folder(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_folder(list_of_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods for downloading the dataset\n",
    "The following code block is a method to download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "The method starts by creating a session (s = requests.Session()) and then mounting it to the URL (s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))). This sets the maximum number of retries to 3 if the connection to the URL fails.\n",
    "\n",
    "Then, the method makes a GET request to the URL (r = s.get(url, stream=True, allow_redirects=True)) and checks if it returns a successful response (r.raise_for_status()). If there was an HTTP error during the request, the error message is printed (print(f\"HTTP error occurred while downloading dataset: {e}\")).\n",
    "\n",
    "The method also checks the file size specified in the response headers and assigns it to the variable file_size (file_size = int(r.headers.get('Content-Length', 0))). If the file size is 0, a default value of \"(Unknown total file size)\" is assigned to the variable desc; otherwise, the variable desc is left empty.\n",
    "\n",
    "Next, the method resolves the file path and creates a directory if it doesn't already exist (path.parent.mkdir(parents=True, exist_ok=True)). The method then creates a tqdm progress bar to show the download progress (with tqdm.tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:).\n",
    "\n",
    "Finally, the method writes the contents of the file to disk in chunks (for chunk in r.iter_content(chunk_size=1024):), updating the progress bar for each chunk that is written to disk (pbar.update(len(chunk))). If an error occurred during the download, a message with the error is printed (print(f\"Error occurred while downloading dataset: {e}\")). The file path is returned when the method is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    \"\"\"\n",
    "    This download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "    Parameters:\n",
    "        :param url (str): The URL of the file to be downloaded.\n",
    "        :param filename (str): The filename to save the file as.\n",
    "\n",
    "    Returns:\n",
    "    path (str): The path of the downloaded file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a session object to persist the state of connection\n",
    "        s = requests.Session()\n",
    "        # Retry connecting to the URL up to 3 times\n",
    "        s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        # Send a GET request to the URL to start the download\n",
    "        r = s.get(url, stream=True, allow_redirects=True)\n",
    "        # Raise an error if the response is not 200 OK\n",
    "        r.raise_for_status()\n",
    "        # Get the file size from the Content-Length header, default to 0 if not present\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        # Get the absolute path to the target file\n",
    "        path = pathlib.Path(filename).expanduser().resolve()\n",
    "        # Create parent directories if they don't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Set the description to display while downloading, \"(Unknown total file size)\" if file size is 0\n",
    "        desc = \"(Unknown total file size)\" if file_size == 0 else \"\"\n",
    "        # Enable decoding the response content\n",
    "        r.raw.read = functools.partial(r.raw.read, decode_content=True)\n",
    "        # Use tqdm to display the download progress\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "            # Open the target file in binary write mode\n",
    "            with path.open(\"wb\") as f:\n",
    "                # Write each chunk of data from the response to the file\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        # Return the path to the downloaded file\n",
    "        return path\n",
    "    # Handle HTTP error if the response is not 200 OK\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading dataset: {e}\")\n",
    "    # Handle any other exceptions that might occur while downloading the file\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "The following code block downloads the dataset from the URL and saves it to the specified filename. The method also prints a message to inform the user that the download is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(dataset_url, image_path):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from the given URL, unzips it, and stores the images in the specified image path.\n",
    "\n",
    "    Args:\n",
    "        :param dataset_url (str): URL of the dataset to be downloaded\n",
    "        :param image_path (str): Path to store the images after unzipping the dataset\n",
    "    \"\"\"\n",
    "    # Check if the dataset has already been downloaded\n",
    "    # Check if the archive.zip file exists or if the images folder is empty\n",
    "    if not os.path.exists('archive.zip'):\n",
    "        # Download the dataset from the given url\n",
    "        download(dataset_url, 'archive.zip')\n",
    "        print(\"Dataset downloaded!\")\n",
    "        try:\n",
    "            # Extract the contents of the archive.zip to the specified image path\n",
    "            with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(image_path)\n",
    "            print(\"Dataset unzipped\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while unzipping dataset: {e}\")\n",
    "        try:\n",
    "            # Remove the archive.zip file\n",
    "            os.remove('archive.zip')\n",
    "            print(\"archive.zip removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while removing archive.zip: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dataset(dataset_url, images_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Allow the notebook to run asynchronously"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Read photo.tsv file in images folder\n",
    "photo_df = pd.read_csv(os.path.join(images_path, 'photos.tsv000'), sep='\\t')\n",
    "# read photo_image_url column and photo_id in index\n",
    "photo_df = photo_df[['photo_id', 'photo_image_url']]\n",
    "\n",
    "print(photo_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method downloads an image from a given URL using an asynchronous HTTP client library called aiohttp. The downloaded image is saved to a file on the local file system with a filename in the format \"image_#index.jpg\", where #index is a given integer value.\n",
    "\n",
    "The method takes four arguments:\n",
    "\n",
    "session: an instance of an aiohttp client session that manages HTTP requests and responses.\n",
    "url: a string representing the URL of the image to download.\n",
    "i: an integer representing the index of the image to download.\n",
    "err_cnt: an optional integer representing the number of times that the download has failed due to a client error. If this is not provided, it defaults to 0.\n",
    "The method first checks whether an error count was provided, and if not, sets it to 0. It then attempts to download the image using the aiohttp session.get() method, which returns a response object. The async with statement ensures that the response is properly handled and that the connection to the server is closed when the request is completed.\n",
    "\n",
    "Once the response is obtained, the method constructs a filename using the given index value, and writes the image content to a file with that filename in binary mode. It then prints a message indicating that the image was downloaded and saved to the specified filename.\n",
    "\n",
    "If the download fails due to a client error (e.g., a network timeout), the method catches the error using an except block, prints an error message indicating the URL that failed and the error that occurred, and then waits for 10 seconds before retrying the download. If the error count reaches 10, the method returns without attempting to download the image again.\n",
    "\n",
    "If the download fails due to a server error (e.g., a 404 Not Found response), the exception is not caught and will propagate up the call stack.\n",
    "\n",
    "If the download is successful, the method returns nothing."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def download_image(session: aiohttp.ClientSession, url: str, i: int, err_cnt=None):\n",
    "    \"\"\"\n",
    "    Downloads an image from the given URL using an aiohttp client session and saves it to the local file system.\n",
    "\n",
    "    Args:\n",
    "        session: An aiohttp client session that manages HTTP requests and responses.\n",
    "        url: The URL of the image to download.\n",
    "        i: An integer representing the index of the image to download.\n",
    "        err_cnt: An optional integer representing the number of times that the download has failed due to a client error.\n",
    "                 If not provided, it defaults to 0.\n",
    "\n",
    "    Raises:\n",
    "        This method does not raise any exceptions.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    if err_cnt is None:\n",
    "        err_cnt = 0\n",
    "    try:\n",
    "        async with session.get(url) as response:\n",
    "            filename = os.path.join(images_path, \"image_\" + str(i) + \".jpg\")\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(await response.content.read())\n",
    "            print(f\"Downloaded {url} to {filename} idx: {i}\")\n",
    "    except aiohttp.ClientError as e:\n",
    "        print(f\"Error occurred while downloading {url}: {e}\")\n",
    "        if err_cnt == 10:\n",
    "            return\n",
    "        await asyncio.sleep(10)\n",
    "        err_cnt += 1\n",
    "        await download_image(session, url, i, err_cnt)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method, download_images, is a function that downloads a list of images from the web using the aiohttp library and saves them to the local file system. The method takes two arguments: image_urls, a list of URLs where the images are hosted, and images_ids, a list of identifiers for the images that will be used to name the files when they are saved locally.\n",
    "\n",
    "The method starts by creating an aiohttp.ClientSession object, which is used to manage HTTP requests and responses. It then initializes an empty list tasks and creates a semaphore object with a maximum value of 5000. The semaphore is used to limit the number of concurrent downloads and prevent overloading the server.\n",
    "\n",
    "Next, the method loops through the image_urls list using the built-in enumerate function to keep track of the index of each URL. For each URL, the method tries to acquire a permit from the semaphore to start a new download. If the maximum number of concurrent downloads has been reached, the method blocks until a permit becomes available.\n",
    "\n",
    "Once a permit is acquired, the method creates a new task using the asyncio.ensure_future function and adds it to the tasks list. The task represents the asynchronous download of the image from the current URL using the download_image method. A callback function is also added to the task that releases the semaphore permit when the task completes, so that another download can start.\n",
    "\n",
    "If an error occurs while trying to download an image, the method prints an error message and releases the semaphore permit. The method then continues with the next URL in the list.\n",
    "\n",
    "After all tasks have been created and added to the tasks list, the method waits for them to complete using the asyncio.wait function. Finally, it gathers all the completed tasks using the asyncio.gather function, which ensures that all tasks have finished before the method returns.\n",
    "\n",
    "Overall, the download_images method is an efficient and asynchronous way to download a large number of images from the web and save them to the local file system."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def download_images(image_urls, images_ids):\n",
    "    \"\"\"\n",
    "    Downloads a list of images from the given URLs using an aiohttp client session and saves them to the local file system.\n",
    "\n",
    "    Args:\n",
    "        image_urls: A list of strings representing the URLs of the images to download.\n",
    "        images_ids: A list of integers representing the indices of the images to download.\n",
    "\n",
    "    Raises:\n",
    "        This method does not raise any exceptions.\n",
    "\n",
    "    Returns:\n",
    "        None.\n",
    "    \"\"\"\n",
    "    # Create a new aiohttp client session to manage HTTP requests and responses\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []  # Create an empty list to hold the tasks that will download the images\n",
    "        semaphore = asyncio.Semaphore(5000)  # Create a semaphore to limit the number of concurrent downloads\n",
    "        # Loop through the image URLs and create a new task for each one\n",
    "        for i, url in enumerate(image_urls):\n",
    "            try:\n",
    "                await semaphore.acquire()  # Acquire a permit from the semaphore to limit concurrency\n",
    "                #url = url + \"?w=1000&fm=jpg&fit=max\"  # Append query parameters to resize and optimize the image\n",
    "                task = asyncio.ensure_future(download_image(session, url, images_ids[i]))  # Create a new download task\n",
    "                task.add_done_callback(\n",
    "                    lambda x: semaphore.release())  # Release the semaphore permit when the task completes\n",
    "                tasks.append(task)  # Add the task to the list of download tasks\n",
    "            except Exception:\n",
    "                print(f\"Error occurred while downloading {url}\")\n",
    "                semaphore.release()  # Release the semaphore permit if an exception occurs\n",
    "        # Wait for all download tasks to complete\n",
    "        await asyncio.wait(tasks)\n",
    "        # Gather the results of all download tasks (not necessary because the tasks have already completed)\n",
    "        await asyncio.gather(*tasks)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the list of image urls and image ids\n",
    "image_urls = photo_df['photo_image_url'].values.tolist()[:num_images]\n",
    "# img id are from 0 to size of the list\n",
    "images_ids = [i for i in range(len(image_urls))][:num_images]\n",
    "# filter by looking if the image already exist in fact of the image_id is already in the folder\n",
    "# Loop on the image_id and check if the image exist in the folder\n",
    "image_urls = [url for url, image_id in zip(image_urls, images_ids) if\n",
    "              not os.path.exists(os.path.join(images_path, \"image_\" + str(image_id) + \".jpg\"))]\n",
    "print(f\"Number of images to download: {len(image_urls)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the list of image urls into chunks of max and add a timeout of 30 seconds\n",
    "chunks = [image_urls[i:i + 5000] for i in range(0, len(image_urls), 5000)]\n",
    "start_t = time.time()\n",
    "loop = None\n",
    "for i, chunk in enumerate(chunks):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.run_until_complete(download_images(chunk, images_ids[i * 5000:(i + 1) * 5000]))\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading chunk {i}: {e}\")\n",
    "    finally:\n",
    "        loop.close()\n",
    "        print(f\"[Chunk {i}] Downloaded {len(chunk)} images in {time.time() - start} seconds\")\n",
    "\n",
    "print(f'Downloaded {len(image_urls)} images in {time.time() - start_t} seconds')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Clear the images folder from all files except images"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove all files except images\n",
    "for file in os.listdir(images_path):\n",
    "    if file.endswith('.jpg'):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            # Don't delete TERMS.md\n",
    "            if file == 'TERMS.md':\n",
    "                continue\n",
    "            os.remove(os.path.join(images_path, file))\n",
    "        except Exception as e:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to get all the image paths\n",
    "The get_all_images method is used to retrieve all images present in the specified image path. It uses the os.walk function to traverse through all subdirectories within the image path and collects the file names that end with either '.png' or '.jpg' extensions. The full path of each image is then generated by joining the root directory and the file name. The method returns a list of all images' full paths. In case of any error, an error message is printed and an empty list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(path):\n",
    "    \"\"\"Get all images from the given path.\n",
    "\n",
    "    Args:\n",
    "    param: image_path (str): path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "    - list: a list of full path to all the images with png or jpg extensions.\n",
    "    - empty list: an empty list if an error occurred while fetching images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use os.walk to traverse all the subdirectories and get all images\n",
    "        return [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(path)\n",
    "                for name in files\n",
    "                if name.endswith((\".png\", \".jpg\"))]\n",
    "    except Exception as e:\n",
    "        # return an empty list and log the error message if an error occurred\n",
    "        print(f\"An error occurred while fetching images: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to get metadata\n",
    "The goal of the get_metadata method is to extract metadata information from a list of image files and return it in a dictionary format. This method uses the ExifTool software to extract the metadata information from the images. The input parameter is a string containing all image file paths separated by a space. The output of the method is a dictionary containing the metadata information of the images. If an error occurs for any image, the metadata for that image will be None. This method is implemented as an asynchronous coroutine using the asyncio module in Python.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "This function takes a list of dictionaries containing metadata information of images and generates a list of SQL requests to insert the metadata into a database.\n",
    "\n",
    "The function first creates an empty list to store the SQL requests. It then loops over each metadata dictionary in the input list using the tqdm function to provide a progress bar. For each metadata dictionary, the function extracts the filename of the image and then loops over all the items in the dictionary.\n",
    "\n",
    "For each key-value pair in the metadata dictionary, the function creates an SQL request to insert the metadata into the database. The SQL request is in the form of a string that contains the filename, key, and value of the metadata item. The function adds each SQL request to the list of SQL requests.\n",
    "\n",
    "After processing all the metadata dictionaries, the function returns the list of SQL requests."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_sql_requests(metadatas):\n",
    "    \"\"\"\n",
    "    This function generates a list of SQL requests to insert metadata into a database.\n",
    "\n",
    "    Parameters:\n",
    "    metadatas (list): A list of dictionaries containing the metadata information of the images.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of SQL requests to insert metadata into a database.\n",
    "    \"\"\"\n",
    "    # Create a list to store SQL requests\n",
    "    sql_requests = []\n",
    "    # Loop over all metadata\n",
    "    for i, metadata in enumerate(metadatas):\n",
    "        try:\n",
    "            # Get the filename of the image\n",
    "            filename = metadatas[i]['filename']\n",
    "\n",
    "            # Loop over all metadata items\n",
    "            for key, value in metadatas[i].items():\n",
    "                # Create SQL request to insert metadata into database\n",
    "                # replace \" by space\n",
    "                value = value.replace('\"', ' ')\n",
    "                # replace ' by space\n",
    "                value = value.replace(\"'\", ' ')\n",
    "\n",
    "                sql_request = f\"INSERT INTO metadata VALUES ('{filename}', '{key}', '{value}')\"\n",
    "                # Add SQL request to list\n",
    "                sql_requests.append(sql_request)\n",
    "\n",
    "        except Exception as e:\n",
    "            # Print an error message if an error occurs\n",
    "            print(\"An error occurred while generating SQL requests: \", e)\n",
    "            continue\n",
    "    # Return the list of SQL requests\n",
    "    return sql_requests"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This method executes a SQL query on a SQLite database. The method takes a single parameter, queries, which should be a valid SQL queries that is compatible with the SQLite database.\n",
    "\n",
    "The method first establishes a connection to the SQLite database using the connect() method of the sqlite3 module in Python. It then executes the SQL query using the execute() method of the connection object. After executing the query, the method commits the changes to the database using the commit() method of the connection object, and then closes the connection using the close() method of the connection object.\n",
    "\n",
    "This method is typically used to insert metadata information into a SQLite database. It assumes that the SQLite database already exists and is located in the metadata_path directory."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_server_connection(host_name, user_name, user_password, db_name):\n",
    "    connection = None\n",
    "    try:\n",
    "        connection = mysql.connector.connect(\n",
    "            host=host_name,\n",
    "            user=user_name,\n",
    "            passwd=user_password,\n",
    "            database=db_name\n",
    "        )\n",
    "        print(\"MySQL Database connection successful\")\n",
    "    except Error as err:\n",
    "        print(f\"Error: '{err}'\")\n",
    "\n",
    "    return connection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an asynchronous coroutine get_all_metadata that extracts metadata from all images in a directory and saves the metadata information in either pickle or JSON format. The function takes two parameters: image_path which is the path to the directory where the images are stored, and metadata_path which is the path to the directory where the metadata will be saved.\n",
    "\n",
    "Firstly, the function retrieves a list of all images in the directory using the get_all_images method. It then creates a Semaphore object with a limit of 5000 to limit the number of simultaneous coroutines to 5000.\n",
    "\n",
    "A progress bar is created using the tqdm_asyncio library to track the progress of processing all images. A list of coroutines is created using a list comprehension with each coroutine being an instance of the get_metadata method applied to each image file in the directory.\n",
    "\n",
    "The coroutines are then executed concurrently using the asyncio.as_completed method, with a maximum of 5000 coroutines being executed at a time, and their results are appended to a metadatas list. For each successfully extracted metadata, a SQL query is generated and executed to insert the metadata into the database using the gen_sql_requests and execute_query functions.\n",
    "\n",
    "Once all coroutines are completed, the metadata information is saved into the database. If an error occurs during the metadata extraction process, the None value is returned for that image and the error message is printed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "async def get_all_metadata(images_path):\n",
    "    \"\"\"\n",
    "    This coroutine extracts metadata from all images in a directory and saves the metadata information in either pickle or json format.\n",
    "\n",
    "    Parameters:\n",
    "    image_path (str): The path to the directory where the images are stored.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Use the binary exifextract from include path\n",
    "    binary = include_path + '/exifextract'\n",
    "    command = [binary, images_path, metadata_path + '/metadata.csv']\n",
    "    import subprocess\n",
    "    # execute command\n",
    "    popen = subprocess.Popen(command, stdout=subprocess.PIPE)\n",
    "    popen.wait()\n",
    "\n",
    "    # wait for the process to terminate\n",
    "    output, error = popen.communicate()\n",
    "\n",
    "    while popen.poll() is None:\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    # check if the process terminated successfully\n",
    "    if popen.returncode != 0:\n",
    "        raise subprocess.CalledProcessError(popen.returncode, command)\n",
    "\n",
    "    # load metadata from csv\n",
    "    with open(metadata_path + '/metadata.csv', 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        metadata = list(reader)\n",
    "        header = metadata[0]\n",
    "\n",
    "    metadata = metadata[1:]\n",
    "    metadata_dict = {}\n",
    "    for i, row in enumerate(metadata):\n",
    "        metadata_dict[i] = {}\n",
    "        for j in range(1, len(header)):\n",
    "            metadata_dict[i][header[j]] = row[j]\n",
    "        # add filename to metadata\n",
    "        # remove ' from row[0]\n",
    "        row[0] = row[0].replace(\"'\", '')\n",
    "        metadata_dict[i]['filename'] = row[0]\n",
    "\n",
    "\n",
    "    # save metadata to database\n",
    "    print(\"Generating SQL requests...\")\n",
    "    queries = gen_sql_requests(metadata_dict)\n",
    "    # save to file queries.sql\n",
    "    with open(metadata_path + '/queries.sql', 'w') as f:\n",
    "        f.write(\";\\n\".join(queries))\n",
    "\n",
    "\n",
    "    print(\"Saving metadata to database...\")\n",
    "    # TODO: Save to database\n",
    "    conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "    # Execute file to database\n",
    "    cursor = conn.cursor()\n",
    "    #cursor.execute(\"DROP TABLE IF EXISTS metadata\")\n",
    "    #cursor.execute(\"CREATE TABLE metadata (filename VARCHAR(255), key VARCHAR(255), value VARCHAR(255))\")\n",
    "    cursor.execute(\"LOAD DATA LOCAL INFILE '\" + metadata_path + \"/queries.sql' INTO TABLE metadata FIELDS TERMINATED BY ';' LINES TERMINATED BY '\\n'\")\n",
    "    conn.commit()\n",
    "    conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asyncio.run(get_all_metadata(images_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to look at the metadata (sqlite format)\n",
    "This is the way to look at the metadata information of an image in sqlite database format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "conn = create_server_connection(sql_host, sql_user, sql_password, sql_database)\n",
    "\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT * FROM metadata WHERE filename = 'image_0.jpg'\")\n",
    "\n",
    "result = cursor.fetchall()\n",
    "for x in result:\n",
    "    print(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
