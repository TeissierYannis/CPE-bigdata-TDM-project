{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "The code imports several libraries to work correctly. The libraries are as follows:\n",
    "\n",
    "- tqdm is a library that provides a progress bar for loops and iteration, which is helpful for keeping track of the progress of long-running processes.\n",
    "- PIL (Python Imaging Library) is a library used for opening, manipulating, and saving image files.\n",
    "- The pickle library allows the user to serialize and deserialize Python objects, meaning that the user can save a Python object to a file and then load it back later.\n",
    "- The os library provides a way of using operating system dependent functionality like reading or writing to the file system.\n",
    "- The zipfile library provides functionality to create, read, write, append, and list a ZIP file.\n",
    "- requests library is used for sending HTTP requests to web servers and downloading content.\n",
    "- functools is a module that implements higher-order functions. Higher-order functions are functions that take other functions as inputs, or that return functions as output.\n",
    "- pathlib is a library for working with file paths, which is part of the Python Standard Library starting from Python 3.4.\n",
    "- Finally, tqdm.auto automatically enables or disables the progress bar depending on the context of use, so that it only shows when the output is connected to a terminal or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pickle\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings base variables and paths\n",
    "For this project, we used the unsplash dataset, which is a large-scale image dataset. The dataset contains over 25,000 images.\n",
    "The code sets the base variables and paths for the project. The variables are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "config_path = os.path.join(output_path, \"config\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, config_path]\n",
    "\n",
    "# Set the base URL for the dataset\n",
    "dataset_url = \"https://unsplash-datasets.s3.amazonaws.com/lite/latest/unsplash-research-dataset-lite-latest.zip\"\n",
    "# metadata mode (used to save metadata)\n",
    "metadata_mode = \"sqlite\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create folder structure\n",
    "The code creates the folder structure for the project. The folder structure is as follows:\n",
    "- output\n",
    "    - images\n",
    "    - metadata\n",
    "    - config\n",
    "\n",
    "This method creates a folder with the given path if it doesn't already exist, It also outputs a message to inform the user if the folder was created or if it already exists.\n",
    "This is useful for organizing and managing files in a project. By creating a folder to store data and resources, it keeps the working directory tidy and makes it easier to locate files. Additionally, by checking if the folder exists before creating it, it prevents the program from overwriting existing data or throwing an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    This function creates a folder at the specified path.\n",
    "    If the folder already exists, it will print a message saying so.\n",
    "    If there is an error creating the folder, it will print the error message.\n",
    "\n",
    "    Parameters:\n",
    "        :param path (str): The path of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use os.mkdir to create the folder at the specified path\n",
    "        os.mkdir(path)\n",
    "        print(f\"Folder {path} created\")\n",
    "    except FileExistsError:\n",
    "        # If the folder already exists, print a message saying so\n",
    "        print(f\"Folder {path} already exists\")\n",
    "    except Exception as e:\n",
    "        # If there is an error creating the folder, print the error message\n",
    "        print(f\"Error creating folder {path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the folder structure\n",
    "This method initializes a list of folders by calling the create_folder method for each folder in the list.\n",
    "The purpose of this method is to make sure that all necessary folders exist before the program continues its execution.\n",
    "If a folder does not exist, the create_folder method will create it. If a folder already exists, the method will simply print a message indicating that the folder already exists. In case of any other error, the method will print the error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_folder(folder_names: list):\n",
    "    for folder_name in folder_names:\n",
    "        create_folder(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ../output created\n",
      "Folder ../output/images created\n",
      "Folder ../output/metadata created\n",
      "Folder ../output/config created\n"
     ]
    }
   ],
   "source": [
    "init_folder(list_of_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods for downloading the dataset\n",
    "The following code block is a method to download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "The method starts by creating a session (s = requests.Session()) and then mounting it to the URL (s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))). This sets the maximum number of retries to 3 if the connection to the URL fails.\n",
    "\n",
    "Then, the method makes a GET request to the URL (r = s.get(url, stream=True, allow_redirects=True)) and checks if it returns a successful response (r.raise_for_status()). If there was an HTTP error during the request, the error message is printed (print(f\"HTTP error occurred while downloading dataset: {e}\")).\n",
    "\n",
    "The method also checks the file size specified in the response headers and assigns it to the variable file_size (file_size = int(r.headers.get('Content-Length', 0))). If the file size is 0, a default value of \"(Unknown total file size)\" is assigned to the variable desc; otherwise, the variable desc is left empty.\n",
    "\n",
    "Next, the method resolves the file path and creates a directory if it doesn't already exist (path.parent.mkdir(parents=True, exist_ok=True)). The method then creates a tqdm progress bar to show the download progress (with tqdm.tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:).\n",
    "\n",
    "Finally, the method writes the contents of the file to disk in chunks (for chunk in r.iter_content(chunk_size=1024):), updating the progress bar for each chunk that is written to disk (pbar.update(len(chunk))). If an error occurred during the download, a message with the error is printed (print(f\"Error occurred while downloading dataset: {e}\")). The file path is returned when the method is finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    \"\"\"\n",
    "    This download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "    Parameters:\n",
    "        :param url (str): The URL of the file to be downloaded.\n",
    "        :param filename (str): The filename to save the file as.\n",
    "\n",
    "    Returns:\n",
    "    path (str): The path of the downloaded file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a session object to persist the state of connection\n",
    "        s = requests.Session()\n",
    "        # Retry connecting to the URL up to 3 times\n",
    "        s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        # Send a GET request to the URL to start the download\n",
    "        r = s.get(url, stream=True, allow_redirects=True)\n",
    "        # Raise an error if the response is not 200 OK\n",
    "        r.raise_for_status()\n",
    "        # Get the file size from the Content-Length header, default to 0 if not present\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        # Get the absolute path to the target file\n",
    "        path = pathlib.Path(filename).expanduser().resolve()\n",
    "        # Create parent directories if they don't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Set the description to display while downloading, \"(Unknown total file size)\" if file size is 0\n",
    "        desc = \"(Unknown total file size)\" if file_size == 0 else \"\"\n",
    "        # Enable decoding the response content\n",
    "        r.raw.read = functools.partial(r.raw.read, decode_content=True)\n",
    "        # Use tqdm to display the download progress\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "            # Open the target file in binary write mode\n",
    "            with path.open(\"wb\") as f:\n",
    "                # Write each chunk of data from the response to the file\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        # Return the path to the downloaded file\n",
    "        return path\n",
    "    # Handle HTTP error if the response is not 200 OK\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading dataset: {e}\")\n",
    "    # Handle any other exceptions that might occur while downloading the file\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the dataset\n",
    "The following code block downloads the dataset from the URL and saves it to the specified filename. The method also prints a message to inform the user that the download is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_dataset(dataset_url, image_path):\n",
    "    \"\"\"\n",
    "    Downloads the dataset from the given URL, unzips it, and stores the images in the specified image path.\n",
    "\n",
    "    Args:\n",
    "        :param dataset_url (str): URL of the dataset to be downloaded\n",
    "        :param image_path (str): Path to store the images after unzipping the dataset\n",
    "    \"\"\"\n",
    "    # Check if the dataset has already been downloaded\n",
    "    # Check if the archive.zip file exists or if the images folder is empty\n",
    "    if not os.path.exists('archive.zip'):\n",
    "        # Download the dataset from the given url\n",
    "        download(dataset_url, 'archive.zip')\n",
    "        print(\"Dataset downloaded!\")\n",
    "        try:\n",
    "            # Extract the contents of the archive.zip to the specified image path\n",
    "            with zipfile.ZipFile('archive.zip', 'r') as zip_ref:\n",
    "                zip_ref.extractall(image_path)\n",
    "            print(\"Dataset unzipped\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while unzipping dataset: {e}\")\n",
    "        try:\n",
    "            # Remove the archive.zip file\n",
    "            os.remove('archive.zip')\n",
    "            print(\"archive.zip removed\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while removing archive.zip: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 632M/632M [00:30<00:00, 20.7MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset downloaded!\n",
      "Dataset unzipped\n",
      "archive.zip removed\n"
     ]
    }
   ],
   "source": [
    "download_dataset(dataset_url, images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.56M/1.56M [00:00<00:00, 13.7MB/s]\n",
      "100%|██████████| 2.92M/2.92M [00:00<00:00, 22.5MB/s]\n",
      "100%|██████████| 1.13M/1.13M [00:00<00:00, 11.7MB/s]\n",
      "100%|██████████| 6.92M/6.92M [00:00<00:00, 31.2MB/s]\n",
      "100%|██████████| 7.53M/7.53M [00:00<00:00, 33.5MB/s]\n",
      "100%|██████████| 1.00M/1.00M [00:00<00:00, 11.3MB/s]\n",
      "100%|██████████| 603k/603k [00:00<00:00, 7.99MB/s]\n",
      "100%|██████████| 399k/399k [00:00<00:00, 6.12MB/s]\n",
      "100%|██████████| 3.68M/3.68M [00:00<00:00, 23.4MB/s]\n",
      "100%|██████████| 4.15M/4.15M [00:00<00:00, 26.2MB/s]\n",
      "100%|██████████| 7.21M/7.21M [00:00<00:00, 34.3MB/s]\n",
      "100%|██████████| 5.96M/5.96M [00:00<00:00, 21.8MB/s]\n",
      "100%|██████████| 6.68M/6.68M [00:00<00:00, 29.2MB/s]\n",
      "100%|██████████| 5.41M/5.41M [00:00<00:00, 27.2MB/s]\n",
      "100%|██████████| 4.92M/4.92M [00:00<00:00, 27.5MB/s]\n",
      "100%|██████████| 4.39M/4.39M [00:00<00:00, 25.0MB/s]\n",
      "100%|██████████| 1.02M/1.02M [00:00<00:00, 10.6MB/s]\n",
      "100%|██████████| 11.7M/11.7M [00:00<00:00, 33.1MB/s]\n",
      "100%|██████████| 5.72M/5.72M [00:00<00:00, 26.0MB/s]\n",
      "100%|██████████| 1.12M/1.12M [00:00<00:00, 13.7MB/s]\n",
      "100%|██████████| 2.50M/2.50M [00:00<00:00, 16.7MB/s]\n",
      "100%|██████████| 3.92M/3.92M [00:00<00:00, 21.8MB/s]\n",
      "100%|██████████| 6.66M/6.66M [00:00<00:00, 26.8MB/s]\n",
      "100%|██████████| 8.81M/8.81M [00:00<00:00, 31.1MB/s]\n",
      "100%|██████████| 3.76M/3.76M [00:00<00:00, 21.3MB/s]\n",
      "100%|██████████| 8.07M/8.07M [00:00<00:00, 26.7MB/s]\n",
      "100%|██████████| 18.4M/18.4M [00:00<00:00, 38.4MB/s]\n",
      "100%|██████████| 15.8M/15.8M [00:00<00:00, 38.9MB/s]\n",
      "100%|██████████| 7.93M/7.93M [00:00<00:00, 21.9MB/s]\n",
      "100%|██████████| 16.1M/16.1M [00:00<00:00, 40.5MB/s]\n",
      "100%|██████████| 2.65M/2.65M [00:00<00:00, 19.5MB/s]\n",
      "100%|██████████| 7.75M/7.75M [00:00<00:00, 26.2MB/s]\n",
      "100%|██████████| 3.99M/3.99M [00:00<00:00, 25.7MB/s]\n",
      "100%|██████████| 5.95M/5.95M [00:00<00:00, 26.2MB/s]\n",
      "100%|██████████| 8.56M/8.56M [00:00<00:00, 32.5MB/s]\n",
      "100%|██████████| 4.28M/4.28M [00:00<00:00, 26.4MB/s]\n",
      "100%|██████████| 9.61M/9.61M [00:00<00:00, 37.0MB/s]\n",
      "100%|██████████| 6.92M/6.92M [00:00<00:00, 30.1MB/s]\n",
      "100%|██████████| 7.43M/7.43M [00:00<00:00, 30.8MB/s]\n",
      "100%|██████████| 20.5M/20.5M [00:00<00:00, 46.2MB/s]\n",
      "100%|██████████| 4.39M/4.39M [00:00<00:00, 26.6MB/s]\n",
      "100%|██████████| 4.50M/4.50M [00:00<00:00, 25.3MB/s]\n",
      "100%|██████████| 16.7M/16.7M [00:00<00:00, 37.3MB/s]\n",
      "100%|██████████| 10.0M/10.0M [00:00<00:00, 13.1MB/s]\n",
      "100%|██████████| 2.82M/2.82M [00:00<00:00, 19.3MB/s]\n",
      "100%|██████████| 5.04M/5.04M [00:00<00:00, 27.7MB/s]\n",
      "100%|██████████| 5.37M/5.37M [00:00<00:00, 28.3MB/s]\n",
      "100%|██████████| 709k/709k [00:00<00:00, 6.04MB/s]\n",
      "100%|██████████| 7.49M/7.49M [00:01<00:00, 3.95MB/s]\n",
      "100%|██████████| 20.8M/20.8M [00:00<00:00, 41.2MB/s]\n",
      "100%|██████████| 3.11M/3.11M [00:00<00:00, 22.0MB/s]\n",
      "100%|██████████| 2.46M/2.46M [00:00<00:00, 21.0MB/s]\n",
      "100%|██████████| 15.0M/15.0M [00:00<00:00, 36.5MB/s]\n",
      "100%|██████████| 5.33M/5.33M [00:00<00:00, 27.2MB/s]\n",
      "100%|██████████| 2.34M/2.34M [00:00<00:00, 18.3MB/s]\n",
      "100%|██████████| 1.52M/1.52M [00:00<00:00, 15.9MB/s]\n",
      "100%|██████████| 8.33M/8.33M [00:00<00:00, 33.2MB/s]\n",
      "100%|██████████| 3.74M/3.74M [00:00<00:00, 22.7MB/s]\n",
      "100%|██████████| 1.86M/1.86M [00:00<00:00, 17.2MB/s]\n",
      "100%|██████████| 3.31M/3.31M [00:00<00:00, 23.0MB/s]\n",
      "100%|██████████| 11.2M/11.2M [00:00<00:00, 18.3MB/s]\n",
      "100%|██████████| 474k/474k [00:00<00:00, 7.06MB/s]\n",
      "100%|██████████| 6.40M/6.40M [00:00<00:00, 28.4MB/s]\n",
      "100%|██████████| 3.84M/3.84M [00:00<00:00, 25.2MB/s]\n",
      "100%|██████████| 2.42M/2.42M [00:00<00:00, 19.5MB/s]\n",
      "100%|██████████| 4.41M/4.41M [00:00<00:00, 26.9MB/s]\n",
      "100%|██████████| 15.8M/15.8M [00:00<00:00, 41.3MB/s]\n",
      "100%|██████████| 392k/392k [00:00<00:00, 6.04MB/s]\n",
      "100%|██████████| 2.06M/2.06M [00:00<00:00, 17.9MB/s]\n",
      "100%|██████████| 5.61M/5.61M [00:00<00:00, 26.9MB/s]\n",
      "100%|██████████| 10.2M/10.2M [00:00<00:00, 36.4MB/s]\n",
      "100%|██████████| 8.53M/8.53M [00:00<00:00, 29.2MB/s]\n",
      "100%|██████████| 24.0M/24.0M [00:00<00:00, 39.2MB/s]\n",
      "100%|██████████| 860k/860k [00:00<00:00, 9.77MB/s]\n",
      "100%|██████████| 3.46M/3.46M [00:00<00:00, 22.6MB/s]\n",
      "100%|██████████| 18.1M/18.1M [00:00<00:00, 40.4MB/s]\n",
      "100%|██████████| 1.71M/1.71M [00:00<00:00, 15.9MB/s]\n",
      "100%|██████████| 10.8M/10.8M [00:00<00:00, 36.3MB/s]\n",
      "100%|██████████| 1.02M/1.02M [00:00<00:00, 10.3MB/s]\n",
      "100%|██████████| 48.3M/48.3M [00:01<00:00, 39.6MB/s]\n",
      "100%|██████████| 5.86M/5.86M [00:00<00:00, 13.7MB/s]\n",
      "100%|██████████| 5.45M/5.45M [00:00<00:00, 26.2MB/s]\n",
      "100%|██████████| 773k/773k [00:00<00:00, 7.26MB/s]\n",
      "100%|██████████| 5.65M/5.65M [00:00<00:00, 15.4MB/s]\n",
      "100%|██████████| 10.4M/10.4M [00:00<00:00, 33.9MB/s]\n",
      "100%|██████████| 6.04M/6.04M [00:00<00:00, 17.8MB/s]\n",
      "100%|██████████| 8.93M/8.93M [00:00<00:00, 31.7MB/s]\n",
      "100%|██████████| 12.7M/12.7M [00:00<00:00, 39.3MB/s]\n",
      "100%|██████████| 15.9M/15.9M [00:00<00:00, 36.9MB/s]\n",
      "100%|██████████| 1.18M/1.18M [00:00<00:00, 12.4MB/s]\n",
      "100%|██████████| 23.7M/23.7M [00:00<00:00, 46.4MB/s]\n",
      "100%|██████████| 13.2M/13.2M [00:00<00:00, 40.4MB/s]\n",
      "100%|██████████| 19.8M/19.8M [00:00<00:00, 27.5MB/s]\n",
      "100%|██████████| 17.2M/17.2M [00:00<00:00, 40.4MB/s]\n",
      "100%|██████████| 19.4M/19.4M [00:00<00:00, 35.5MB/s]\n",
      "100%|██████████| 8.66M/8.66M [00:00<00:00, 23.4MB/s]\n",
      "100%|██████████| 8.06M/8.06M [00:00<00:00, 27.0MB/s]\n",
      "100%|██████████| 10.1M/10.1M [00:00<00:00, 35.0MB/s]\n",
      "100%|██████████| 17.6M/17.6M [00:00<00:00, 39.5MB/s]\n",
      "100%|██████████| 4.80M/4.80M [00:00<00:00, 25.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Read photo.tsv file in images folder\n",
    "photo_df = pd.read_csv(os.path.join(images_path, 'photos.tsv000'), sep='\\t')\n",
    "# read photo_image_url column\n",
    "photo_df = photo_df['photo_image_url']\n",
    "# Download 100 images from the photo_image_url column and save them in the images folder\n",
    "for i in range(100):\n",
    "    download(photo_df[i], os.path.join(images_path, f'image_{i}.jpg'))\n",
    "\n",
    "# Remove all files except images\n",
    "for file in os.listdir(images_path):\n",
    "    if file.endswith('.jpg'):\n",
    "        continue\n",
    "    else:\n",
    "        try:\n",
    "            # Don't delete TERMS.md\n",
    "            if file == 'TERMS.md':\n",
    "                continue\n",
    "            os.remove(os.path.join(images_path, file))\n",
    "        except Exception as e:\n",
    "            continue"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to get all the image paths\n",
    "The get_all_images method is used to retrieve all images present in the specified image path. It uses the os.walk function to traverse through all subdirectories within the image path and collects the file names that end with either '.png' or '.jpg' extensions. The full path of each image is then generated by joining the root directory and the file name. The method returns a list of all images' full paths. In case of any error, an error message is printed and an empty list is returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(path):\n",
    "    \"\"\"Get all images from the given path.\n",
    "\n",
    "    Args:\n",
    "    param: image_path (str): path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "    - list: a list of full path to all the images with png or jpg extensions.\n",
    "    - empty list: an empty list if an error occurred while fetching images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use os.walk to traverse all the subdirectories and get all images\n",
    "        return [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(path)\n",
    "                for name in files\n",
    "                if name.endswith((\".png\", \".jpg\"))]\n",
    "    except Exception as e:\n",
    "        # return an empty list and log the error message if an error occurred\n",
    "        print(f\"An error occurred while fetching images: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to set a checkpoint system\n",
    "\n",
    "## Method: create_checkpoint\n",
    "This method is used to create a checkpoint file containing the latest processed image.\n",
    "\n",
    "The method first tries to open a file named checkpoint.txt in write mode. Then it writes the latest_file into it. If any error occurs during this process, the error message is printed with the message \"An error occurred while creating checkpoint: [error message]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to create a checkpoint with the latest file\n",
    "def create_checkpoint(latest_file):\n",
    "    \"\"\"\n",
    "   Creates a checkpoint file containing the latest processed file name.\n",
    "\n",
    "   Parameters:\n",
    "       :param latest_file (str): The name of the latest processed file.\n",
    "\n",
    "   Returns:\n",
    "       None\n",
    "   \"\"\"\n",
    "    try:\n",
    "        # Open a file in write mode\n",
    "        with open('checkpoint.txt', 'w') as f:\n",
    "            # Write the latest file to the checkpoint\n",
    "            f.write(latest_file)\n",
    "    except Exception as e:\n",
    "        # Print error message\n",
    "        print(f\"An error occurred while creating checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: load_checkpoint\n",
    "This method is used to load the checkpoint file to get the latest processed image.\n",
    "\n",
    "The method first checks if the checkpoint file exists by verifying the existence of checkpoint.txt. If the checkpoint file exists, it opens the file in read mode, reads the content, and returns it. If the checkpoint file does not exist, it prints \"Checkpoint not found\" and returns None. If any error occurs during this process, the error message is printed with the message \"An error occurred while loading checkpoint: [error message]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to load a checkpoint\n",
    "def load_checkpoint():\n",
    "    \"\"\"\n",
    "    Loads the checkpoint file if it exists.\n",
    "\n",
    "    Returns:\n",
    "        str: The name of the latest processed file, None if checkpoint file not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if checkpoint exists\n",
    "        if os.path.exists('checkpoint.txt'):\n",
    "            # Open the checkpoint in read mode\n",
    "            with open('checkpoint.txt', 'r') as f:\n",
    "                # Return the contents of the checkpoint\n",
    "                return f.read()\n",
    "        else:\n",
    "            # Print message if checkpoint not found\n",
    "            print(\"Checkpoint not found\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        # Print error message\n",
    "        print(f\"An error occurred while loading checkpoint: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method: remove_checkpoint\n",
    "This method is used to remove the checkpoint file.\n",
    "\n",
    "The method first checks if the checkpoint file exists by verifying the existence of checkpoint.txt. If the checkpoint file exists, it removes the file and prints \"Checkpoint removed successfully\". If the checkpoint file does not exist, it prints \"Checkpoint not found\". If any error occurs during this process, the error message is printed with the message \"An error occurred while removing checkpoint: [error message]\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to remove a checkpoint\n",
    "def remove_checkpoint():\n",
    "    \"\"\"\n",
    "    Removes the checkpoint file if it exists.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if checkpoint exists\n",
    "        if os.path.exists('checkpoint.txt'):\n",
    "            # Remove the checkpoint\n",
    "            os.remove('checkpoint.txt')\n",
    "            # Print success message\n",
    "            print(\"Checkpoint removed successfully\")\n",
    "        else:\n",
    "            # Print message if checkpoint not found\n",
    "            print(\"Checkpoint not found\")\n",
    "    except Exception as e:\n",
    "        # Print error message\n",
    "        print(f\"An error occurred while removing checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to set test dataset\n",
    "The set_test_dataset method is used to limit the number of images in a given directory. The method takes two arguments, image_path and amount. The image_path argument specifies the directory containing the images, while the amount argument specifies the number of images that should be kept in the directory. The method works by looping through all the files in the directory and removing any file if the number of files exceeds the value specified by amount. If an error occurs during the execution of the method, an error message will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_test_dataset(image_path, amount=100):\n",
    "    \"\"\"\n",
    "    This function removes all images from the given image_path except the first amount images.\n",
    "\n",
    "    Parameters:\n",
    "    :param image_path (str): The path to the image folder.\n",
    "    :param amount (int, optional): The number of images to keep in the folder. Defaults to 100.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        # loop through the images in the directory using tqdm\n",
    "        for root, dirs, files in os.walk(image_path, topdown=False):\n",
    "            for name in tqdm(files, desc=\"Removing images\"):\n",
    "                # check if the number of images is greater than the amount specified\n",
    "                if len(os.listdir(image_path)) > amount:\n",
    "                    # remove the image if the number of images is greater than the specified amount\n",
    "                    os.remove(os.path.join(root, name))\n",
    "        # print message indicating that images were removed successfully\n",
    "        print(\"All images removed except \" + str(amount) + \" images\")\n",
    "    except Exception as e:\n",
    "        # print error message if there was an error during removal of images\n",
    "        print(f\"An error occurred while setting test dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to process dataset\n",
    "The arrange_dataset method performs several tasks to clean up and organize a directory containing images. The steps are as follows:\n",
    "\n",
    "- Get a list of all the image files in the directory using the get_all_images method.\n",
    "\n",
    "- Load the latest checkpoint using the load_checkpoint method, which is used to keep track of the last image file processed.\n",
    "\n",
    "- For each image file in the list, the method checks if it is the same as the latest checkpoint. If it is, it skips it and sets the checkpoint to None. If the checkpoint is not None, the method skips it. If the checkpoint is None, it moves the file to the image_path directory using the os.rename method. It also creates a checkpoint using the create_checkpoint method to keep track of the last image processed.\n",
    "\n",
    "- After processing all the image files, the method removes the checkpoint using the remove_checkpoint method.\n",
    "\n",
    "- The method then removes all subdirectories in the image_path directory using os.rmdir.\n",
    "\n",
    "- If the is_test argument is set to True, the method calls the set_test_dataset method to remove all images except for a certain number specified by the amount argument (defaults to 100).\n",
    "\n",
    "The method includes a try-except block to catch any errors that may occur while processing the images. If an error occurs, it prints a message indicating that an error occurred while arranging the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_dataset(image_path, is_test=False):\n",
    "    \"\"\"\n",
    "    Arrange the dataset stored in `image_path`.\n",
    "\n",
    "    :param image_path: path to the dataset folder.\n",
    "    :param is_test: If True, the dataset will be set to a test set with only 100 images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get a list of all images in the path\n",
    "        img_files = get_all_images(image_path)\n",
    "        # Load the last checkpoint if it exists\n",
    "        checkpoint = load_checkpoint()\n",
    "        # Iterate over all image files\n",
    "        for file in tqdm(img_files, desc=\"Moving all file to images folder\"):\n",
    "            # Check if the current file matches the checkpoint\n",
    "            if checkpoint == file:\n",
    "                # If it does, reset the checkpoint\n",
    "                checkpoint = None\n",
    "                continue\n",
    "            # If the checkpoint is not None, skip this file\n",
    "            elif checkpoint is not None:\n",
    "                continue\n",
    "            # If neither of the above conditions are met, move the file\n",
    "            else:\n",
    "                os.rename(file, os.path.join(image_path, os.path.basename(file)))\n",
    "                # Create a new checkpoint after moving the file\n",
    "                create_checkpoint(file)\n",
    "        # Print a message indicating that all files have been moved\n",
    "        print(\"All files moved to images folder\")\n",
    "        # Remove the checkpoint since all files have been moved\n",
    "        remove_checkpoint()\n",
    "\n",
    "        # Remove all subfolders in the image path\n",
    "        for root, dirs, files in os.walk(image_path, topdown=False):\n",
    "            for name in dirs:\n",
    "                os.rmdir(os.path.join(root, name))\n",
    "        # Print a message indicating that all subfolders have been removed\n",
    "        print(\"All subfolders removed\")\n",
    "\n",
    "        print(is_test)\n",
    "        # If is_test is True, set the test dataset\n",
    "        if is_test:\n",
    "            print(\"Setting test dataset 13&3\" + image_path)\n",
    "            set_test_dataset(image_path)\n",
    "            print(\"Test dataset set successfully\")\n",
    "    # Catch any exceptions that may occur\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while arranging the dataset: \", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Moving all file to images folder: 100%|██████████| 100/100 [00:00<00:00, 9007.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files moved to images folder\n",
      "Checkpoint removed successfully\n",
      "All subfolders removed\n",
      "False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "arrange_dataset(images_path, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define methods to get metadata\n",
    "The method get_metadata is used to extract metadata information from an image file and return it in a dictionary format. The method takes two parameters: img_file and image_path. img_file is the file name of the image and image_path is the path to the directory where the image is stored.\n",
    "\n",
    "The method uses the Image module from the Python Imaging Library (PIL) to open the image file. Then it gets the EXIF data from the image and stores it in a dictionary along with other metadata information such as the file name, size, height, width, format, and mode of the image.\n",
    "\n",
    "If an error occurs while processing the image file, the method will print an error message and return None. Otherwise, the method returns the metadata information in the form of a dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(img_file):\n",
    "    \"\"\"\n",
    "    This function extracts metadata information from an image file and returns it in a dictionary format.\n",
    "\n",
    "    Parameters:\n",
    "    img_file (str): The file name of the image.\n",
    "    image_path (str): The path to the directory where the image is stored.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the metadata information of the image. If an error occurs, the function returns None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Image.open(img_file) as image:\n",
    "            metadata = image._getexif()\n",
    "\n",
    "        # replace the key with the name of the tag\n",
    "        metadata = {TAGS.get(key, key): value for key, value in metadata.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        # print an error message if an error occurs\n",
    "        print(f\"An error occurred while processing {img_file}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "    # return the metadata information\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Define the method to save the metadata\n",
    "The method save_metadata is used to save the metadata information of an image in either pickle or json format. The method takes in 4 arguments:\n",
    "\n",
    "- metadata (dict): The metadata information of an image.\n",
    "- img_name (str): The file name of the image.\n",
    "- metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "- save_format (str): The format in which the metadata will be saved. The default is 'pickle'.\n",
    "\n",
    "The method first checks if the save_format argument is equal to 'pickle'. If it is, it uses the pickle module to save the metadata information in pickle format. The file name of the saved metadata is created by joining the metadata_path with the base name of the image file (obtained using os.path.basename) and the '.pickle' extension.\n",
    "\n",
    "If the save_format argument is equal to 'json', the method uses the json module to save the metadata information in json format. The file name of the saved metadata is created by joining the metadata_path with the base name of the image file (obtained using os.path.basename) and the '.json' extension.\n",
    "\n",
    "If the save_format argument is neither 'pickle' nor 'json', the method raises a ValueError with the message \"Invalid save format\".\n",
    "\n",
    "The method uses a try-except block to catch any exceptions that may occur during the saving of the metadata. If an error occurs, it prints an error message that includes the image file name and the error message.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_metadata(metadata, img_name, metadata_path, save_format='pickle'):\n",
    "    \"\"\"\n",
    "    This function saves the metadata information of an image in either pickle or json format.\n",
    "    Parameters:\n",
    "    metadata (dict): The metadata information of an image.\n",
    "    img_name (str): The file name of the image.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "    save_format (str): The format in which the metadata will be saved. The default is 'pickle'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if save_format == 'pickle':\n",
    "            # save the metadata in pickle format\n",
    "            with open(os.path.join(metadata_path, os.path.splitext(os.path.basename(img_name))[0] + '.pickle'), 'wb') as f:\n",
    "                pickle.dump(metadata, f)\n",
    "        elif save_format == 'json':\n",
    "            # save the metadata in json format\n",
    "            with open(os.path.join(metadata_path, os.path.splitext(os.path.basename(img_name))[0] + '.json'), 'w') as f:\n",
    "                json.dump(metadata, f)\n",
    "        elif save_format == 'sqlite':\n",
    "            # Get only the file name of the image\n",
    "            img_name = os.path.basename(img_name)\n",
    "            # Open a connection to the database\n",
    "            conn = sqlite3.connect(os.path.join(metadata_path, 'metadata.db'))\n",
    "            # Create a cursor\n",
    "            c = conn.cursor()\n",
    "            # Create a table if it doesn't exist : filename, key, value\n",
    "            c.execute('''CREATE TABLE IF NOT EXISTS metadata (filename text, key text, value text)''')\n",
    "            # Insert the metadata into the table\n",
    "            for key, value in metadata.items():\n",
    "                # Convert key, value to string\n",
    "                key = str(key)\n",
    "                value = str(value)\n",
    "                # Check if the key is already in the table\n",
    "                c.execute(\"SELECT * FROM metadata WHERE filename=? AND key=?\", (img_name, key))\n",
    "                # If the key is already in the table, update the value\n",
    "                if c.fetchone():\n",
    "                    c.execute(\"UPDATE metadata SET value=? WHERE filename=? AND key=?\", (value, img_name, key))\n",
    "                    # Commit the changes\n",
    "                    conn.commit()\n",
    "                # If the key is not in the table, insert the key, value pair\n",
    "                else:\n",
    "                    c.execute(\"INSERT INTO metadata VALUES (?, ?, ?)\", (img_name, key, value))\n",
    "                    # Commit the changes\n",
    "                    conn.commit()\n",
    "            # Close the connection\n",
    "            conn.close()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid save format\")\n",
    "    except Exception as e:\n",
    "        # print an error message if an error occurs\n",
    "        print(f\"An error occurred while saving metadata for {img_name}: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all the metadata\n",
    "The method get_all_metadata is used to extract metadata information from all images in a directory and save the metadata information in either pickle or json format. The method takes in 3 arguments:\n",
    "- image_path (str): The path to the directory where the images are stored.\n",
    "- metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "- save_format (str): The format in which the metadata will be saved. The default is 'pickle'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_metadata(image_path, metadata_path, save_format='pickle'):\n",
    "    \"\"\"\n",
    "    This function extracts metadata from all images in a directory and saves the metadata information in either pickle or json format.\n",
    "    Parameters:\n",
    "    image_path (str): The path to the directory where the images are stored.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Get a list of all images in the directory\n",
    "    img_files = get_all_images(image_path)\n",
    "    # Create a progress bar to track the progress of processing all images\n",
    "    checkpoint = load_checkpoint()\n",
    "    for img in tqdm(img_files, desc=\"Get all metadata of the images and save it\"):\n",
    "        # Check if the metadata extraction process was interrupted previously\n",
    "        if checkpoint == img:\n",
    "            # If the checkpoint is found, set it to None and continue processing the remaining images\n",
    "            checkpoint = None\n",
    "            continue\n",
    "        elif checkpoint is not None:\n",
    "            # If the checkpoint is not None and not equal to the current image, continue to the next image\n",
    "            continue\n",
    "        else:\n",
    "            # Extract the metadata of the current image\n",
    "            metadata = get_metadata(img)\n",
    "            if metadata:\n",
    "                # Save the metadata of the current image\n",
    "                save_metadata(metadata, img, metadata_path, save_format)\n",
    "                # Create a checkpoint to track the progress of the metadata extraction process\n",
    "                create_checkpoint(img)\n",
    "\n",
    "    # Remove the checkpoint file once all metadata have been extracted and saved\n",
    "    remove_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get all metadata of the images and save it:  51%|█████     | 51/100 [00:00<00:00, 87.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing ../output/images/image_99.jpg: 'NoneType' object has no attribute 'items'\n",
      "An error occurred while processing ../output/images/image_57.jpg: 'NoneType' object has no attribute 'items'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get all metadata of the images and save it:  78%|███████▊  | 78/100 [00:00<00:00, 78.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred while processing ../output/images/image_5.jpg: 'NoneType' object has no attribute 'items'\n",
      "An error occurred while processing ../output/images/image_91.jpg: 'NoneType' object has no attribute 'items'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get all metadata of the images and save it: 100%|██████████| 100/100 [00:01<00:00, 75.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint removed successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "get_all_metadata(images_path, metadata_path, save_format=metadata_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to look at the metadata (pickle format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "the STRING opcode argument must be quoted",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mUnpicklingError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[117], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Load the metadata\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(metadata_path, metadata_file), \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m'\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m----> 5\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[43mpickle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Print the metadata\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(metadata)\n",
      "\u001B[0;31mUnpicklingError\u001B[0m: the STRING opcode argument must be quoted"
     ]
    }
   ],
   "source": [
    "# Get the first file of metadata directory\n",
    "metadata_file = os.listdir(metadata_path)[0]\n",
    "# Load the metadata\n",
    "with open(os.path.join(metadata_path, metadata_file), 'rb') as f:\n",
    "    metadata = pickle.load(f)\n",
    "# Print the metadata\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to look at the metadata (json format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first file of metadata directory\n",
    "metadata_file = os.listdir(metadata_path)[0]\n",
    "# Load the metadata\n",
    "with open(os.path.join(metadata_path, metadata_file), 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "# Print the metadata\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# How to look at the metadata (sqlite format)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_15.jpg': [{'ResolutionUnit': '2'}, {'ExifOffset': '148'}, {'Software': 'Aperture 3.6'}, {'Orientation': '1'}, {'DateTime': '2009:10:14 13:03:25'}, {'XResolution': '72.0'}, {'YResolution': '72.0'}, {'ColorSpace': '1'}, {'ExifImageWidth': '3888'}, {'DateTimeOriginal': '2009:10:14 13:03:25'}, {'DateTimeDigitized': '2009:10:14 13:03:25'}, {'ExifImageHeight': '2592'}]}\n"
     ]
    }
   ],
   "source": [
    "# Open a connection to the database\n",
    "conn = sqlite3.connect(os.path.join(metadata_path, 'metadata.db'))\n",
    "# Create a cursor\n",
    "c = conn.cursor()\n",
    "# Get a name of the first file in the images directory\n",
    "metadata_file = os.listdir(images_path)[0]\n",
    "# Get the metadata of the first file\n",
    "c.execute(\"SELECT * FROM metadata WHERE filename=?\", (metadata_file,))\n",
    "# Print the metadata\n",
    "#Convert result to format: filename: [{key: value}...]\n",
    "metadata = c.fetchall()\n",
    "result = {}\n",
    "for row in metadata:\n",
    "    if row[0] not in result:\n",
    "        result[row[0]] = []\n",
    "    result[row[0]].append({row[1]: row[2]})\n",
    "\n",
    "print(result)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
