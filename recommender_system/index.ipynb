{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from mysql.connector import pooling\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import namedtuple\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T20:57:29.674216Z",
     "end_time": "2023-04-07T20:57:31.557965Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ImageProperties = namedtuple('ImageProperties', ['name', 'hex_color', 'tags', 'make', 'orientation', 'width', 'height'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T16:14:10.853332Z",
     "end_time": "2023-04-03T16:14:10.861384Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "preferences = {\n",
    "    'Make': '',\n",
    "    'ImageWidth': '',\n",
    "    'ImageHeight': '',\n",
    "    'Orientation': 1,\n",
    "    'dominant_color': '#73AD3D',\n",
    "    'tags': ['vase', 'toilet']\n",
    "}\n",
    "\n",
    "# Set SQL variables\n",
    "sql_host = os.getenv(\"SQL_HOST\")\n",
    "sql_user = os.getenv(\"SQL_USER\")\n",
    "sql_password = os.getenv(\"SQL_PASSWORD\")\n",
    "sql_database = os.getenv(\"SQL_DATABASE\")\n",
    "\n",
    "# set the database config\n",
    "config = {\n",
    "    'user': sql_user,\n",
    "    'password': sql_password,\n",
    "    'host': sql_host,\n",
    "    'port': '3306',\n",
    "    'database': sql_database,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T16:14:13.423959Z",
     "end_time": "2023-04-03T16:14:13.435545Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a connection pool\n",
    "connection_pool = pooling.MySQLConnectionPool(pool_name=\"mypool\",\n",
    "                                              pool_size=2,\n",
    "                                              **config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T16:13:37.085936Z",
     "end_time": "2023-04-03T16:13:37.729551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_metadata_from_mariadb_db():\n",
    "    \"\"\"\n",
    "    Get the metadata from the MariaDB database\n",
    "\n",
    "    :return: A pandas DataFrame with the metadata\n",
    "    \"\"\"\n",
    "    # Open a connection to the database\n",
    "    conn = connection_pool.get_connection()\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Retrieve the metadata\n",
    "    c.execute(\"\"\"\n",
    "        SELECT filename, GROUP_CONCAT(CONCAT(mkey, '\\t', mvalue) SEPARATOR '\\n') AS metadata\n",
    "        FROM metadata\n",
    "        GROUP BY filename;\n",
    "    \"\"\")\n",
    "    metadata = c.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # Create an empty DataFrame with the desired columns\n",
    "    columns = ['filename', 'Make', 'Software', 'ImageWidth', 'ImageHeight', 'Orientation', 'DateTimeOriginal',\n",
    "               'dominant_color', 'tags']\n",
    "    df = pd.DataFrame(columns=columns)\n",
    "\n",
    "    # Fill the DataFrame with the metadata\n",
    "    for image in tqdm(metadata, desc=\"Get metadata from database\"):\n",
    "        try:\n",
    "            props = {'filename': image[0]}\n",
    "            metadata_str = image[1].split('\\n')\n",
    "            for prop in metadata_str:\n",
    "                if prop:\n",
    "                    k, value = prop.split('\\t')\n",
    "                    if k in columns[1:]:\n",
    "                        if k == 'dominant_color':\n",
    "                            color_list = eval(value)\n",
    "                            color_list = [c[0] for c in color_list]\n",
    "                            props[k] = color_list\n",
    "                        elif k == 'tags':\n",
    "                            props[k] = eval(value)\n",
    "                        else:\n",
    "                            props[k] = value\n",
    "            df = df.append(props, ignore_index=True)\n",
    "        except Exception as e:\n",
    "            print(e, image)\n",
    "\n",
    "    return df\n",
    "\n",
    "import ast\n",
    "\n",
    "ImageProperties = namedtuple('ImageProperties', ['name', 'hex_color', 'tags', 'make', 'orientation', 'width', 'height'])\n",
    "\n",
    "def get_metadata_from_mariadb_as_imageproperties():\n",
    "    # Open a connection to the database\n",
    "    conn = connection_pool.get_connection()\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # Retrieve the metadata\n",
    "    c.execute(\"\"\"\n",
    "        SELECT DISTINCT filename, GROUP_CONCAT(CONCAT(mkey, '\\t', mvalue) SEPARATOR '\\n') AS metadata\n",
    "        FROM metadata\n",
    "        WHERE mkey IN ('Make', 'Orientation', 'ImageWidth', 'ImageHeight', 'tags', 'dominant_color')\n",
    "        GROUP BY filename;\n",
    "    \"\"\")\n",
    "    metadata = c.fetchall()\n",
    "\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "\n",
    "    # use the namedtuple ImageProperties to store the metadata\n",
    "    images = []\n",
    "\n",
    "    # Loop through the rows of metadata\n",
    "    for row in metadata:\n",
    "        filename, metadata_str = row\n",
    "        metadata_items = metadata_str.split('\\n')\n",
    "        metadata_dict = {key: value for key, value in (item.split('\\t') for item in metadata_items)}\n",
    "\n",
    "        # Clean dominant colors: convert the string to a list of tuples and extract only the color hex codes\n",
    "        dominant_colors = ast.literal_eval(metadata_dict.get('dominant_color', '[]'))\n",
    "        hex_colors = [color[0] for color in dominant_colors]\n",
    "\n",
    "        # Clean tags: convert the string to a list of strings\n",
    "        tags = ast.literal_eval(metadata_dict.get('tags', '[]'))\n",
    "\n",
    "        # Create an ImageProperties object for each row\n",
    "        image = ImageProperties(\n",
    "            name=filename,\n",
    "            hex_color=hex_colors,\n",
    "            tags=tags,\n",
    "            make=metadata_dict.get('Make', None),\n",
    "            orientation=metadata_dict.get('Orientation', None),\n",
    "            width=metadata_dict.get('ImageWidth', None),\n",
    "            height=metadata_dict.get('ImageHeight', None)\n",
    "        )\n",
    "\n",
    "        # Add the ImageProperties object to the list\n",
    "        images.append(image)\n",
    "\n",
    "    return images"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T18:53:14.888090Z",
     "end_time": "2023-04-03T18:53:15.687142Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hex_to_rgb(color):\n",
    "    try:\n",
    "        # remove the # from the color\n",
    "        color = color[1:]\n",
    "        # convert the color to rgb values\n",
    "        rgb = tuple(int(color[i:i + 2], 16) for i in (0, 2, 4))\n",
    "        return rgb\n",
    "    except:\n",
    "        return 0, 0, 0\n",
    "\n",
    "\n",
    "def get_clean_preferences(df_preferences):\n",
    "    # remove the rows with nan in dominant_color\n",
    "    df_preferences = df_preferences.dropna(subset=['dominant_color'])\n",
    "    # split dominant color into 4 columns and remove the dominant_color column\n",
    "    # convert the tags column to a list of strings\n",
    "    # Replace all NaN values with empty strings with the fillna() method\n",
    "    df_preferences = df_preferences.fillna(0)\n",
    "    # convert colors to rgb values\n",
    "    df_preferences['dominant_color'] = df_preferences['dominant_color'].apply(lambda x: hex_to_rgb(x))\n",
    "    # replace all 0 values with empty strings\n",
    "    df_preferences['dominant_color'] = df_preferences['dominant_color'].replace(0, '')\n",
    "\n",
    "    return df_preferences\n",
    "\n",
    "\n",
    "def get_clean_dataset():\n",
    "    metadata = get_metadata_from_mariadb_db()\n",
    "    df_metadata = pd.DataFrame(metadata)\n",
    "    # remove the rows with nan in dominant_color\n",
    "    df_metadata = df_metadata.dropna(subset=['dominant_color'])\n",
    "    # split dominant color into 4 columns and remove the dominant_color column\n",
    "    if 'dominant_color' in df_metadata.columns:\n",
    "        df_metadata['color1'] = df_metadata['dominant_color'].apply(lambda x: x[0] if len(x) >= 1 else 0)\n",
    "        df_metadata['color2'] = df_metadata['dominant_color'].apply(lambda x: x[1] if len(x) >= 2 else 0)\n",
    "        df_metadata['color3'] = df_metadata['dominant_color'].apply(lambda x: x[2] if len(x) == 3 else 0)\n",
    "        df_metadata['color4'] = df_metadata['dominant_color'].apply(lambda x: x[3] if len(x) == 4 else 0)\n",
    "        # convert colors to rgb values\n",
    "        df_metadata['color1'] = df_metadata['color1'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata['color2'] = df_metadata['color2'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata['color3'] = df_metadata['color3'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata['color4'] = df_metadata['color4'].apply(lambda x: hex_to_rgb(x) if x else (0, 0, 0))\n",
    "        df_metadata = df_metadata.drop('dominant_color', axis=1)\n",
    "    else:\n",
    "        df_metadata['color1'] = 0\n",
    "        df_metadata['color2'] = 0\n",
    "        df_metadata['color3'] = 0\n",
    "        df_metadata['color4'] = 0\n",
    "\n",
    "    # convert the tags column to a list of strings\n",
    "    df_metadata = df_metadata.fillna(0)\n",
    "    # remove all columns except filename, tags, color1, color2, color3, color4, Make, Width, Height\n",
    "    df_metadata = df_metadata[\n",
    "        ['filename', 'Make', 'ImageWidth', 'ImageHeight', 'Orientation', 'DateTimeOriginal', 'tags', 'color1', 'color2',\n",
    "         'color3', 'color4']]\n",
    "    # replace all 0 values with empty strings\n",
    "    df_metadata['Make'] = df_metadata['Make'].replace(0, '')\n",
    "\n",
    "    return df_metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_pref = pd.DataFrame([preferences])\n",
    "df_preferences = get_clean_preferences(df_pref)\n",
    "df_preferences.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_metadata = get_clean_dataset()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_metadata.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Color Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_colors(df_metadata, df_preferences, n=0):\n",
    "    # Load the dataset into a Pandas DataFrame\n",
    "    data = df_metadata.copy()\n",
    "\n",
    "    # Extract the individual r, g, and b values from tupbles in the color columns\n",
    "    data[['r1', 'g1', 'b1']] = pd.DataFrame(data['color1'].tolist(), index=data.index)\n",
    "    data[['r2', 'g2', 'b2']] = pd.DataFrame(data['color2'].tolist(), index=data.index)\n",
    "    data[['r3', 'g3', 'b3']] = pd.DataFrame(data['color3'].tolist(), index=data.index)\n",
    "    data[['r4', 'g4', 'b4']] = pd.DataFrame(data['color4'].tolist(), index=data.index)\n",
    "\n",
    "    # Normalize the r, g, and b columns to be between 0 and 1\n",
    "    data[['r1', 'g1', 'b1', 'r2', 'g2', 'b2', 'r3', 'g3', 'b3', 'r4', 'g4', 'b4']] = data[['r1', 'g1', 'b1', 'r2', 'g2',\n",
    "                                                                                           'b2', 'r3', 'g3', 'b3', 'r4',\n",
    "                                                                                           'g4', 'b4']] / 255\n",
    "\n",
    "    # Normalize the input RGB color to be between 0 and 1\n",
    "    r, g, b = df_preferences['dominant_color'][0]\n",
    "    r_norm, g_norm, b_norm = r / 255, g / 255, b / 255\n",
    "\n",
    "    # Compute the Euclidean distance between the input color and all the colors in the dataset\n",
    "    data['similarity_dominant_color'] = euclidean_distances(\n",
    "        [[r_norm, g_norm, b_norm, r_norm, g_norm, b_norm, r_norm, g_norm, b_norm, r_norm, g_norm, b_norm]],\n",
    "        data[['r1', 'g1', 'b1', 'r2', 'g2', 'b2', 'r3', 'g3', 'b3', 'r4', 'g4', 'b4']])[0]\n",
    "\n",
    "    # Sort the dataset by Euclidean distance in ascending order and return the top 10 closest matches\n",
    "    if n == 0:\n",
    "        closest_matches = data.sort_values('similarity_dominant_color', ascending=True)[\n",
    "            ['filename', 'color1', 'color2', 'color3', 'color4', 'similarity_dominant_color']]\n",
    "    else:\n",
    "        closest_matches = data.sort_values('similarity_dominant_color', ascending=True).head(n)[\n",
    "            ['filename', 'color1', 'color2', 'color3', 'color4', 'similarity_dominant_color']]\n",
    "\n",
    "    return closest_matches"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_colors(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Tag Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_tags(df_metadata, df_preferences, n=0, nlp=None):\n",
    "    # Load the spaCy model if it hasn't been loaded\n",
    "    if not nlp:\n",
    "        nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Define the preferences list and the dataframe\n",
    "    preferences = df_preferences['tags'][0]\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.copy()\n",
    "    df = df.dropna(subset=[\"tags\"]).reset_index(drop=True)\n",
    "    # replace int with empty list\n",
    "    df['tags'] = df['tags'].apply(lambda x: x if x else [])\n",
    "\n",
    "    # Precompute the similarity between each tag word and each preference word\n",
    "    similarity_dict = {}\n",
    "    for tag_word in set([word for tags in df['tags'] for word in tags]):\n",
    "        for pref_word in set(preferences):\n",
    "            similarity_dict[(tag_word, pref_word)] = nlp(tag_word).similarity(nlp(pref_word))\n",
    "\n",
    "    # Compute the average similarity for each row in the dataframe\n",
    "    similarities = []\n",
    "    for tags in df['tags']:\n",
    "        sum_similarity = 0\n",
    "        for tag_word in tags:\n",
    "            for pref_word in preferences:\n",
    "                sum_similarity += similarity_dict[(tag_word, pref_word)]\n",
    "        avg_similarity = sum_similarity / (len(tags) * len(preferences)) if len(tags) > 0 else 0\n",
    "        similarities.append(avg_similarity)\n",
    "\n",
    "    # Add the similarity scores to a new column in the dataframe\n",
    "    df['similarity_tags'] = similarities\n",
    "    if n == 0:\n",
    "        closest_matches = df.sort_values('similarity_tags', ascending=False)[\n",
    "            ['filename', 'similarity_tags']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_tags', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_tags']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_tags(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_make(df_metadata, df_preferences, n=0):\n",
    "    # Load the spaCy model\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "    # Define the preferences list and the dataframe\n",
    "    make = df_preferences['Make'][0]\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.copy()\n",
    "    df = df.dropna(subset=[\"Make\"]).reset_index(drop=True)\n",
    "\n",
    "    # Convert make and Make to document objects\n",
    "    make_doc = nlp(make)\n",
    "    df['Make'] = df['Make'].apply(nlp)\n",
    "\n",
    "    # Compute the cosine similarity between the make preferences and all the makes in the dataset\n",
    "    similarities = [make_doc.similarity(doc) for doc in df['Make']]\n",
    "\n",
    "    # Add the similarity scores to a new column in the dataframe\n",
    "    df['similarity_make'] = similarities\n",
    "    if n == 0:\n",
    "        closest_matches = df.sort_values('similarity_make', ascending=False)[\n",
    "            ['filename', 'similarity_make']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_make', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_make']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_make(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Orientation Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_orientation(df_metadata, df_preferences, n=0):\n",
    "    # Define the preferences list and the dataframe\n",
    "    orientation = df_preferences['Orientation'][0]\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.dropna(subset=[\"Orientation\"]).reset_index(drop=True)\n",
    "    # if Orientation contain '' or '0' or '1' then replace with 0 or 1\n",
    "    df['Orientation'] = df['Orientation'].apply(lambda x: 0 if x == '' or x == '0' else 1)\n",
    "\n",
    "    # Convert the Orientation column to integer type\n",
    "    df['Orientation'] = df['Orientation'].astype(int)\n",
    "\n",
    "    # Orientation is 0 or 1, so we can just subtract the preference from the orientation\n",
    "    df['similarity_orientation'] = df['Orientation'].apply(lambda x: abs(x - orientation))\n",
    "\n",
    "    # sort by similarity\n",
    "    if n > 0:\n",
    "        closest_matches = df.sort_values('similarity_orientation', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_orientation']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_orientation', ascending=False)[\n",
    "            ['filename', 'similarity_orientation']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_orientation(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Size Similarity"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend_size(df_metadata, df_preferences, n=0):\n",
    "    # Define the preferences list and the dataframe\n",
    "    width = int(df_preferences['ImageWidth'][0])\n",
    "    height = int(df_preferences['ImageHeight'][0])\n",
    "    # Load dataset with words and drop duplicate rows\n",
    "    df = df_metadata.dropna(subset=[\"ImageWidth\", \"ImageHeight\"]).reset_index(drop=True)\n",
    "\n",
    "    # Convert the ImageWidth and ImageHeight column to integer type\n",
    "    df[['ImageWidth', 'ImageHeight']] = df[['ImageWidth', 'ImageHeight']].astype(int)\n",
    "\n",
    "    # Compute the product of width and height outside the loop\n",
    "    product = width * height\n",
    "\n",
    "    # Use apply method to compute similarity score for each row\n",
    "    df['similarity_size'] = df.apply(lambda x: 1 - abs(product - (x['ImageWidth'] * x['ImageHeight'])) / product, axis=1)\n",
    "\n",
    "    if n == 0:\n",
    "        closest_matches = df.sort_values('similarity_size', ascending=False)[\n",
    "            ['filename', 'similarity_size']]\n",
    "    else:\n",
    "        closest_matches = df.sort_values('similarity_size', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_size']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend_size(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def recommend(df_metadata, df_preferences, n=0):\n",
    "    # Assign weights to properties based on user preferences\n",
    "    weights = {\n",
    "        'Make': float(5.0),\n",
    "        'ImageWidth': float(1.0),\n",
    "        'ImageHeight': float(1.0),\n",
    "        'Orientation': float(2.0),\n",
    "        'dominant_color': float(3.0),\n",
    "        'tags': float(5.0)\n",
    "    }\n",
    "\n",
    "    # Create a dictionary with the preferences and the corresponding recommendation methods\n",
    "    preference_methods = {\n",
    "        'Make': recommend_make,\n",
    "        'ImageWidth': recommend_size,\n",
    "        'ImageHeight': recommend_size,\n",
    "        'Orientation': recommend_orientation,\n",
    "        'dominant_color': recommend_colors,\n",
    "        'tags': recommend_tags\n",
    "    }\n",
    "\n",
    "    # Remove preferences with no values\n",
    "    preferences = {k: v for k, v in df_preferences.squeeze().to_dict().items() if v != ''}\n",
    "\n",
    "    # Calculate the sum of the weights\n",
    "    weights_sum = 0\n",
    "    for key in weights:\n",
    "        weights_sum += weights[key]\n",
    "    for key in weights:\n",
    "        weights[key] = weights[key] / weights_sum\n",
    "\n",
    "    # Calculate similarity score for each property\n",
    "    df_metadata['similarity_score'] = 0.0\n",
    "    for preference, value in preferences.items():\n",
    "        method = preference_methods[preference]\n",
    "        if preference == 'ImageWidth' or preference == 'ImageHeight':\n",
    "            similarity = method(df_metadata, df_preferences, n)['similarity_size'].astype(float)\n",
    "        else:\n",
    "            similarity = method(df_metadata, df_preferences, n)[f'similarity_{preference.lower()}'].astype(float)\n",
    "        df_metadata['similarity_score'] += similarity * (weights[preference] / weights_sum)\n",
    "\n",
    "    # Replace NaN values in the 'similarity_score' column with 0\n",
    "    df_metadata['similarity_score'].fillna(0, inplace=True)\n",
    "\n",
    "    # Sort by similarity score\n",
    "    if n == 0:\n",
    "        closest_matches = df_metadata.sort_values('similarity_score', ascending=False)[\n",
    "            ['filename', 'similarity_score']]\n",
    "    else:\n",
    "        closest_matches = df_metadata.sort_values('similarity_score', ascending=False).head(n)[\n",
    "            ['filename', 'similarity_score']]\n",
    "\n",
    "    return closest_matches\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend(df_metadata, df_preferences)  # OK"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Test\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = get_metadata_from_mariadb_as_imageproperties()\n",
    "\n",
    "preferences = {\n",
    "    'Make': 'Canon',\n",
    "    'ImageWidth': '',\n",
    "    'ImageHeight': '',\n",
    "    'Orientation': 1,\n",
    "    'dominant_color': '#00000',\n",
    "    'tags': ['cat']\n",
    "}\n",
    "\n",
    "\n",
    "def hex_to_rgb(hex_color):\n",
    "    return tuple(int(hex_color[i:i+2], 16) for i in (1, 3, 5))\n",
    "\n",
    "\n",
    "def preprocess_data(data):\n",
    "    for image in data:\n",
    "        image_tags = \" \".join(image.tags)\n",
    "        avg_rgb_color = np.mean([hex_to_rgb(color) for color in image.hex_color], axis=0)\n",
    "        try:\n",
    "            make_len = len(image.make)\n",
    "        except TypeError:\n",
    "            make_len = 0\n",
    "\n",
    "        try:\n",
    "            width = int(image.width)\n",
    "        except TypeError:\n",
    "            width = 0\n",
    "\n",
    "        try:\n",
    "            height = int(image.height)\n",
    "        except TypeError:\n",
    "            height = 0\n",
    "\n",
    "        try:\n",
    "            orientation = int(image.orientation)\n",
    "        except:\n",
    "            orientation = 0\n",
    "\n",
    "        avg_rgb_color_list = avg_rgb_color.tolist() if hasattr(avg_rgb_color, 'tolist') else [avg_rgb_color]\n",
    "\n",
    "        try:\n",
    "            yield np.array([\n",
    "                *avg_rgb_color_list,  # Ensure avg_rgb_color is an iterable before unpacking\n",
    "                len(image_tags),\n",
    "                make_len,\n",
    "                orientation,\n",
    "                width,\n",
    "                height\n",
    "            ])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "def user_preferences_vector(preferences):\n",
    "    dominant_color = hex_to_rgb(preferences['dominant_color'])\n",
    "    tags = \" \".join(preferences['tags'])\n",
    "    make = preferences['Make']\n",
    "\n",
    "    try:\n",
    "        make_len = len(make)\n",
    "    except TypeError:\n",
    "        make_len = 0\n",
    "\n",
    "    orientation = preferences['Orientation']\n",
    "\n",
    "    try:\n",
    "        width = int(preferences['ImageWidth'])\n",
    "    except (TypeError, ValueError):\n",
    "        width = 0\n",
    "\n",
    "    try:\n",
    "        height = int(preferences['ImageHeight'])\n",
    "    except (TypeError, ValueError):\n",
    "        height = 0\n",
    "\n",
    "    return np.array([\n",
    "        *dominant_color,\n",
    "        len(tags),\n",
    "        make_len,\n",
    "        orientation,\n",
    "        width,\n",
    "        height\n",
    "    ])\n",
    "\n",
    "\n",
    "def recommend_images(preferences, data, top_n=10):\n",
    "    preprocessed_data = np.array(list(preprocess_data(data)))\n",
    "    user_vector = user_preferences_vector(preferences)\n",
    "    similarity_matrix = cosine_similarity(np.vstack([user_vector, preprocessed_data]))\n",
    "    most_similar_indices = np.argsort(-similarity_matrix[0])[1:top_n+1]\n",
    "    return [data[i] for i in most_similar_indices]\n",
    "\n",
    "# Test the recommender system\n",
    "recommended_images = recommend_images(preferences, data)\n",
    "for image in recommended_images:\n",
    "    print(image.name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-03T19:16:06.032561Z",
     "end_time": "2023-04-03T19:16:06.634478Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-07T15:58:33.738710Z",
     "end_time": "2023-04-07T15:58:43.140031Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import torch.nn as nn\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean(data):\n",
    "    \"\"\"\n",
    "    :param data: DataFrame\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 1. first, get only the first hex_color\n",
    "    # 2. Convert each column to the right type of data\n",
    "    # 2. Remove rows with missing values\n",
    "\n",
    "    data['hex_color'] = data['hex_color'].apply(lambda x: eval(x)[0] if len(x) > 0 else None)\n",
    "    data['width'] = pd.to_numeric(data['width'], errors='coerce')\n",
    "    data['height'] = pd.to_numeric(data['height'], errors='coerce')\n",
    "    data['orientation'] = pd.to_numeric(data['orientation'], errors='coerce')\n",
    "    data['width'].fillna(0, inplace=True)\n",
    "    data['height'].fillna(0, inplace=True)\n",
    "    data['orientation'].fillna(0, inplace=True)\n",
    "\n",
    "    data = data.dropna()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    # 1. Normalize and scale numerical properties\n",
    "    # Width and Height: scale to [0, 1] by dividing each value by the maximum value\n",
    "    # Hex colors: Convert hex colors to RGB values in the range 0-255 and normalize it by dividing by 255 (range [0, 1])\n",
    "\n",
    "    data['width'] = data['width'] / data['width'].max()\n",
    "    data['height'] = data['height'] / data['height'].max()\n",
    "\n",
    "    # Convert hex color to RGB and normalize\n",
    "    # Remove # from hex color\n",
    "    data[\"hex_color\"] = data[\"hex_color\"].apply(lambda x: x[1:])\n",
    "    data[\"rgb_color\"] = data[\"hex_color\"].apply(lambda x: tuple(int(x[i:i + 2], 16) for i in (0, 2, 4)))\n",
    "    data[[\"r\", \"g\", \"b\"]] = pd.DataFrame(data[\"rgb_color\"].tolist(), index=data.index) / 255\n",
    "    data.drop([\"hex_color\", \"rgb_color\"], axis=1, inplace=True)\n",
    "\n",
    "    # One-hot encode categorical properties\n",
    "    # 1. tags, make, orientation\n",
    "    # eval tags to have an array\n",
    "    convert_to_array = lambda tag_string: ast.literal_eval(tag_string)\n",
    "\n",
    "    data[\"tags\"] = data[\"tags\"].apply(convert_to_array)\n",
    "    data = pd.concat([data, pd.get_dummies(data[\"tags\"].apply(pd.Series).stack()).sum(level=0)], axis=1)\n",
    "    data = pd.concat([data, pd.get_dummies(data[\"make\"], prefix=\"make\")], axis=1)\n",
    "    data[\"landscape\"] = data[\"orientation\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "    data[\"portrait\"] = data[\"orientation\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "    data.drop([\"tags\", \"make\", \"orientation\"], axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def train_and_test():\n",
    "    df = pd.read_csv('./raw_image_properties.csv', sep=\"|\")\n",
    "\n",
    "    # Convert the list of ImageProperties objects to a pandas DataFrame\n",
    "    #df = pd.DataFrame(raw_image_properties)\n",
    "\n",
    "    clean_image_properties = clean(df)\n",
    "    processed_image_properties = preprocess(clean_image_properties)\n",
    "\n",
    "    # Check for NaN values in the DataFrame\n",
    "    if processed_image_properties.isnull().values.any():\n",
    "        processed_image_properties.fillna(0, inplace=True)\n",
    "\n",
    "    # Create feature vectors (numpy array)\n",
    "    feature_vectors = processed_image_properties.drop([\"name\"], axis=1).values\n",
    "\n",
    "    # Combine the feature vectors with the image names in a new DataFrame\n",
    "    data_with_features = processed_image_properties[['name']].copy()\n",
    "    data_with_features['features'] = list(feature_vectors)\n",
    "\n",
    "    # Split the dataset into training (70%) and the remaining 30%\n",
    "    train_data, temp_data = train_test_split(data_with_features, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Split the remaining data into validation (15%) and test (15%) sets\n",
    "    validation_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "    env = DummyVecEnv([lambda: ImageRecommenderEnvironment(data_with_features)])\n",
    "\n",
    "    input_dim = env.observation_space.shape[0]\n",
    "    output_dim = env.action_space.n\n",
    "\n",
    "    policy_kwargs = dict(\n",
    "        features_extractor_class=CustomDQNNetwork,\n",
    "        features_extractor_kwargs={'input_dim': input_dim, 'output_dim': output_dim}\n",
    "    )\n",
    "\n",
    "    agent = DQN(MlpPolicy, env, policy_kwargs=policy_kwargs, verbose=0)\n",
    "\n",
    "    total_timesteps = 500000  # Adjust this value based on the problem and computational resources\n",
    "    print(\"Starting training at time\", time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "    start_time = time.time()\n",
    "    agent.learn(total_timesteps=total_timesteps)\n",
    "    end_time = time.time()\n",
    "    print(f\"Training completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    # Save the trained agent\n",
    "    agent.save(\"dqn_agent.pkl\")\n",
    "\n",
    "    # Load the trained agent\n",
    "    #agent = DQN.load(\"dqn_agent.pkl\")\n",
    "\n",
    "    # Create validation and test environments\n",
    "    validation_env = DummyVecEnv([lambda: ImageRecommenderEnvironment(validation_data)])\n",
    "    test_env = DummyVecEnv([lambda: ImageRecommenderEnvironment(test_data)])\n",
    "\n",
    "    # Evaluate the agent on the validation and test sets\n",
    "    num_validation_episodes = 100000\n",
    "    num_test_episodes = 100000\n",
    "\n",
    "    print(\"Starting evaluation at time\", time.strftime(\"%H:%M:%S\", time.localtime()))\n",
    "    start_time = time.time()\n",
    "    validation_reward, validation_duration, validation_steps = evaluate_agent(agent, validation_env, num_validation_episodes)\n",
    "    test_reward, test_duration, test_steps = evaluate_agent(agent, test_env, num_test_episodes)\n",
    "    end_time = time.time()\n",
    "    print(f\"Evaluation completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "    print(f\"Validation reward: {validation_reward}, duration: {validation_duration:.2f}s, steps: {validation_steps}\")\n",
    "    print(f\"Test reward: {test_reward}, duration: {test_duration:.2f}s, steps: {test_steps}\")\n",
    "\n",
    "    # Precision = TP / (TP + FP)\n",
    "    # Recall = TP / (TP + FN)\n",
    "    # F1-score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "\n",
    "def preprocess_preferences(preferences, max_width, max_height, all_indexes):\n",
    "        # 1. Normalize and scale numerical properties\n",
    "        # Width and Height: scale to [0, 1] by dividing each value by the maximum value\n",
    "        # Hex colors: Convert hex colors to RGB values in the range 0-255 and normalize it by dividing by 255 (range [0, 1])\n",
    "\n",
    "        preferences['width'] = preferences['width'] / max_width\n",
    "        preferences['height'] = preferences['height'] / max_height\n",
    "\n",
    "        # Convert hex color to RGB and normalize\n",
    "        # Remove # from hex color\n",
    "        try:\n",
    "            preferences[\"hex_color\"] = preferences[\"hex_color\"].apply(lambda x: x[1:])\n",
    "            preferences[\"rgb_color\"] = preferences[\"hex_color\"].apply(lambda x: tuple(int(x[i:i + 2], 16) for i in (0, 2, 4)))\n",
    "            preferences[[\"r\", \"g\", \"b\"]] = pd.DataFrame(preferences[\"rgb_color\"].tolist(), index=preferences.index) / 255\n",
    "            preferences.drop([\"hex_color\", \"rgb_color\"], axis=1, inplace=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # One-hot encode categorical properties\n",
    "        # 1. tags, make, orientation\n",
    "        # eval tags to have an array\n",
    "        convert_to_array = lambda tag_string: ast.literal_eval(tag_string)\n",
    "\n",
    "        preferences[\"tags\"] = preferences[\"tags\"].apply(convert_to_array)\n",
    "        data = pd.concat([preferences, pd.get_dummies(preferences[\"tags\"].apply(pd.Series).stack()).sum(level=0)], axis=1)\n",
    "        data = pd.concat([data, pd.get_dummies(data[\"make\"], prefix=\"make\")], axis=1)\n",
    "        data[\"landscape\"] = data[\"orientation\"].apply(lambda x: 1 if x == 0 else 0)\n",
    "        data[\"portrait\"] = data[\"orientation\"].apply(lambda x: 1 if x == 1 else 0)\n",
    "        data.drop([\"tags\", \"make\", \"orientation\"], axis=1, inplace=True)\n",
    "\n",
    "        present_indexes = data.columns.values.tolist()\n",
    "\n",
    "        # Add all indexes to the data as columns and set it to 0 if it is not present\n",
    "        for index in all_indexes:\n",
    "            if index not in present_indexes:\n",
    "                data[index] = 0\n",
    "\n",
    "        try:\n",
    "            data.drop(\"hex_color\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # if prepo_preferences do not have the same lenght as processed_image_properties remove columns that is not in processed_image_properties and add the one that are not in  prepro_preferences bt processed_image_properties\n",
    "\n",
    "        if len(data.columns) != len(all_indexes):\n",
    "            for col in data.columns:\n",
    "                if col not in all_indexes:\n",
    "                    data.drop(col, axis=1, inplace=True)\n",
    "            for col in data.columns:\n",
    "                if col not in data.columns:\n",
    "                    data[col] = 0\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "def clean_prefs(preferences):\n",
    "\n",
    "    try:\n",
    "        preferences['hex_color'] = preferences['hex_color'].apply(lambda x: eval(x)[0] if len(x) > 0 else None)\n",
    "    except:\n",
    "        preferences['hex_color'] = 0\n",
    "\n",
    "    try:\n",
    "        preferences['width'] = pd.to_numeric(preferences['width'], errors='coerce')\n",
    "    except:\n",
    "        preferences['width'] = None\n",
    "\n",
    "    try:\n",
    "        preferences['height'] = pd.to_numeric(preferences['height'], errors='coerce')\n",
    "    except:\n",
    "        preferences['height'] = None\n",
    "\n",
    "    try:\n",
    "        preferences['orientation'] = pd.to_numeric(preferences['orientation'], errors='coerce')\n",
    "    except:\n",
    "        preferences['orientation'] = None\n",
    "\n",
    "    preferences['width'].fillna(0, inplace=True)\n",
    "    preferences['height'].fillna(0, inplace=True)\n",
    "    preferences['orientation'].fillna(0, inplace=True)\n",
    "\n",
    "    # check if there is make or others to set it to None\n",
    "    return preferences\n",
    "\n",
    "\n",
    "def get_image_name(image_filenames, index):\n",
    "    return image_filenames[index]\n",
    "\n",
    "def recommend():\n",
    "    # TODO: convert the user preferences to clean and processed data\n",
    "    # and : action, _ = agent.predict(user_preferences, deterministic=True)\n",
    "    # return int(action)\n",
    "\n",
    "    df = pd.read_csv('./raw_image_properties.csv', sep=\"|\")\n",
    "\n",
    "    # Convert the list of ImageProperties objects to a pandas DataFrame\n",
    "    #df = pd.DataFrame(raw_image_properties)\n",
    "\n",
    "    clean_image_properties = clean(df)\n",
    "    max_width = clean_image_properties[\"width\"].max()\n",
    "    max_height = clean_image_properties[\"height\"].max()\n",
    "    processed_image_properties = preprocess(clean_image_properties)\n",
    "\n",
    "    images_filenames = processed_image_properties[\"name\"].values.tolist()\n",
    "\n",
    "    tags = \"['cat']\"\n",
    "\n",
    "    preferences = {\n",
    "        'make': 'Apple',\n",
    "        'tags': tags\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([preferences])\n",
    "\n",
    "    clean_preferences = clean_prefs(df)\n",
    "\n",
    "    prepro_preferences = preprocess_preferences(clean_preferences, max_width, max_height, processed_image_properties.columns)\n",
    "\n",
    "    # do the prediction by loading the model\n",
    "    model = DQN.load(\"dqn_agent.pkl\")\n",
    "    # drop the first column\n",
    "    prepro_preferences.drop(prepro_preferences.columns[0], axis=1, inplace=True)\n",
    "    action, _ = model.predict(prepro_preferences, deterministic=True)\n",
    "\n",
    "    return get_image_name(images_filenames, int(action))\n",
    "\n",
    "\n",
    "\n",
    "class ImageRecommenderEnvironment(gym.Env):\n",
    "    def __init__(self, data_with_features, max_steps=100):\n",
    "        super(ImageRecommenderEnvironment, self).__init__()\n",
    "\n",
    "        self.data = data_with_features\n",
    "        self.current_state = None\n",
    "        self.max_steps = max_steps\n",
    "        self.step_count = 0\n",
    "\n",
    "        # Define action space as the indices of the images\n",
    "        self.action_space = spaces.Discrete(len(self.data))\n",
    "\n",
    "        # Define state space as the feature vectors\n",
    "        feature_vector_length = len(self.data['features'].iloc[0])\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(feature_vector_length,), dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset the environment to an initial state\n",
    "        self.current_state = self._get_initial_state()\n",
    "        return self.current_state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute the given action and observe the next state and reward\n",
    "        next_state, reward = self._get_next_state_and_reward(action)\n",
    "\n",
    "        # Check if the episode is done\n",
    "        self.step_count += 1\n",
    "        done = self._is_done(next_state)\n",
    "\n",
    "        # Update the current state\n",
    "        self.current_state = next_state\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # Render the current state of the environment (optional)\n",
    "        pass\n",
    "\n",
    "    def _get_initial_state(self):\n",
    "        # Randomly select an initial state from the data\n",
    "        initial_state = self.data.sample().iloc[0]['features']\n",
    "        return initial_state\n",
    "\n",
    "    def _get_next_state_and_reward(self, action):\n",
    "        # Limit the action value to the maximum index of the DataFrame\n",
    "        action = min(action, len(self.data) - 1)\n",
    "\n",
    "        # Get the recommended image's features based on the action\n",
    "        recommended_image_features = self.data.iloc[action]['features']\n",
    "\n",
    "        # Calculate the reward using the reward_function (you need to define user_preferences)\n",
    "        user_preferences = self.current_state.reshape(1, -1)\n",
    "        recommended_image_features = recommended_image_features.reshape(1, -1)\n",
    "        reward = reward_function(user_preferences, recommended_image_features)\n",
    "\n",
    "        # Get the next state (in this case, we will use the same state as the current state)\n",
    "        next_state = self.current_state\n",
    "\n",
    "        return next_state, reward\n",
    "\n",
    "\n",
    "    def _is_done(self, next_state):\n",
    "        # In this example, we will define a specific condition for the episode to be done.\n",
    "        # If the step_count reaches the max_steps, the episode is done.\n",
    "        return self.step_count >= self.max_steps\n",
    "\n",
    "class CustomDQNNetwork(nn.Module):\n",
    "    def __init__(self, *args, input_dim=None, output_dim=None, **kwargs):\n",
    "        super(CustomDQNNetwork, self).__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, output_dim)\n",
    "        )\n",
    "\n",
    "        self.features_dim = output_dim\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def reward_function(user_preferences, recommended_image_features):\n",
    "    similarities = cosine_similarity(user_preferences, recommended_image_features)\n",
    "    return similarities.mean()\n",
    "\n",
    "import time\n",
    "\n",
    "def evaluate_agent(agent, env, num_episodes):\n",
    "    total_rewards = []\n",
    "    episode_durations = []\n",
    "    episode_steps = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        start_time = time.time()\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "\n",
    "        while not done:\n",
    "            action, _ = agent.predict(state, deterministic=True)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            step_count += 1\n",
    "            print(f\"Episode {episode + 1}, Step {step_count}: Reward={reward}\")\n",
    "\n",
    "\n",
    "        end_time = time.time()\n",
    "        episode_duration = end_time - start_time\n",
    "        total_rewards.append(episode_reward)\n",
    "        episode_durations.append(episode_duration)\n",
    "        episode_steps.append(step_count)\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Reward={episode_reward}, Duration={episode_duration:.2f}s, Steps={step_count}\")\n",
    "\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    avg_duration = np.mean(episode_durations)\n",
    "    avg_steps = np.mean(episode_steps)\n",
    "\n",
    "    return avg_reward, avg_duration, avg_steps\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T19:16:09.507729Z",
     "end_time": "2023-04-08T19:16:09.509529Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_and_test()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T18:58:28.295669Z",
     "end_time": "2023-04-08T19:03:24.765380Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "recommend()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-08T19:16:15.830020Z",
     "end_time": "2023-04-08T19:16:15.874282Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
