{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import functools\n",
    "import pathlib\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import threading\n",
    "from multiprocessing import Pool\n",
    "import concurrent\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sqlite3\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Settings base variables and paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Set the base folder path for the project\n",
    "output_path = \"../output\"\n",
    "images_path = os.path.join(output_path, \"images\")\n",
    "metadata_path = os.path.join(output_path, \"metadata\")\n",
    "config_path = os.path.join(output_path, \"config\")\n",
    "\n",
    "list_of_paths = [output_path, images_path, metadata_path, config_path]\n",
    "\n",
    "# Set the base URL for the dataset\n",
    "metadata_extension = \"sqlite\"\n",
    "\n",
    "yolo_cfg = \"https://raw.githubusercontent.com/pjreddie/darknet/master/cfg/yolov3.cfg\"\n",
    "yolo_weights = \"https://pjreddie.com/media/files/yolov3.weights\"\n",
    "yolo_classes = \"https://raw.githubusercontent.com/pjreddie/darknet/master/data/coco.names\"\n",
    "\n",
    "files_to_download = [\n",
    "    (yolo_cfg, os.path.join(config_path, \"yolov3.cfg\")),\n",
    "    (yolo_weights, os.path.join(config_path, \"yolov3.weights\")),\n",
    "    (yolo_classes, os.path.join(config_path, \"coco.names\")),\n",
    "]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create folder structure\n",
    "The code creates the folder structure for the project. The folder structure is as follows:\n",
    "- output\n",
    "    - images\n",
    "    - metadata\n",
    "    - config\n",
    "\n",
    "This method creates a folder with the given path if it doesn't already exist, It also outputs a message to inform the user if the folder was created or if it already exists.\n",
    "This is useful for organizing and managing files in a project. By creating a folder to store data and resources, it keeps the working directory tidy and makes it easier to locate files. Additionally, by checking if the folder exists before creating it, it prevents the program from overwriting existing data or throwing an error."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def create_folder(path):\n",
    "    \"\"\"\n",
    "    This function creates a folder at the specified path.\n",
    "    If the folder already exists, it will print a message saying so.\n",
    "    If there is an error creating the folder, it will print the error message.\n",
    "\n",
    "    Parameters:\n",
    "        :param path (str): The path of the folder to be created.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use os.mkdir to create the folder at the specified path\n",
    "        os.mkdir(path)\n",
    "        print(f\"Folder {path} created\")\n",
    "    except FileExistsError:\n",
    "        # If the folder already exists, print a message saying so\n",
    "        print(f\"Folder {path} already exists\")\n",
    "    except Exception as e:\n",
    "        # If there is an error creating the folder, print the error message\n",
    "        print(f\"Error creating folder {path}: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create the folder structure\n",
    "This method initializes a list of folders by calling the create_folder method for each folder in the list.\n",
    "The purpose of this method is to make sure that all necessary folders exist before the program continues its execution.\n",
    "If a folder does not exist, the create_folder method will create it. If a folder already exists, the method will simply print a message indicating that the folder already exists. In case of any other error, the method will print the error message."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def init_folder(folder_names: list):\n",
    "    for folder_name in folder_names:\n",
    "        create_folder(folder_name)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder ../output already exists\n",
      "Folder ../output/images already exists\n",
      "Folder ../output/metadata already exists\n",
      "Folder ../output/config already exists\n"
     ]
    }
   ],
   "source": [
    "init_folder(list_of_paths)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define methods for downloading the necessary files for yolo\n",
    "The following code block is a method to download a file from a given URL and save it to a specified filename.\n",
    "The method starts by creating a session (s = requests.Session()) and then mounting it to the URL (s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))). This sets the maximum number of retries to 3 if the connection to the URL fails.\n",
    "Then, the method makes a GET request to the URL (r = s.get(url, stream=True, allow_redirects=True)) and checks if it returns a successful response (r.raise_for_status()). If there was an HTTP error during the request, the error message is printed (print(f\"HTTP error occurred while downloading dataset: {e}\")).\n",
    "The method also checks the file size specified in the response headers and assigns it to the variable file_size (file_size = int(r.headers.get(‘Content-Length’, 0))). If the file size is 0, a default value of “(Unknown total file size)” is assigned to the variable desc; otherwise, the variable desc is left empty.\n",
    "Next, the method resolves the file path and creates a directory if it doesn’t already exist (path.parent.mkdir(parents=True, exist_ok=True)). The method then creates a tqdm progress bar to show the download progress (with tqdm.tqdm(total=file_size, unit=‘B’, unit_scale=True, desc=desc) as pbar:).\n",
    "Finally, the method writes the contents of the file to disk in chunks (for chunk in r.iter_content(chunk_size=1024):), updating the progress bar for each chunk that is written to disk (pbar.update(len(chunk))). If an error occurred during the download, a message with the error is printed (print(f\"Error occurred while downloading dataset: {e}\")). The file path is returned when the method is finished."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def download(url, filename):\n",
    "    \"\"\"\n",
    "    This download a file from a given URL and save it to a specified filename.\n",
    "\n",
    "    Parameters:\n",
    "        :param url (str): The URL of the file to be downloaded.\n",
    "        :param filename (str): The filename to save the file as.\n",
    "\n",
    "    Returns:\n",
    "    path (str): The path of the downloaded file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a session object to persist the state of connection\n",
    "        s = requests.Session()\n",
    "        # Retry connecting to the URL up to 3 times\n",
    "        s.mount(url, requests.adapters.HTTPAdapter(max_retries=3))\n",
    "        # Send a GET request to the URL to start the download\n",
    "        r = s.get(url, stream=True, allow_redirects=True)\n",
    "        # Raise an error if the response is not 200 OK\n",
    "        r.raise_for_status()\n",
    "        # Get the file size from the Content-Length header, default to 0 if not present\n",
    "        file_size = int(r.headers.get('Content-Length', 0))\n",
    "        # Get the absolute path to the target file\n",
    "        path = pathlib.Path(filename).expanduser().resolve()\n",
    "        # Create parent directories if they don't exist\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        # Set the description to display while downloading, \"(Unknown total file size)\" if file size is 0\n",
    "        desc = \"(Unknown total file size)\" if file_size == 0 else \"\"\n",
    "        # Enable decoding the response content\n",
    "        r.raw.read = functools.partial(r.raw.read, decode_content=True)\n",
    "        # Use tqdm to display the download progress\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=desc) as pbar:\n",
    "            # Open the target file in binary write mode\n",
    "            with path.open(\"wb\") as f:\n",
    "                # Write each chunk of data from the response to the file\n",
    "                for chunk in r.iter_content(chunk_size=1024):\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "        # Return the path to the downloaded file\n",
    "        return path\n",
    "    # Handle HTTP error if the response is not 200 OK\n",
    "    except requests.exceptions.HTTPError as e:\n",
    "        print(f\"HTTP error occurred while downloading dataset: {e}\")\n",
    "    # Handle any other exceptions that might occur while downloading the file\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while downloading dataset: {e}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Download the yolov3 files if they don't already exist"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99.6M/99.6M [00:02<00:00, 47.6MB/s]\n",
      "100%|██████████| 95.6M/95.6M [00:02<00:00, 39.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files are downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for url, filename in files_to_download:\n",
    "    # check if the file already exists\n",
    "    if not pathlib.Path(filename).exists():\n",
    "        # download the file\n",
    "        download(url, filename)\n",
    "\n",
    "print(\"All files are downloaded\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "with open(config_path + \"/yolo9000.weights\", \"wb\") as outfile:\n",
    "    for filename in glob.glob(config_path + \"/x*\"):\n",
    "        with open(filename, \"rb\") as infile:\n",
    "            outfile.write(infile.read())\n",
    "\n",
    "    # Delete the temporary files\n",
    "    for filename in glob.glob(config_path + \"/x*\"):\n",
    "        os.remove(filename)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "# Define methods to get all the image paths\n",
    "The get_all_images method is used to retrieve all images present in the specified image path. It uses the os.walk function to traverse through all subdirectories within the image path and collects the file names that end with either '.png' or '.jpg' extensions. The full path of each image is then generated by joining the root directory and the file name. The method returns a list of all images' full paths. In case of any error, an error message is printed and an empty list is returned."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_images(path):\n",
    "    \"\"\"Get all images from the given path.\n",
    "\n",
    "    Args:\n",
    "    param: image_path (str): path to the directory containing the images.\n",
    "\n",
    "    Returns:\n",
    "    - list: a list of full path to all the images with png or jpg extensions.\n",
    "    - empty list: an empty list if an error occurred while fetching images.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # use os.walk to traverse all the subdirectories and get all images\n",
    "        return [os.path.join(root, name)\n",
    "                for root, dirs, files in os.walk(path)\n",
    "                for name in files\n",
    "                if name.endswith((\".png\", \".jpg\"))]\n",
    "    except Exception as e:\n",
    "        # return an empty list and log the error message if an error occurred\n",
    "        print(f\"An error occurred while fetching images: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def get_labels(label_path):\n",
    "    # check if the file exists\n",
    "    if not pathlib.Path(label_path).exists():\n",
    "        print(f\"Label file {label_path} does not exist\")\n",
    "        return []\n",
    "    return open(label_path).read().strip().split(\"\\n\")\n",
    "\n",
    "def get_colors(labels):\n",
    "    return np.random.randint(0, 255, size=(len(labels), 3), dtype=\"uint8\")\n",
    "\n",
    "def detect(\n",
    "        path_name,\n",
    "        output_folder=\"../output/images_labelized/\",\n",
    "        label_path=\"../output/config/coco.names\",\n",
    "        weights_path=\"../output/config/yolov3.weights\",\n",
    "        config_path=\"../output/config/yolov3.cfg\",\n",
    "        CONFIDENCE=0.5,\n",
    "        SCORE_THRESHOLD=0.5,\n",
    "        IOU_THRESHOLD=0.5\n",
    "):\n",
    "    labels = get_labels(label_path)\n",
    "    colors = get_colors(labels)\n",
    "\n",
    "    # load the COCO class labels our YOLO model was trained on\n",
    "    net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "    image = cv2.imread(path_name)\n",
    "    file_name = os.path.basename(path_name)\n",
    "    filename, ext = file_name.split(\".\")\n",
    "\n",
    "    h, w = image.shape[:2]\n",
    "    # create 4D blob\n",
    "    blob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "    # sets the blob as the input of the network\n",
    "    net.setInput(blob)\n",
    "    # get all the layer names\n",
    "    ln = net.getLayerNames()\n",
    "    print(ln)\n",
    "    try:\n",
    "        ln = [ln[i[0] - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    except IndexError:\n",
    "        # in case getUnconnectedOutLayers() returns 1D array when CUDA isn't available\n",
    "        ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "    # feed forward (inference) and get the network output\n",
    "    # measure how much it took in seconds\n",
    "    layer_outputs = net.forward(ln)\n",
    "\n",
    "    font_scale = 1\n",
    "    thickness = 1\n",
    "    boxes, confidences, class_ids = [], [], []\n",
    "    text_labels = []\n",
    "    # loop over each of the layer outputs\n",
    "    for output in layer_outputs:\n",
    "        # loop over each of the object detections\n",
    "        for detection in output:\n",
    "            # extract the class id (label) and confidence (as a probability) of\n",
    "            # the current object detection\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            # discard out weak predictions by ensuring the detected\n",
    "            # probability is greater than the minimum probability\n",
    "            if confidence > CONFIDENCE:\n",
    "                # scale the bounding box coordinates back relative to the\n",
    "                # size of the image, keeping in mind that YOLO actually\n",
    "                # returns the center (x, y)-coordinates of the bounding\n",
    "                # box followed by the boxes' width and height\n",
    "                box = detection[:4] * np.array([w, h, w, h])\n",
    "                (centerX, centerY, width, height) = box.astype(\"int\")\n",
    "                # use the center (x, y)-coordinates to derive the top and\n",
    "                # and left corner of the bounding box\n",
    "                x = int(centerX - (width / 2))\n",
    "                y = int(centerY - (height / 2))\n",
    "                # update our list of bounding box coordinates, confidences,\n",
    "                # and class IDs\n",
    "                boxes.append([x, y, int(width), int(height)])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # loop over the indexes we are keeping\n",
    "        for i in range(len(boxes)):\n",
    "            text_labels.append(labels[class_ids[i]])\n",
    "            # extract the bounding box coordinates\n",
    "            x, y = boxes[i][0], boxes[i][1]\n",
    "            w, h = boxes[i][2], boxes[i][3]\n",
    "            # draw a bounding box rectangle and label on the image\n",
    "            color = [int(c) for c in colors[class_ids[i]]]\n",
    "            cv2.rectangle(image, (x, y), (x + w, y + h), color=color, thickness=thickness)\n",
    "            text = f\"{labels[class_ids[i]]}: {confidences[i]:.2f}\"\n",
    "            # calculate text width & height to draw the transparent boxes as background of the text\n",
    "            (text_width, text_height) = \\\n",
    "            cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, fontScale=font_scale, thickness=thickness)[0]\n",
    "            text_offset_x = x\n",
    "            text_offset_y = y - 5\n",
    "            box_coords = ((text_offset_x, text_offset_y), (text_offset_x + text_width + 2, text_offset_y - text_height))\n",
    "            overlay = image.copy()\n",
    "            cv2.rectangle(overlay, box_coords[0], box_coords[1], color=color, thickness=cv2.FILLED)\n",
    "            # add opacity (transparency to the box)\n",
    "            image = cv2.addWeighted(overlay, 0.6, image, 0.4, 0)\n",
    "            # now put the text (label: confidence %)\n",
    "            cv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        fontScale=font_scale, color=(0, 0, 0), thickness=thickness)\n",
    "\n",
    "    cv2.imwrite(os.path.join(output_folder, filename + \".\" + ext), image)\n",
    "\n",
    "    return text_labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "\n",
    "def save_metadata(metadata, img_name, metadata_path, save_format='pickle'):\n",
    "    \"\"\"\n",
    "    This function saves the metadata information of an image in either pickle or json format.\n",
    "    Parameters:\n",
    "    metadata (dict): The metadata information of an image.\n",
    "    img_name (str): The file name of the image.\n",
    "    metadata_path (str): The path to the directory where the metadata will be saved.\n",
    "    save_format (str): The format in which the metadata will be saved. The default is 'pickle'.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if save_format == 'pickle':\n",
    "            # save the metadata in pickle format\n",
    "            with open(os.path.join(metadata_path, os.path.splitext(os.path.basename(img_name))[0] + '.pickle'),\n",
    "                      'wb') as f:\n",
    "                pickle.dump(metadata, f)\n",
    "        elif save_format == 'json':\n",
    "            # save the metadata in json format\n",
    "            with open(os.path.join(metadata_path, os.path.splitext(os.path.basename(img_name))[0] + '.json'), 'w') as f:\n",
    "                json.dump(metadata, f)\n",
    "        elif save_format == 'sqlite':\n",
    "            # Get only the file name of the image\n",
    "            img_name = os.path.basename(img_name)\n",
    "            # Open a connection to the database\n",
    "            conn = sqlite3.connect(os.path.join(metadata_path, 'metadata.db'))\n",
    "            # Create a cursor\n",
    "            c = conn.cursor()\n",
    "            # Create a table if it doesn't exist : filename, key, value\n",
    "            c.execute('''CREATE TABLE IF NOT EXISTS metadata (filename text, key text, value text)''')\n",
    "            # Insert the metadata into the table\n",
    "            for key, value in metadata.items():\n",
    "                # Convert key, value to string\n",
    "                key = str(key)\n",
    "                value = str(value)\n",
    "                # Check if the key is already in the table\n",
    "                c.execute(\"SELECT * FROM metadata WHERE filename=? AND key=?\", (img_name, key))\n",
    "                # If the key is already in the table, update the value\n",
    "                if c.fetchone():\n",
    "                    c.execute(\"UPDATE metadata SET value=? WHERE filename=? AND key=?\", (value, img_name, key))\n",
    "                    # Commit the changes\n",
    "                    conn.commit()\n",
    "                # If the key is not in the table, insert the key, value pair\n",
    "                else:\n",
    "                    c.execute(\"INSERT INTO metadata VALUES (?, ?, ?)\", (img_name, key, value))\n",
    "                    # Commit the changes\n",
    "                    conn.commit()\n",
    "            # Close the connection\n",
    "            conn.close()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid save format\")\n",
    "    except Exception as e:\n",
    "        # print an error message if an error occurs\n",
    "        print(f\"An error occurred while saving metadata for {img_name}: {str(e)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def read_sqlite(metadata_path, filename):\n",
    "    # Open a connection to the database\n",
    "    conn = sqlite3.connect(os.path.join(metadata_path, 'metadata.db'))\n",
    "    # Create a cursor\n",
    "    c = conn.cursor()\n",
    "    # Create a table if it doesn't exist : filename, key, value\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS metadata (filename text, key text, value text)''')\n",
    "    # Insert the metadata into the table\n",
    "    c.execute(\"SELECT * FROM metadata WHERE filename=?\", (filename,))\n",
    "    # If the key is already in the table, update the value\n",
    "    metadata = {}\n",
    "    for row in c.fetchall():\n",
    "        metadata[row[1]] = row[2]\n",
    "    # Close the connection\n",
    "    conn.close()\n",
    "    return metadata"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating tags: 100%|██████████| 101/101 [00:00<00:00, 277.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error:  'NoneType' object has no attribute 'shape'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def update_tags(images, metadata_path, save_format='pickle'):\n",
    "    # Run the YOLOv3 algorithm on each image\n",
    "    # display progress bar in the first thread only\n",
    "    for image in tqdm(images, desc=\"Updating tags\"):\n",
    "        # read pickle file from ../output/metadata/file_name.pkl\n",
    "        file_name = os.path.basename(image)\n",
    "        file_name, ext = file_name.split(\".\")\n",
    "        try:\n",
    "            if save_format == 'sqlite':\n",
    "\n",
    "                metadata = read_sqlite(metadata_path, file_name + \".\" + ext)\n",
    "            else:\n",
    "                with open(os.path.join(\"../output/metadata\", file_name + \".\" + metadata_extension), \"rb\") as f:\n",
    "                    if metadata_extension == \"json\":\n",
    "                        metadata = json.load(f)\n",
    "                    else:\n",
    "                        metadata = pickle.load(f)\n",
    "\n",
    "            if \"tags\" in metadata:\n",
    "                continue\n",
    "\n",
    "            labels = detect(image)\n",
    "\n",
    "            # Remove duplicates from labels\n",
    "            labels = list(set(labels))\n",
    "            # add labels to metadata\n",
    "            metadata[\"tags\"] = labels\n",
    "\n",
    "            save_metadata(metadata, file_name + \".\" + ext, metadata_path, save_format)\n",
    "        except FileNotFoundError:\n",
    "            print(\"File not found: \", file_name)\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(\"Error: \", e)\n",
    "            continue\n",
    "\n",
    "\n",
    "# Get the list of images\n",
    "images = os.listdir(\"../output/images\")\n",
    "images = [os.path.join(\"../output/images\", image) for image in images]\n",
    "\n",
    "update_tags(images, \"../output/metadata\", save_format='sqlite')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, find dominant colors in the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Convert RGB to HEX (RGB is an array of 3 values)\n",
    "def rgb_to_hex(rgb):\n",
    "    return '#%02x%02x%02x' % (int(rgb[0]), int(rgb[1]), int(rgb[2]))\n",
    "\n",
    "\n",
    "# Find 4 dominant colors in the image\n",
    "def find_dominant_colors(image, k=4, image_processing_size=None):\n",
    "    # Resize image if new dims provided, just to speed up processing\n",
    "    if image_processing_size is not None:\n",
    "        image = cv2.resize(image, image_processing_size, interpolation=cv2.INTER_AREA)\n",
    "    # Convert to RGB from BGR\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    # Reshape the image to be a list of pixels\n",
    "    image = image.reshape((image.shape[0] * image.shape[1], 3))\n",
    "    # Cluster and assign labels to the pixels\n",
    "    clt = KMeans(n_clusters=k, n_init=10)\n",
    "    labels = clt.fit_predict(image)\n",
    "    # Count labels to find most popular\n",
    "    label_counts = Counter(labels)\n",
    "    # Subset out most popular centroid\n",
    "    dominant_colors = [clt.cluster_centers_[i] for i in label_counts.keys()]\n",
    "    # For each color, convert to hex and append to list\n",
    "    dominant_colors = [rgb_to_hex(color) for color in dominant_colors]\n",
    "\n",
    "    # find percent of each color in the image\n",
    "    percent = [int((label_counts[i] / len(labels)) * 100) for i in label_counts.keys()]\n",
    "\n",
    "    # return the 4 dominant colors and their percent\n",
    "    dominant_colors = list(zip(dominant_colors, percent))\n",
    "    return dominant_colors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding dominant colors: 100%|██████████| 100/100 [31:45<00:00, 19.05s/it]\n"
     ]
    }
   ],
   "source": [
    "images = get_all_images(\"../output/images\")\n",
    "# For each image in the folder \"../images_labelized\"\n",
    "for image in tqdm(images, desc=\"Finding dominant colors\"):\n",
    "    img = cv2.imread(image)\n",
    "    # Save the dominant color\n",
    "    # First read metadata from the file\n",
    "    # TODO: make a method to read metadata in fact of the save_metadata pproperty\n",
    "    metadata = read_sqlite(\"../output/metadata\", os.path.basename(image))\n",
    "\n",
    "    if \"dominant_color\" in metadata:\n",
    "        continue\n",
    "\n",
    "    dominant_color = find_dominant_colors(img)\n",
    "\n",
    "    # Add the dominant color to the metadata\n",
    "    metadata[\"dominant_color\"] = dominant_color\n",
    "    # Save the metadata\n",
    "    save_metadata(metadata, os.path.basename(image), \"../output/metadata\", save_format='sqlite')"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
